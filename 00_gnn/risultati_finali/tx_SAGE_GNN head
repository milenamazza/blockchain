{"cells":[{"cell_type":"markdown","source":["# Addestramento\n","Addestramento del migliore modello SAGE per calcolare le performance sul test set per la classificazione delle transazioni"],"metadata":{"id":"fPrQ3dOEx291"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iidXjXM7pXO"},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","!pip install torch_geometric\n","#!pip install torch-sparse\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","import os.path as osp\n","import torch\n","import warnings\n","from torch_geometric.data import Data, HeteroData\n","from torch_geometric.transforms import RandomNodeSplit\n","from torch_geometric.nn import GCNConv, GATConv, SAGEConv, ChebConv\n","import torch_geometric.nn as pyg_nn\n","import torch.nn as nn\n","import torch_geometric.utils as pyg_utils\n","from torch.nn import Module, Linear\n","import torch.nn.functional as F\n","from sklearn.metrics import precision_recall_fscore_support, f1_score, classification_report\n","from torch_geometric.seed import seed_everything\n","import joblib\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","warnings.simplefilter(action='ignore')\n","SEED = 51\n","FILEPATH_TX = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/tx_result_1.csv\"\n","FILEPATH_WALLET = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/w_result_1.csv\"\n","\n","base_path = \"/content/drive/MyDrive/blockchain/E++/\"\n","path_comb = '/content/drive/MyDrive/blockchain/00_gnn/combination_do.csv'\n","\n","type_classification = 'tx'"]},{"cell_type":"markdown","metadata":{"id":"XOk9zPoYDC5I"},"source":["##Crea db vuoto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCrI1kO2CYxi"},"outputs":[],"source":["def create_df():\n","  # Crea DataFrame vuoti\n","  df_tx = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","  df_wallet = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","\n","  # Salva i DataFrame come file CSV\n","  df_tx.to_csv(FILEPATH_TX, index=False)\n","  df_wallet.to_csv(FILEPATH_WALLET, index=False)\n","\n","#create_df()"]},{"cell_type":"markdown","metadata":{"id":"W1Syx8hzDPvM"},"source":["##Carica dati"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qobTcbrVqpwG"},"outputs":[],"source":["def load_data():\n","    # Loading transactions\n","\n","    #Reading edges, features and classes from transaction files (as done with the original dataset)\n","    df_edges_tx = pd.read_csv(osp.join(base_path, \"txs_edgelist.csv\"))\n","    df_features_tx = pd.read_csv(osp.join(base_path, \"txs_features.csv\"), header=None)\n","    df_classes_tx = pd.read_csv(osp.join(base_path, \"txs_classes.csv\"))\n","\n","    #Columns naming based on index\n","    colNames1_tx = {'0': 'txId', 1: \"Time step\"}\n","    colNames2_tx = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(94)}\n","    colNames3_tx = {str(ii+96): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n","\n","    colNames_tx = dict(colNames1_tx, **colNames2_tx, **colNames3_tx)\n","    colNames_tx = {int(jj): item_kk for jj, item_kk in colNames_tx.items()}\n","\n","    # Rename feature columns\n","    df_features_tx = df_features_tx.rename(columns=colNames_tx)\n","\n","    # Map unknown class to '3'\n","    df_classes_tx.loc[df_classes_tx['class'] == 'unknown', 'class'] = '3'\n","\n","    # Merge classes and features in one Dataframe\n","    df_class_feature_tx = pd.merge(df_classes_tx, df_features_tx)\n","\n","    # Exclude records with unknown class transaction\n","    df_class_feature_tx = df_class_feature_tx[df_class_feature_tx['class'] != 3]\n","\n","    # Build Dataframe with head and tail of transactions (edges)\n","    known_txs = df_class_feature_tx[\"txId\"].values\n","    df_edges_tx = df_edges_tx[(df_edges_tx[\"txId1\"].isin(known_txs)) & (df_edges_tx[\"txId2\"].isin(known_txs))]\n","\n","    # Build indices for features and edge types\n","    features_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"txId\"].unique()))}\n","    class_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_tx[\"txId\"] = df_class_feature_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_class_feature_tx[\"class\"] = df_class_feature_tx[\"class\"].apply(lambda name: class_idx_tx[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_tx[\"txId1\"] = df_edges_tx[\"txId1\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx[\"txId2\"] = df_edges_tx[\"txId2\"].apply(lambda name: features_idx_tx[name])\n","\n","    # Loading wallets\n","\n","    # From file\n","    df_edges_wallet = pd.read_csv(osp.join(base_path, \"AddrAddr_edgelist.csv\"))\n","    df_class_feature_wallet = pd.read_csv(osp.join(base_path, \"wallets_features_classes_combined.csv\"))\n","\n","    # Exclude records with unknown class transaction\n","    #print(df_class_feature_wallet.shape)\n","    df_class_feature_wallet = df_class_feature_wallet[df_class_feature_wallet[\"class\"] != 3]\n","    #print(df_class_feature_wallet.shape)\n","\n","    # Build Dataframe with head and tail of AddrToAddr (edges)\n","    known_wallets = df_class_feature_wallet[\"address\"].values\n","    df_edges_wallet = df_edges_wallet[(df_edges_wallet[\"input_address\"].isin(known_wallets)) & (df_edges_wallet[\"output_address\"].isin(known_wallets))]\n","\n","    # Building indices for features and edge types\n","    features_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"address\"].unique()))}\n","    class_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_wallet[\"address\"] = df_class_feature_wallet[\"address\"].apply(lambda name: features_idx_wallet[name])\n","    df_class_feature_wallet[\"class\"] = df_class_feature_wallet[\"class\"].apply(lambda name: class_idx_wallet[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet[\"input_address\"] = df_edges_wallet[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet[\"output_address\"] = df_edges_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    # Loading AddrTx and TxAddr\n","\n","    # From file\n","    df_edges_wallet_tx = pd.read_csv(osp.join(base_path, \"AddrTx_edgelist.csv\"))\n","    df_edges_tx_wallet = pd.read_csv(osp.join(base_path, \"TxAddr_edgelist.csv\"))\n","\n","    # Build Dataframe with head and tail of AddrTx (edges)\n","    df_edges_wallet_tx = df_edges_wallet_tx[(df_edges_wallet_tx[\"input_address\"].isin(known_wallets)) & df_edges_wallet_tx[\"txId\"].isin(known_txs)]\n","    df_edges_tx_wallet = df_edges_tx_wallet[(df_edges_tx_wallet[\"txId\"].isin(known_txs)) & df_edges_tx_wallet[\"output_address\"].isin(known_wallets)]\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet_tx[\"input_address\"] = df_edges_wallet_tx[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet_tx[\"txId\"] = df_edges_wallet_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"txId\"] = df_edges_tx_wallet[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"output_address\"] = df_edges_tx_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    return df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet\n","\n","def data_to_pyg(df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet):\n","    data = HeteroData()\n","\n","    # Defining PyG objects for transactions\n","    df_class_feature_tx = df_class_feature_tx.fillna(0)\n","    data['tx'].x = torch.tensor(df_class_feature_tx.iloc[:, 3:].values, dtype=torch.float)\n","    data['tx'].y = torch.tensor(df_class_feature_tx[\"class\"].values, dtype=torch.long)\n","    data['tx','is_related_to','tx'].edge_index = torch.tensor([df_edges_tx[\"txId1\"].values,\n","                            df_edges_tx[\"txId2\"].values], dtype=torch.int64)\n","    #data['tx'] = random_node_split(num_val=0.15, num_test=0.2)(data['tx'])\n","    # Defining PyG objects for wallets\n","    data['wallet'].x = torch.tensor(df_class_feature_wallet.iloc[:, 3:].values, dtype=torch.float)\n","    data['wallet'].y = torch.tensor(df_class_feature_wallet[\"class\"].values, dtype=torch.long)\n","    data['wallet','interacts_with','wallet'].edge_index = torch.tensor([df_edges_wallet[\"input_address\"].values,\n","                            df_edges_wallet[\"output_address\"].values], dtype=torch.int64)\n","    #data['wallet'] = random_node_split(num_val=0.15, num_test=0.2)(data['wallet'])\n","    # Defining PyG objects for cross-edges\n","    data['wallet','performs','tx'].edge_index = torch.tensor([df_edges_wallet_tx[\"input_address\"].values,\n","                                         df_edges_wallet_tx[\"txId\"].values], dtype=torch.int64)\n","\n","    data['tx', 'flows_into', 'wallet'].edge_index = torch.tensor([df_edges_tx_wallet[\"txId\"].values,\n","                                         df_edges_tx_wallet[\"output_address\"].values], dtype=torch.int64)\n","\n","    # Impostare il seed per la divisione del dataset\n","    return RandomNodeSplit(num_val=0.10, num_test=0.15)(data)"]},{"cell_type":"markdown","metadata":{"id":"OhFPkCpExzOL"},"source":["##Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQfEkZ1nx44T"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do2dQUF-x7hj"},"outputs":[],"source":["#NEW -> SU RANGE\n","# Utility per conversione a tensor\n","def to_tensor(arr):\n","    return torch.tensor(arr, dtype=torch.float).to(device)\n","\n","def scale_features(data, method=\"standard\"):\n","  if method == 'no':\n","    return data\n","\n","  # Scaling per training\n","  data['tx'].x[data['tx'].train_mask] = to_tensor(\n","      scale_train_data(data['tx'].x[data['tx'].train_mask].cpu().numpy(), method, 'tx')\n","  )\n","  data['wallet'].x[data['wallet'].train_mask] = to_tensor(\n","      scale_train_data(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), method, 'wallet')\n","  )\n","\n","  # Scaling per validation\n","  data['tx'].x[data['tx'].val_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].val_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].val_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), method, 'wallet'))\n","\n","  data['tx'].x[data['tx'].test_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].test_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].test_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), method, 'wallet'))\n","  return data\n","\n","def scale_train_data(train, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = StandardScaler()\n","        scaled_train = scaler.fit_transform(train)  # Scala tutte le colonne\n","        joblib.dump(scaler, f\"scaler_standard_{df}.pkl\")\n","\n","        if 'l2' in scaling_method:\n","            norm = Normalizer(norm='l2')\n","            scaled_train = norm.fit_transform(scaled_train)\n","            joblib.dump(norm, f\"scaler_l2_{df}.pkl\")\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_train\n","\n","def scale_validation_data(val, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = joblib.load(f\"scaler_standard_{df}.pkl\")\n","        scaled_val = scaler.transform(val)  # Scala tutte le colonne\n","\n","        if 'l2' in scaling_method:\n","            norm = joblib.load(f\"scaler_l2_{df}.pkl\")\n","            scaled_val = norm.transform(scaled_val)\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVRMmabn21eP"},"outputs":[],"source":["def dimentional_reduction(data, dim_reduction, pca_threshold):\n","    if dim_reduction == 'no':\n","        return data\n","    elif dim_reduction == 'pca':\n","        data1 = copy.deepcopy(data)\n","\n","        transformed_tx_data = apply_pca_train(data['tx'].x[data['tx'].train_mask].cpu().numpy(), 'tx', pca_threshold)\n","        transformed_wallet_data = apply_pca_train(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), 'wallet', pca_threshold)\n","\n","        data1['tx'].x = torch.zeros((data['tx'].x.shape[0], transformed_tx_data.shape[1]), dtype=torch.float, device=device)\n","        data1['wallet'].x = torch.zeros((data['wallet'].x.shape[0], transformed_wallet_data.shape[1]), dtype=torch.float, device=device)\n","\n","        data1['tx'].x[data['tx'].train_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].train_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].val_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].val_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].val_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].test_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].test_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].test_mask] = to_tensor(transformed_wallet_data)\n","\n","        return data1\n","\n","def apply_pca_train(train, df, pca_threshold=0.99):\n","    pca = PCA(random_state=SEED)\n","    pca.fit(train)\n","\n","    # Selezione componenti principali\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","\n","    pca = PCA(n_components=n_components, random_state=SEED)\n","    transformed_data = pca.fit_transform(train).astype(np.float32)\n","\n","    joblib.dump(pca, f\"pca_model_{df}.pkl\")\n","    print(f\"  Numero di componenti principali per {df}: {pca.n_components_}\")\n","\n","    return transformed_data\n","\n","def apply_pca_validation(val, df):\n","    pca = joblib.load(f\"pca_model_{df}.pkl\")\n","    transformed_data = pca.transform(val).astype(np.float32)\n","    return transformed_data"]},{"cell_type":"markdown","metadata":{"id":"CaNNtrNLDSdI"},"source":["##Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbfXKtdynFBL"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, Dropout\n","from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, TransformerConv\n","import random\n","from itertools import product\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxfbqge_p64q"},"outputs":[],"source":["def set_seed(seed = 51):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # Per più GPU\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    seed_everything(seed)\n","device = \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiBF-BZYfq7b"},"outputs":[],"source":["class ResidualHeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0.5, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.skips = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1 # Define heads\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","            self.skips.append(Linear(hidden_channels * heads, hidden_channels * heads)) # Fix: Linear layer expects the output of conv\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv, skip in zip(self.convs, self.skips):\n","            x_dict_new = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(F.relu(x + skip(x_dict_new[key]))) for key, x in x_dict_new.items()}  # Residual + Dropout\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdcebmslq5D0"},"outputs":[],"source":["class HeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1  # Definiamo i heads solo se necessario\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv in self.convs:\n","            x_dict = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJEJVt5l04Eb"},"outputs":[],"source":["def is_combination_tested(filepath, new_row):\n","    existing_results = pd.read_csv(filepath)\n","\n","    # Identifica le colonne comuni tra il dataset e new_row\n","    dataset_columns = set(existing_results.columns)\n","    comparison_columns = [col for col in new_row.keys() if col not in ['end', 'num_epoch', 'epoch']]\n","\n","    # Filtra le combinazioni\n","    filtered_results = existing_results.copy()\n","    #filtered_results = filtered_results[filtered_results['end'] == True]\n","    filtered_results = filtered_results[filtered_results['num_epoch'] >= new_row['num_epoch']]\n","\n","    for col in comparison_columns:\n","        if col in dataset_columns:\n","            # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","            filtered_results = filtered_results[\n","                (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","            ]\n","\n","    return not filtered_results.empty\n","\n","def append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, end=False):\n","  def append_and_save_result(filepath, new_row, end=False):\n","    new_row['end'] = end\n","    # Leggi i risultati esistenti\n","    results_df = pd.read_csv(filepath)\n","    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","    results_df.to_csv(filepath, index=False)\n","\n","  append_and_save_result(FILEPATH_TX, params_tx, end)\n","  append_and_save_result(FILEPATH_WALLET, params_wallet, end)\n","  if end:\n","    df_comb = pd.read_csv(path_comb)\n","    filtered_params = {key: params_tx[key] for key in params_tx if \"train\" not in key and \"val\" not in key}\n","    print(filtered_params)\n","    df_comb = pd.concat([df_comb, pd.DataFrame([filtered_params])], ignore_index=True)\n","    df_comb.to_csv(path_comb, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vo1zTF6zxQR"},"outputs":[],"source":["def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def eval(model, data, out_tx, out_wallet, params):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalità di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","6\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  loss = params_tx['train_loss'] + params_wallet['train_loss']\n","\n","  with torch.no_grad():\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for wallets\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    print(f\"Epoch {str(params['epoch']).zfill(3)}:      TX: Train Loss: {params_tx['train_loss']:.4f}, Acc: {params_tx['train_acc']:.4f}, F1: {params_tx['train_f1']:.4f} Bal: {params_tx['train_balanced_acc']:.4f} - Val Loss: {params_tx['val_loss']:.4f}, Accuracy: {params_tx['val_acc']:.4f}, F1: {params_tx['val_f1']:.4f} Bal: {params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"           WALLETS: Train Loss: {params_wallet['train_loss']:.8f}, Acc: {params_wallet['train_acc']:.8f}, F1: {params_wallet['train_f1']:.8f} Bal: {params_wallet['train_balanced_acc']:.4f} - Val Loss: {params_wallet['val_loss']:.8f}, Accuracy: {params_wallet['val_acc']:.4f}, F1: {params_wallet['val_f1']:.4f} Bal: {params_wallet['val_balanced_acc']:.4f}\")\n","\n","  return loss, params_tx, params_wallet\n","\n","\n","def eval_total(model, data, out_tx, out_wallet, params, best_epoch):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalitÃ  di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","  tx_mask_test = data['tx'].test_mask\n","  wallet_mask_test = data['wallet'].test_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  with torch.no_grad():\n","    # Calculate metrics for validation\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    report_tx_val = classification_report(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    report_wallet_val = classification_report(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for test\n","    params_tx['test_loss'] = criterion(out_tx[tx_mask_test], data['tx'].y[tx_mask_test].cpu())  # Convert to scalar\n","    params_tx['test_acc'] = accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_precision'] = precision_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_recall'] = recall_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_f1'] = f1_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    report_tx_test = classification_report(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","\n","    params_wallet['test_loss'] = criterion(out_wallet[wallet_mask_test], data['wallet'].y[wallet_mask_test].cpu())  # Convert to scalar\n","    params_wallet['test_acc'] = accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_precision'] = precision_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_recall'] = recall_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_f1'] = f1_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    report_wallet_test = classification_report(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","\n","    # Stampa delle metriche con formattazione migliorata\n","    print('Final_result for '+type_classification)\n","    print(params)\n","    print(f\"Epoch {best_epoch}:\")\n","    print(\"  TX:\")\n","    print(f\"   Train: Loss={params_tx['train_loss']:.4f}, Acc={params_tx['train_acc']:.4f}, F1={params_tx['train_f1']:.4f}, Bal. Acc={params_tx['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_tx['val_loss']:.4f}, Acc={params_tx['val_acc']:.4f}, F1={params_tx['val_f1']:.4f}, Bal. Acc={params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_tx['test_loss']:.4f}, Acc={params_tx['test_acc']:.4f}, F1={params_tx['test_f1']:.4f}, Bal. Acc={params_tx['test_balanced_acc']:.4f}\")\n","    print(report_tx_val)\n","    print(report_tx_test)\n","    print(\"  WALLETS:\")\n","    print(f\"   Train: Loss={params_wallet['train_loss']:.8f}, Acc={params_wallet['train_acc']:.8f}, F1={params_wallet['train_f1']:.8f}, Bal. Acc={params_wallet['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_wallet['val_loss']:.8f}, Acc={params_wallet['val_acc']:.4f}, F1={params_wallet['val_f1']:.4f}, Bal. Acc={params_wallet['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_wallet['test_loss']:.8f}, Acc={params_wallet['test_acc']:.4f}, F1={params_wallet['test_f1']:.4f}, Bal. Acc={params_wallet['test_balanced_acc']:.4f}\")\n","    print(report_wallet_val)\n","    print(report_wallet_test)\n","    print()\n","\n","def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def train(model, data, params):\n","    best_model = None\n","    best_epoch = None\n","\n","    if params['optimizer'] == 'Adam':\n","      optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    elif params['optimizer'] == 'AdamW':\n","      optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    else:\n","      optimizer = None\n","\n","    if params['lr_scheduler'] == 'ReduceLROnPlateau':\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=params['factor'], patience=params['p'])\n","    elif params['lr_scheduler'] == 'CosineAnnealingLR':\n","      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=params['eta_min'])\n","    elif params['lr_scheduler'] == 'StepLR':\n","      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n","    else:\n","      scheduler = None\n","\n","    print(f'Combinazione: {params}')\n","    model.train()\n","    tx_mask = data['tx'].train_mask\n","    wallet_mask = data['wallet'].train_mask\n","    tx_mask_val =  data['tx'].val_mask\n","    wallet_mask_val = data['wallet'].val_mask\n","\n","    best_val_tx_acc = 0\n","    best_val_wallet_acc = 0\n","\n","    best_val_tx_loss = float('inf')\n","    best_val_wallet_loss = float('inf')\n","    patience = params['patience']\n","    epochs_since_best = 0\n","\n","    for epoch in range(params['num_epoch']):\n","        params['epoch'] = epoch+1\n","        optimizer.zero_grad()\n","        out_tx, out_wallet = model(data.x_dict, data.edge_index_dict)\n","        loss, params_tx, params_wallet = eval(model, data, out_tx, out_wallet, params)\n","\n","        val_tx_loss = params_tx['val_loss']\n","        val_wallet_loss = params_wallet['val_loss']\n","        val_tx_acc = params_tx['val_balanced_acc']\n","        val_wallet_acc = params_wallet['val_balanced_acc']\n","\n","        # Check if validation loss has improved\n","        if val_tx_loss < best_val_tx_loss or val_wallet_loss < best_val_wallet_loss:\n","            best_val_tx_loss = val_tx_loss\n","            best_val_wallet_loss = val_wallet_loss\n","            epochs_since_best = 0\n","        else:\n","            epochs_since_best += 1\n","\n","        # Check if early stopping criteria is met\n","        if epochs_since_best >= patience:\n","            print(f'Early stopping at epoch {epoch}')\n","            append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, True)\n","            break\n","\n","        if type_classification == 'w':\n","          if best_val_wallet_acc < val_wallet_acc:\n","            best_val_wallet_acc = val_wallet_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        elif type_classification == 'tx':\n","          if best_val_tx_acc < val_tx_acc:\n","            best_val_tx_acc = val_tx_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        else:\n","          print('Definisci modello da considerare')\n","          raise ValueError\n","\n","        loss.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","        scheduler.step(loss)\n","\n","        #ppend_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, params['epoch']==params['num_epoch'])\n","\n","    out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","    eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","    return model\n","\n","\n","def train_grid(data_full, param_grid, scalers, dim_reductions, pca_thresholds):\n","    best_model = None\n","    best_f1 = 0\n","\n","    keys, values = zip(*param_grid.items())\n","    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","    combination_counter = 0\n","    total_combinations = len(param_combinations) * len(scalers) * len(dim_reductions) * len(pca_thresholds)\n","\n","    for scaler in scalers:\n","      data = scale_features(data_full.clone(), scaler)\n","      for dim_reduction in dim_reductions:\n","        for pca_threshold in pca_thresholds:\n","          data = dimentional_reduction(data, dim_reduction, pca_threshold)\n","\n","          for params in param_combinations:\n","            combination_counter += 1\n","\n","            set_seed(SEED)\n","            params['scaler'] = scaler\n","            params['dim_reduction'] = dim_reduction # Fixed the typo here: 'dim_reduction' instead of 'dim_reducition'\n","\n","            if params['lr_scheduler'] != 'ReduceLROnPlateau':\n","              params['p'] = None\n","              params['factor'] = None\n","            elif params['lr_scheduler'] != 'CosineAnnealingLR':\n","              params['T_max'] = None\n","              params['eta_min'] = None\n","\n","            if params['conv_type'] != 'Transformer':\n","              params['num_head'] = None\n","\n","            if dim_reduction == 'no':\n","              params['pca_threshold'] = None\n","            else:\n","              params['pca_threshold'] = pca_threshold\n","\n","            if True: #not is_combination_tested(path_comb, params):\n","              print(f\"  Combinazione {combination_counter}/{total_combinations}\")  # Print the counter\n","              model = None\n","              if params[ 'type_model'] == 'HeteroGNN':\n","                model = HeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              elif params[ 'type_model'] == 'ResidualHeteroGNN':\n","                model = ResidualHeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              model = train(model, data, params)\n","            else:\n","              print(f\"  Configurazione {combination_counter}/{total_combinations} già testata, salto...\")\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOYfyiGS4aMO"},"outputs":[],"source":["set_seed(SEED)\n","data = data_to_pyg(*load_data())"]},{"cell_type":"markdown","source":["# Risultati"],"metadata":{"id":"t44Q743l0LGE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2446484,"status":"ok","timestamp":1742657648716,"user":{"displayName":"Milena Mazza","userId":"07299589247263017405"},"user_tz":-60},"id":"LMeCUciZNqIt","outputId":"514986b9-09b6-4db8-f816-0b628b9490bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Numero di componenti principali per tx: 73\n","  Numero di componenti principali per wallet: 22\n","  Combinazione 1/1\n","Combinazione: {'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'SAGE', 'p': 5, 'factor': 0.2, 'eta_min': None, 'T_max': None, 'aggr': 'mean', 'lr_scheduler': 'ReduceLROnPlateau', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99}\n","Epoch 001:      TX: Train Loss: 0.6942, Acc: 0.1004, F1: 0.0015 Bal: 0.4992 - Val Loss: 0.6943, Accuracy: 0.1000, F1: 0.0019 Bal: 0.4994\n","           WALLETS: Train Loss: 0.70157844, Acc: 0.07748436, F1: 0.00000000 Bal: 0.5000 - Val Loss: 0.70102865, Accuracy: 0.0791, F1: 0.0000 Bal: 0.5000\n","Epoch 002:      TX: Train Loss: 0.6921, Acc: 0.1204, F1: 0.0462 Bal: 0.5076 - Val Loss: 0.6921, Accuracy: 0.1202, F1: 0.0480 Bal: 0.5057\n","           WALLETS: Train Loss: 0.69700617, Acc: 0.07748799, F1: 0.00000787 Bal: 0.5000 - Val Loss: 0.69665706, Accuracy: 0.0791, F1: 0.0000 Bal: 0.5000\n","Epoch 003:      TX: Train Loss: 0.6899, Acc: 0.2415, F1: 0.2774 Bal: 0.5611 - Val Loss: 0.6900, Accuracy: 0.2363, F1: 0.2728 Bal: 0.5475\n","           WALLETS: Train Loss: 0.69288898, Acc: 0.18763153, F1: 0.21770790 Bal: 0.5426 - Val Loss: 0.69272685, Accuracy: 0.1845, F1: 0.2099 Bal: 0.5406\n","Epoch 004:      TX: Train Loss: 0.6877, Acc: 0.4558, F1: 0.5739 Bal: 0.6506 - Val Loss: 0.6878, Accuracy: 0.4480, F1: 0.5661 Bal: 0.6425\n","           WALLETS: Train Loss: 0.68910313, Acc: 0.44264960, F1: 0.58454906 Bal: 0.5387 - Val Loss: 0.68911493, Accuracy: 0.4385, F1: 0.5792 Bal: 0.5385\n","Epoch 005:      TX: Train Loss: 0.6854, Acc: 0.6098, F1: 0.7296 Bal: 0.7091 - Val Loss: 0.6855, Accuracy: 0.6029, F1: 0.7232 Bal: 0.7108\n","           WALLETS: Train Loss: 0.68548965, Acc: 0.72965196, F1: 0.83762256 Bal: 0.5867 - Val Loss: 0.68566388, Accuracy: 0.7303, F1: 0.8380 Bal: 0.5870\n","Epoch 006:      TX: Train Loss: 0.6827, Acc: 0.6907, F1: 0.7979 Bal: 0.7400 - Val Loss: 0.6828, Accuracy: 0.6957, F1: 0.8018 Bal: 0.7446\n","           WALLETS: Train Loss: 0.68192309, Acc: 0.88472954, F1: 0.93703860 Bal: 0.6389 - Val Loss: 0.68224782, Accuracy: 0.8826, F1: 0.9358 Bal: 0.6363\n","Epoch 007:      TX: Train Loss: 0.6797, Acc: 0.7428, F1: 0.8379 Bal: 0.7598 - Val Loss: 0.6799, Accuracy: 0.7427, F1: 0.8379 Bal: 0.7589\n","           WALLETS: Train Loss: 0.67830563, Acc: 0.89940639, F1: 0.94574772 Bal: 0.6211 - Val Loss: 0.67877489, Accuracy: 0.8983, F1: 0.9450 Bal: 0.6231\n","Epoch 008:      TX: Train Loss: 0.6762, Acc: 0.7738, F1: 0.8602 Bal: 0.7766 - Val Loss: 0.6764, Accuracy: 0.7778, F1: 0.8631 Bal: 0.7774\n","           WALLETS: Train Loss: 0.67458206, Acc: 0.90370604, F1: 0.94835655 Bal: 0.6054 - Val Loss: 0.67518252, Accuracy: 0.9023, F1: 0.9475 Bal: 0.6060\n","Epoch 009:      TX: Train Loss: 0.6722, Acc: 0.7921, F1: 0.8729 Bal: 0.7878 - Val Loss: 0.6726, Accuracy: 0.7927, F1: 0.8734 Bal: 0.7877\n","           WALLETS: Train Loss: 0.67067939, Acc: 0.90652168, F1: 0.95001154 Bal: 0.5993 - Val Loss: 0.67139488, Accuracy: 0.9053, F1: 0.9493 Bal: 0.6004\n","Epoch 010:      TX: Train Loss: 0.6677, Acc: 0.8005, F1: 0.8786 Bal: 0.7956 - Val Loss: 0.6681, Accuracy: 0.8004, F1: 0.8785 Bal: 0.7959\n","           WALLETS: Train Loss: 0.66650903, Acc: 0.90793675, F1: 0.95082056 Bal: 0.5983 - Val Loss: 0.66731626, Accuracy: 0.9069, F1: 0.9502 Bal: 0.5996\n","Epoch 011:      TX: Train Loss: 0.6625, Acc: 0.8047, F1: 0.8813 Bal: 0.8016 - Val Loss: 0.6631, Accuracy: 0.8050, F1: 0.8816 Bal: 0.8004\n","           WALLETS: Train Loss: 0.66199929, Acc: 0.90835402, F1: 0.95105475 Bal: 0.5985 - Val Loss: 0.66286844, Accuracy: 0.9070, F1: 0.9503 Bal: 0.5994\n","Epoch 012:      TX: Train Loss: 0.6567, Acc: 0.8057, F1: 0.8819 Bal: 0.8042 - Val Loss: 0.6574, Accuracy: 0.8043, F1: 0.8811 Bal: 0.8020\n","           WALLETS: Train Loss: 0.65709013, Acc: 0.90807100, F1: 0.95088038 Bal: 0.6002 - Val Loss: 0.65799505, Accuracy: 0.9068, F1: 0.9502 Bal: 0.6017\n","Epoch 013:      TX: Train Loss: 0.6501, Acc: 0.8055, F1: 0.8817 Bal: 0.8059 - Val Loss: 0.6510, Accuracy: 0.8030, F1: 0.8802 Bal: 0.8013\n","           WALLETS: Train Loss: 0.65175676, Acc: 0.90752674, F1: 0.95054393 Bal: 0.6034 - Val Loss: 0.65267622, Accuracy: 0.9064, F1: 0.9499 Bal: 0.6041\n","Epoch 014:      TX: Train Loss: 0.6427, Acc: 0.8063, F1: 0.8822 Bal: 0.8077 - Val Loss: 0.6438, Accuracy: 0.8047, F1: 0.8813 Bal: 0.8042\n","           WALLETS: Train Loss: 0.64598107, Acc: 0.90642008, F1: 0.94986344 Bal: 0.6093 - Val Loss: 0.64689505, Accuracy: 0.9052, F1: 0.9491 Bal: 0.6091\n","Epoch 015:      TX: Train Loss: 0.6345, Acc: 0.8072, F1: 0.8828 Bal: 0.8090 - Val Loss: 0.6359, Accuracy: 0.8072, F1: 0.8829 Bal: 0.8075\n","           WALLETS: Train Loss: 0.63976276, Acc: 0.90426119, F1: 0.94844470 Bal: 0.6297 - Val Loss: 0.64065129, Accuracy: 0.9025, F1: 0.9474 Bal: 0.6304\n","Epoch 016:      TX: Train Loss: 0.6255, Acc: 0.8098, F1: 0.8845 Bal: 0.8117 - Val Loss: 0.6271, Accuracy: 0.8102, F1: 0.8849 Bal: 0.8102\n","           WALLETS: Train Loss: 0.63309610, Acc: 0.89992526, F1: 0.94556793 Bal: 0.6692 - Val Loss: 0.63394147, Accuracy: 0.8992, F1: 0.9451 Bal: 0.6725\n","Epoch 017:      TX: Train Loss: 0.6157, Acc: 0.8130, F1: 0.8867 Bal: 0.8131 - Val Loss: 0.6175, Accuracy: 0.8148, F1: 0.8880 Bal: 0.8137\n","           WALLETS: Train Loss: 0.62599546, Acc: 0.89418514, F1: 0.94182421 Bal: 0.7072 - Val Loss: 0.62678438, Accuracy: 0.8940, F1: 0.9416 Bal: 0.7142\n","Epoch 018:      TX: Train Loss: 0.6050, Acc: 0.8177, F1: 0.8899 Bal: 0.8148 - Val Loss: 0.6071, Accuracy: 0.8168, F1: 0.8894 Bal: 0.8109\n","           WALLETS: Train Loss: 0.61849171, Acc: 0.88902193, F1: 0.93849019 Bal: 0.7324 - Val Loss: 0.61921585, Accuracy: 0.8887, F1: 0.9382 Bal: 0.7356\n","Epoch 019:      TX: Train Loss: 0.5934, Acc: 0.8226, F1: 0.8932 Bal: 0.8166 - Val Loss: 0.5958, Accuracy: 0.8221, F1: 0.8930 Bal: 0.8128\n","           WALLETS: Train Loss: 0.61060286, Acc: 0.88537539, F1: 0.93615516 Bal: 0.7459 - Val Loss: 0.61125815, Accuracy: 0.8851, F1: 0.9359 Bal: 0.7478\n","Epoch 020:      TX: Train Loss: 0.5811, Acc: 0.8279, F1: 0.8968 Bal: 0.8188 - Val Loss: 0.5837, Accuracy: 0.8269, F1: 0.8962 Bal: 0.8155\n","           WALLETS: Train Loss: 0.60235590, Acc: 0.88169983, F1: 0.93384734 Bal: 0.7539 - Val Loss: 0.60294384, Accuracy: 0.8817, F1: 0.9338 Bal: 0.7544\n","Epoch 021:      TX: Train Loss: 0.5679, Acc: 0.8331, F1: 0.9002 Bal: 0.8210 - Val Loss: 0.5709, Accuracy: 0.8313, F1: 0.8991 Bal: 0.8180\n","           WALLETS: Train Loss: 0.59377497, Acc: 0.87908739, F1: 0.93217400 Bal: 0.7614 - Val Loss: 0.59430450, Accuracy: 0.8788, F1: 0.9319 Bal: 0.7602\n","Epoch 022:      TX: Train Loss: 0.5541, Acc: 0.8369, F1: 0.9027 Bal: 0.8234 - Val Loss: 0.5574, Accuracy: 0.8357, F1: 0.9020 Bal: 0.8204\n","           WALLETS: Train Loss: 0.58489889, Acc: 0.87757072, F1: 0.93117080 Bal: 0.7677 - Val Loss: 0.58538157, Accuracy: 0.8769, F1: 0.9307 Bal: 0.7669\n","Epoch 023:      TX: Train Loss: 0.5397, Acc: 0.8391, F1: 0.9041 Bal: 0.8243 - Val Loss: 0.5433, Accuracy: 0.8387, F1: 0.9040 Bal: 0.8231\n","           WALLETS: Train Loss: 0.57577443, Acc: 0.87602865, F1: 0.93017752 Bal: 0.7718 - Val Loss: 0.57623339, Accuracy: 0.8752, F1: 0.9296 Bal: 0.7696\n","Epoch 024:      TX: Train Loss: 0.5247, Acc: 0.8410, F1: 0.9054 Bal: 0.8257 - Val Loss: 0.5287, Accuracy: 0.8416, F1: 0.9058 Bal: 0.8256\n","           WALLETS: Train Loss: 0.56644422, Acc: 0.87470066, F1: 0.92932993 Bal: 0.7746 - Val Loss: 0.56689948, Accuracy: 0.8738, F1: 0.9287 Bal: 0.7715\n","Epoch 025:      TX: Train Loss: 0.5094, Acc: 0.8420, F1: 0.9060 Bal: 0.8273 - Val Loss: 0.5136, Accuracy: 0.8431, F1: 0.9068 Bal: 0.8265\n","           WALLETS: Train Loss: 0.55696964, Acc: 0.87369196, F1: 0.92869623 Bal: 0.7758 - Val Loss: 0.55744153, Accuracy: 0.8726, F1: 0.9280 Bal: 0.7721\n","Epoch 026:      TX: Train Loss: 0.4938, Acc: 0.8432, F1: 0.9068 Bal: 0.8289 - Val Loss: 0.4984, Accuracy: 0.8420, F1: 0.9061 Bal: 0.8269\n","           WALLETS: Train Loss: 0.54743785, Acc: 0.87289372, F1: 0.92819000 Bal: 0.7771 - Val Loss: 0.54794335, Accuracy: 0.8717, F1: 0.9275 Bal: 0.7723\n","Epoch 027:      TX: Train Loss: 0.4780, Acc: 0.8435, F1: 0.9069 Bal: 0.8297 - Val Loss: 0.4830, Accuracy: 0.8431, F1: 0.9068 Bal: 0.8275\n","           WALLETS: Train Loss: 0.53795415, Acc: 0.87274858, F1: 0.92809385 Bal: 0.7776 - Val Loss: 0.53850621, Accuracy: 0.8719, F1: 0.9276 Bal: 0.7729\n","Epoch 028:      TX: Train Loss: 0.4624, Acc: 0.8445, F1: 0.9075 Bal: 0.8316 - Val Loss: 0.4676, Accuracy: 0.8440, F1: 0.9073 Bal: 0.8280\n","           WALLETS: Train Loss: 0.52864212, Acc: 0.87271593, F1: 0.92806050 Bal: 0.7786 - Val Loss: 0.52924418, Accuracy: 0.8720, F1: 0.9276 Bal: 0.7751\n","Epoch 029:      TX: Train Loss: 0.4469, Acc: 0.8453, F1: 0.9080 Bal: 0.8338 - Val Loss: 0.4523, Accuracy: 0.8449, F1: 0.9078 Bal: 0.8314\n","           WALLETS: Train Loss: 0.51962751, Acc: 0.87336178, F1: 0.92838147 Bal: 0.7841 - Val Loss: 0.52027524, Accuracy: 0.8731, F1: 0.9282 Bal: 0.7820\n","Epoch 030:      TX: Train Loss: 0.4318, Acc: 0.8469, F1: 0.9090 Bal: 0.8368 - Val Loss: 0.4374, Accuracy: 0.8447, F1: 0.9077 Bal: 0.8313\n","           WALLETS: Train Loss: 0.51100981, Acc: 0.87310416, F1: 0.92820057 Bal: 0.7857 - Val Loss: 0.51168782, Accuracy: 0.8730, F1: 0.9281 Bal: 0.7838\n","Epoch 031:      TX: Train Loss: 0.4171, Acc: 0.8492, F1: 0.9104 Bal: 0.8396 - Val Loss: 0.4229, Accuracy: 0.8469, F1: 0.9091 Bal: 0.8325\n","           WALLETS: Train Loss: 0.50286216, Acc: 0.87160927, F1: 0.92727919 Bal: 0.7859 - Val Loss: 0.50355428, Accuracy: 0.8713, F1: 0.9270 Bal: 0.7842\n","Epoch 032:      TX: Train Loss: 0.4031, Acc: 0.8514, F1: 0.9118 Bal: 0.8415 - Val Loss: 0.4090, Accuracy: 0.8482, F1: 0.9100 Bal: 0.8332\n","           WALLETS: Train Loss: 0.49525982, Acc: 0.87070217, F1: 0.92669792 Bal: 0.7876 - Val Loss: 0.49594846, Accuracy: 0.8707, F1: 0.9266 Bal: 0.7863\n","Epoch 033:      TX: Train Loss: 0.3898, Acc: 0.8539, F1: 0.9134 Bal: 0.8442 - Val Loss: 0.3957, Accuracy: 0.8515, F1: 0.9121 Bal: 0.8360\n","           WALLETS: Train Loss: 0.48828048, Acc: 0.86933789, F1: 0.92582286 Bal: 0.7899 - Val Loss: 0.48894411, Accuracy: 0.8691, F1: 0.9256 Bal: 0.7887\n","Epoch 034:      TX: Train Loss: 0.3773, Acc: 0.8565, F1: 0.9151 Bal: 0.8464 - Val Loss: 0.3832, Accuracy: 0.8554, F1: 0.9146 Bal: 0.8392\n","           WALLETS: Train Loss: 0.48198807, Acc: 0.86789379, F1: 0.92491209 Bal: 0.7912 - Val Loss: 0.48261583, Accuracy: 0.8675, F1: 0.9246 Bal: 0.7903\n","Epoch 035:      TX: Train Loss: 0.3657, Acc: 0.8590, F1: 0.9167 Bal: 0.8489 - Val Loss: 0.3715, Accuracy: 0.8583, F1: 0.9164 Bal: 0.8418\n","           WALLETS: Train Loss: 0.47643125, Acc: 0.86569861, F1: 0.92353364 Bal: 0.7924 - Val Loss: 0.47701889, Accuracy: 0.8651, F1: 0.9231 Bal: 0.7917\n","Epoch 036:      TX: Train Loss: 0.3549, Acc: 0.8612, F1: 0.9180 Bal: 0.8512 - Val Loss: 0.3606, Accuracy: 0.8609, F1: 0.9180 Bal: 0.8472\n","           WALLETS: Train Loss: 0.47162297, Acc: 0.86414929, F1: 0.92252395 Bal: 0.7955 - Val Loss: 0.47217441, Accuracy: 0.8632, F1: 0.9219 Bal: 0.7930\n","Epoch 037:      TX: Train Loss: 0.3450, Acc: 0.8630, F1: 0.9191 Bal: 0.8548 - Val Loss: 0.3506, Accuracy: 0.8646, F1: 0.9203 Bal: 0.8522\n","           WALLETS: Train Loss: 0.46754128, Acc: 0.86183074, F1: 0.92106365 Bal: 0.7965 - Val Loss: 0.46807328, Accuracy: 0.8611, F1: 0.9205 Bal: 0.7940\n","Epoch 038:      TX: Train Loss: 0.3361, Acc: 0.8651, F1: 0.9204 Bal: 0.8574 - Val Loss: 0.3414, Accuracy: 0.8655, F1: 0.9209 Bal: 0.8527\n","           WALLETS: Train Loss: 0.46412605, Acc: 0.85873935, F1: 0.91913191 Bal: 0.7963 - Val Loss: 0.46466371, Accuracy: 0.8584, F1: 0.9188 Bal: 0.7940\n","Epoch 039:      TX: Train Loss: 0.3279, Acc: 0.8672, F1: 0.9217 Bal: 0.8604 - Val Loss: 0.3330, Accuracy: 0.8668, F1: 0.9216 Bal: 0.8563\n","           WALLETS: Train Loss: 0.46130088, Acc: 0.85692153, F1: 0.91799842 Bal: 0.7957 - Val Loss: 0.46186802, Accuracy: 0.8565, F1: 0.9176 Bal: 0.7934\n","Epoch 040:      TX: Train Loss: 0.3206, Acc: 0.8695, F1: 0.9231 Bal: 0.8637 - Val Loss: 0.3254, Accuracy: 0.8684, F1: 0.9226 Bal: 0.8592\n","           WALLETS: Train Loss: 0.45898220, Acc: 0.85535406, F1: 0.91701586 Bal: 0.7955 - Val Loss: 0.45959851, Accuracy: 0.8551, F1: 0.9168 Bal: 0.7935\n","Epoch 041:      TX: Train Loss: 0.3139, Acc: 0.8714, F1: 0.9243 Bal: 0.8665 - Val Loss: 0.3185, Accuracy: 0.8692, F1: 0.9231 Bal: 0.8626\n","           WALLETS: Train Loss: 0.45706773, Acc: 0.85416757, F1: 0.91627818 Bal: 0.7949 - Val Loss: 0.45774209, Accuracy: 0.8537, F1: 0.9159 Bal: 0.7925\n","Epoch 042:      TX: Train Loss: 0.3080, Acc: 0.8733, F1: 0.9255 Bal: 0.8689 - Val Loss: 0.3123, Accuracy: 0.8717, F1: 0.9245 Bal: 0.8659\n","           WALLETS: Train Loss: 0.45545101, Acc: 0.85274887, F1: 0.91539621 Bal: 0.7940 - Val Loss: 0.45618248, Accuracy: 0.8523, F1: 0.9150 Bal: 0.7921\n","Epoch 043:      TX: Train Loss: 0.3027, Acc: 0.8749, F1: 0.9265 Bal: 0.8707 - Val Loss: 0.3066, Accuracy: 0.8732, F1: 0.9256 Bal: 0.8648\n","           WALLETS: Train Loss: 0.45401701, Acc: 0.84949420, F1: 0.91336168 Bal: 0.7924 - Val Loss: 0.45479849, Accuracy: 0.8487, F1: 0.9128 Bal: 0.7903\n","Epoch 044:      TX: Train Loss: 0.2979, Acc: 0.8770, F1: 0.9278 Bal: 0.8727 - Val Loss: 0.3016, Accuracy: 0.8745, F1: 0.9264 Bal: 0.8655\n","           WALLETS: Train Loss: 0.45266512, Acc: 0.84967562, F1: 0.91334666 Bal: 0.8001 - Val Loss: 0.45348188, Accuracy: 0.8487, F1: 0.9127 Bal: 0.7968\n","Epoch 045:      TX: Train Loss: 0.2935, Acc: 0.8788, F1: 0.9289 Bal: 0.8748 - Val Loss: 0.2970, Accuracy: 0.8756, F1: 0.9271 Bal: 0.8661\n","           WALLETS: Train Loss: 0.45132062, Acc: 0.84711760, F1: 0.91172496 Bal: 0.7997 - Val Loss: 0.45215991, Accuracy: 0.8463, F1: 0.9111 Bal: 0.7965\n","Epoch 046:      TX: Train Loss: 0.2896, Acc: 0.8807, F1: 0.9301 Bal: 0.8771 - Val Loss: 0.2929, Accuracy: 0.8765, F1: 0.9276 Bal: 0.8676\n","           WALLETS: Train Loss: 0.44992453, Acc: 0.84666405, F1: 0.91142281 Bal: 0.8004 - Val Loss: 0.45077792, Accuracy: 0.8462, F1: 0.9110 Bal: 0.7974\n","Epoch 047:      TX: Train Loss: 0.2859, Acc: 0.8817, F1: 0.9307 Bal: 0.8782 - Val Loss: 0.2891, Accuracy: 0.8776, F1: 0.9283 Bal: 0.8682\n","           WALLETS: Train Loss: 0.44844016, Acc: 0.84599643, F1: 0.91100021 Bal: 0.8002 - Val Loss: 0.44930461, Accuracy: 0.8454, F1: 0.9105 Bal: 0.7968\n","Epoch 048:      TX: Train Loss: 0.2826, Acc: 0.8831, F1: 0.9316 Bal: 0.8800 - Val Loss: 0.2856, Accuracy: 0.8820, F1: 0.9310 Bal: 0.8746\n","           WALLETS: Train Loss: 0.44684234, Acc: 0.84545580, F1: 0.91065087 Bal: 0.8004 - Val Loss: 0.44772297, Accuracy: 0.8450, F1: 0.9103 Bal: 0.7973\n","Epoch 049:      TX: Train Loss: 0.2794, Acc: 0.8850, F1: 0.9327 Bal: 0.8813 - Val Loss: 0.2825, Accuracy: 0.8844, F1: 0.9325 Bal: 0.8759\n","           WALLETS: Train Loss: 0.44512001, Acc: 0.84525987, F1: 0.91052633 Bal: 0.8004 - Val Loss: 0.44602457, Accuracy: 0.8450, F1: 0.9103 Bal: 0.7976\n","Epoch 050:      TX: Train Loss: 0.2764, Acc: 0.8866, F1: 0.9338 Bal: 0.8830 - Val Loss: 0.2796, Accuracy: 0.8848, F1: 0.9327 Bal: 0.8781\n","           WALLETS: Train Loss: 0.44328132, Acc: 0.84576058, F1: 0.91084354 Bal: 0.8006 - Val Loss: 0.44421369, Accuracy: 0.8455, F1: 0.9106 Bal: 0.7978\n","Epoch 051:      TX: Train Loss: 0.2736, Acc: 0.8878, F1: 0.9344 Bal: 0.8840 - Val Loss: 0.2768, Accuracy: 0.8853, F1: 0.9330 Bal: 0.8784\n","           WALLETS: Train Loss: 0.44135129, Acc: 0.84663503, F1: 0.91139676 Bal: 0.8009 - Val Loss: 0.44231528, Accuracy: 0.8463, F1: 0.9111 Bal: 0.7983\n","Epoch 052:      TX: Train Loss: 0.2708, Acc: 0.8891, F1: 0.9352 Bal: 0.8858 - Val Loss: 0.2743, Accuracy: 0.8872, F1: 0.9342 Bal: 0.8804\n","           WALLETS: Train Loss: 0.43935943, Acc: 0.84791222, F1: 0.91220162 Bal: 0.8014 - Val Loss: 0.44034663, Accuracy: 0.8476, F1: 0.9119 Bal: 0.7986\n","Epoch 053:      TX: Train Loss: 0.2681, Acc: 0.8900, F1: 0.9358 Bal: 0.8867 - Val Loss: 0.2719, Accuracy: 0.8890, F1: 0.9353 Bal: 0.8824\n","           WALLETS: Train Loss: 0.43732581, Acc: 0.84923296, F1: 0.91304311 Bal: 0.8014 - Val Loss: 0.43832201, Accuracy: 0.8490, F1: 0.9129 Bal: 0.7984\n","Epoch 054:      TX: Train Loss: 0.2654, Acc: 0.8910, F1: 0.9364 Bal: 0.8879 - Val Loss: 0.2695, Accuracy: 0.8896, F1: 0.9357 Bal: 0.8818\n","           WALLETS: Train Loss: 0.43527660, Acc: 0.85059723, F1: 0.91390133 Bal: 0.8019 - Val Loss: 0.43626329, Accuracy: 0.8506, F1: 0.9138 Bal: 0.7992\n","Epoch 055:      TX: Train Loss: 0.2627, Acc: 0.8920, F1: 0.9370 Bal: 0.8885 - Val Loss: 0.2673, Accuracy: 0.8903, F1: 0.9361 Bal: 0.8812\n","           WALLETS: Train Loss: 0.43325263, Acc: 0.85198328, F1: 0.91476817 Bal: 0.8027 - Val Loss: 0.43421486, Accuracy: 0.8517, F1: 0.9145 Bal: 0.7998\n","Epoch 056:      TX: Train Loss: 0.2601, Acc: 0.8931, F1: 0.9377 Bal: 0.8896 - Val Loss: 0.2652, Accuracy: 0.8929, F1: 0.9377 Bal: 0.8836\n","           WALLETS: Train Loss: 0.43128055, Acc: 0.85357977, F1: 0.91576983 Bal: 0.8032 - Val Loss: 0.43220812, Accuracy: 0.8534, F1: 0.9156 Bal: 0.8004\n","Epoch 057:      TX: Train Loss: 0.2575, Acc: 0.8941, F1: 0.9383 Bal: 0.8903 - Val Loss: 0.2631, Accuracy: 0.8938, F1: 0.9383 Bal: 0.8831\n","           WALLETS: Train Loss: 0.42938602, Acc: 0.85553911, F1: 0.91698949 Bal: 0.8044 - Val Loss: 0.43027464, Accuracy: 0.8549, F1: 0.9165 Bal: 0.8009\n","Epoch 058:      TX: Train Loss: 0.2549, Acc: 0.8952, F1: 0.9390 Bal: 0.8916 - Val Loss: 0.2611, Accuracy: 0.8949, F1: 0.9390 Bal: 0.8837\n","           WALLETS: Train Loss: 0.42758635, Acc: 0.85812252, F1: 0.91859821 Bal: 0.8056 - Val Loss: 0.42843366, Accuracy: 0.8576, F1: 0.9182 Bal: 0.8024\n","Epoch 059:      TX: Train Loss: 0.2523, Acc: 0.8963, F1: 0.9396 Bal: 0.8928 - Val Loss: 0.2592, Accuracy: 0.8953, F1: 0.9393 Bal: 0.8830\n","           WALLETS: Train Loss: 0.42589176, Acc: 0.85982424, F1: 0.91965301 Bal: 0.8065 - Val Loss: 0.42669708, Accuracy: 0.8594, F1: 0.9194 Bal: 0.8032\n","Epoch 060:      TX: Train Loss: 0.2498, Acc: 0.8975, F1: 0.9404 Bal: 0.8936 - Val Loss: 0.2574, Accuracy: 0.8969, F1: 0.9402 Bal: 0.8848\n","           WALLETS: Train Loss: 0.42430815, Acc: 0.86087647, F1: 0.92030653 Bal: 0.8069 - Val Loss: 0.42506704, Accuracy: 0.8603, F1: 0.9199 Bal: 0.8034\n","Epoch 061:      TX: Train Loss: 0.2473, Acc: 0.8985, F1: 0.9410 Bal: 0.8942 - Val Loss: 0.2557, Accuracy: 0.8978, F1: 0.9407 Bal: 0.8853\n","           WALLETS: Train Loss: 0.42283133, Acc: 0.86193234, F1: 0.92096014 Bal: 0.8074 - Val Loss: 0.42353600, Accuracy: 0.8610, F1: 0.9204 Bal: 0.8036\n","Epoch 062:      TX: Train Loss: 0.2448, Acc: 0.8995, F1: 0.9416 Bal: 0.8956 - Val Loss: 0.2540, Accuracy: 0.8989, F1: 0.9414 Bal: 0.8869\n","           WALLETS: Train Loss: 0.42144167, Acc: 0.86336193, F1: 0.92183829 Bal: 0.8085 - Val Loss: 0.42207402, Accuracy: 0.8624, F1: 0.9212 Bal: 0.8040\n","Epoch 063:      TX: Train Loss: 0.2423, Acc: 0.9007, F1: 0.9423 Bal: 0.8968 - Val Loss: 0.2524, Accuracy: 0.9008, F1: 0.9426 Bal: 0.8870\n","           WALLETS: Train Loss: 0.42011929, Acc: 0.86474071, F1: 0.92269151 Bal: 0.8090 - Val Loss: 0.42066148, Accuracy: 0.8640, F1: 0.9222 Bal: 0.8047\n","Epoch 064:      TX: Train Loss: 0.2398, Acc: 0.9019, F1: 0.9431 Bal: 0.8979 - Val Loss: 0.2509, Accuracy: 0.9022, F1: 0.9434 Bal: 0.8887\n","           WALLETS: Train Loss: 0.41883817, Acc: 0.86535391, F1: 0.92306735 Bal: 0.8094 - Val Loss: 0.41927791, Accuracy: 0.8649, F1: 0.9228 Bal: 0.8054\n","Epoch 065:      TX: Train Loss: 0.2374, Acc: 0.9032, F1: 0.9439 Bal: 0.8990 - Val Loss: 0.2494, Accuracy: 0.9032, F1: 0.9441 Bal: 0.8893\n","           WALLETS: Train Loss: 0.41757399, Acc: 0.86559339, F1: 0.92321596 Bal: 0.8095 - Val Loss: 0.41790909, Accuracy: 0.8651, F1: 0.9229 Bal: 0.8058\n","Epoch 066:      TX: Train Loss: 0.2350, Acc: 0.9041, F1: 0.9444 Bal: 0.8998 - Val Loss: 0.2479, Accuracy: 0.9041, F1: 0.9446 Bal: 0.8888\n","           WALLETS: Train Loss: 0.41630754, Acc: 0.86591631, F1: 0.92341521 Bal: 0.8096 - Val Loss: 0.41654286, Accuracy: 0.8654, F1: 0.9230 Bal: 0.8056\n","Epoch 067:      TX: Train Loss: 0.2326, Acc: 0.9055, F1: 0.9453 Bal: 0.9009 - Val Loss: 0.2464, Accuracy: 0.9048, F1: 0.9450 Bal: 0.8892\n","           WALLETS: Train Loss: 0.41502631, Acc: 0.86656217, F1: 0.92381031 Bal: 0.8101 - Val Loss: 0.41516608, Accuracy: 0.8656, F1: 0.9232 Bal: 0.8059\n","Epoch 068:      TX: Train Loss: 0.2302, Acc: 0.9067, F1: 0.9460 Bal: 0.9020 - Val Loss: 0.2450, Accuracy: 0.9054, F1: 0.9454 Bal: 0.8896\n","           WALLETS: Train Loss: 0.41373035, Acc: 0.86696492, F1: 0.92406120 Bal: 0.8100 - Val Loss: 0.41378173, Accuracy: 0.8657, F1: 0.9233 Bal: 0.8058\n","Epoch 069:      TX: Train Loss: 0.2279, Acc: 0.9076, F1: 0.9466 Bal: 0.9026 - Val Loss: 0.2436, Accuracy: 0.9072, F1: 0.9465 Bal: 0.8905\n","           WALLETS: Train Loss: 0.41242090, Acc: 0.86722617, F1: 0.92421911 Bal: 0.8104 - Val Loss: 0.41239110, Accuracy: 0.8663, F1: 0.9236 Bal: 0.8066\n","Epoch 070:      TX: Train Loss: 0.2256, Acc: 0.9094, F1: 0.9476 Bal: 0.9043 - Val Loss: 0.2421, Accuracy: 0.9083, F1: 0.9472 Bal: 0.8912\n","           WALLETS: Train Loss: 0.41109902, Acc: 0.86737130, F1: 0.92430602 Bal: 0.8106 - Val Loss: 0.41099110, Accuracy: 0.8663, F1: 0.9236 Bal: 0.8066\n","Epoch 071:      TX: Train Loss: 0.2233, Acc: 0.9106, F1: 0.9484 Bal: 0.9047 - Val Loss: 0.2407, Accuracy: 0.9107, F1: 0.9486 Bal: 0.8915\n","           WALLETS: Train Loss: 0.40977108, Acc: 0.86729148, F1: 0.92425545 Bal: 0.8106 - Val Loss: 0.40958998, Accuracy: 0.8664, F1: 0.9236 Bal: 0.8073\n","Epoch 072:      TX: Train Loss: 0.2210, Acc: 0.9119, F1: 0.9491 Bal: 0.9066 - Val Loss: 0.2392, Accuracy: 0.9127, F1: 0.9498 Bal: 0.8936\n","           WALLETS: Train Loss: 0.40844491, Acc: 0.86725519, F1: 0.92423191 Bal: 0.8107 - Val Loss: 0.40819755, Accuracy: 0.8663, F1: 0.9236 Bal: 0.8076\n","Epoch 073:      TX: Train Loss: 0.2187, Acc: 0.9133, F1: 0.9500 Bal: 0.9080 - Val Loss: 0.2377, Accuracy: 0.9140, F1: 0.9506 Bal: 0.8963\n","           WALLETS: Train Loss: 0.40712795, Acc: 0.86702660, F1: 0.92408870 Bal: 0.8108 - Val Loss: 0.40682441, Accuracy: 0.8663, F1: 0.9236 Bal: 0.8079\n","Epoch 074:      TX: Train Loss: 0.2164, Acc: 0.9148, F1: 0.9509 Bal: 0.9093 - Val Loss: 0.2362, Accuracy: 0.9151, F1: 0.9512 Bal: 0.8969\n","           WALLETS: Train Loss: 0.40582615, Acc: 0.86707014, F1: 0.92411105 Bal: 0.8111 - Val Loss: 0.40548384, Accuracy: 0.8663, F1: 0.9236 Bal: 0.8083\n","Epoch 075:      TX: Train Loss: 0.2141, Acc: 0.9162, F1: 0.9517 Bal: 0.9101 - Val Loss: 0.2347, Accuracy: 0.9151, F1: 0.9512 Bal: 0.8979\n","           WALLETS: Train Loss: 0.40454566, Acc: 0.86731687, F1: 0.92425818 Bal: 0.8115 - Val Loss: 0.40417808, Accuracy: 0.8665, F1: 0.9237 Bal: 0.8083\n","Epoch 076:      TX: Train Loss: 0.2118, Acc: 0.9173, F1: 0.9524 Bal: 0.9114 - Val Loss: 0.2331, Accuracy: 0.9153, F1: 0.9514 Bal: 0.8980\n","           WALLETS: Train Loss: 0.40328798, Acc: 0.86777405, F1: 0.92453979 Bal: 0.8117 - Val Loss: 0.40290543, Accuracy: 0.8668, F1: 0.9239 Bal: 0.8086\n","Epoch 077:      TX: Train Loss: 0.2096, Acc: 0.9189, F1: 0.9533 Bal: 0.9128 - Val Loss: 0.2316, Accuracy: 0.9151, F1: 0.9512 Bal: 0.8979\n","           WALLETS: Train Loss: 0.40205336, Acc: 0.86817318, F1: 0.92478750 Bal: 0.8117 - Val Loss: 0.40165690, Accuracy: 0.8673, F1: 0.9242 Bal: 0.8087\n","Epoch 078:      TX: Train Loss: 0.2073, Acc: 0.9201, F1: 0.9540 Bal: 0.9140 - Val Loss: 0.2300, Accuracy: 0.9164, F1: 0.9520 Bal: 0.9006\n","           WALLETS: Train Loss: 0.40083936, Acc: 0.86853964, F1: 0.92501133 Bal: 0.8120 - Val Loss: 0.40042508, Accuracy: 0.8678, F1: 0.9245 Bal: 0.8087\n","Epoch 079:      TX: Train Loss: 0.2050, Acc: 0.9217, F1: 0.9550 Bal: 0.9156 - Val Loss: 0.2285, Accuracy: 0.9175, F1: 0.9526 Bal: 0.9022\n","           WALLETS: Train Loss: 0.39964172, Acc: 0.86873921, F1: 0.92513400 Bal: 0.8121 - Val Loss: 0.39920455, Accuracy: 0.8680, F1: 0.9246 Bal: 0.8091\n","Epoch 080:      TX: Train Loss: 0.2028, Acc: 0.9229, F1: 0.9557 Bal: 0.9164 - Val Loss: 0.2269, Accuracy: 0.9193, F1: 0.9537 Bal: 0.9031\n","           WALLETS: Train Loss: 0.39845970, Acc: 0.86906576, F1: 0.92533169 Bal: 0.8124 - Val Loss: 0.39799747, Accuracy: 0.8684, F1: 0.9249 Bal: 0.8095\n","Epoch 081:      TX: Train Loss: 0.2005, Acc: 0.9239, F1: 0.9563 Bal: 0.9175 - Val Loss: 0.2254, Accuracy: 0.9212, F1: 0.9549 Bal: 0.9042\n","           WALLETS: Train Loss: 0.39729038, Acc: 0.86946488, F1: 0.92557839 Bal: 0.8125 - Val Loss: 0.39680532, Accuracy: 0.8689, F1: 0.9252 Bal: 0.8093\n","Epoch 082:      TX: Train Loss: 0.1983, Acc: 0.9250, F1: 0.9570 Bal: 0.9184 - Val Loss: 0.2239, Accuracy: 0.9221, F1: 0.9554 Bal: 0.9037\n","           WALLETS: Train Loss: 0.39613119, Acc: 0.87009985, F1: 0.92596813 Bal: 0.8128 - Val Loss: 0.39562377, Accuracy: 0.8693, F1: 0.9254 Bal: 0.8090\n","Epoch 083:      TX: Train Loss: 0.1961, Acc: 0.9265, F1: 0.9579 Bal: 0.9198 - Val Loss: 0.2225, Accuracy: 0.9239, F1: 0.9565 Bal: 0.9037\n","           WALLETS: Train Loss: 0.39498237, Acc: 0.87068402, F1: 0.92632802 Bal: 0.8129 - Val Loss: 0.39445069, Accuracy: 0.8700, F1: 0.9259 Bal: 0.8086\n","Epoch 084:      TX: Train Loss: 0.1939, Acc: 0.9276, F1: 0.9585 Bal: 0.9210 - Val Loss: 0.2211, Accuracy: 0.9243, F1: 0.9567 Bal: 0.9040\n","           WALLETS: Train Loss: 0.39384431, Acc: 0.87128634, F1: 0.92669480 Bal: 0.8134 - Val Loss: 0.39328718, Accuracy: 0.8709, F1: 0.9264 Bal: 0.8094\n","Epoch 085:      TX: Train Loss: 0.1916, Acc: 0.9287, F1: 0.9592 Bal: 0.9217 - Val Loss: 0.2197, Accuracy: 0.9258, F1: 0.9577 Bal: 0.9048\n","           WALLETS: Train Loss: 0.39271897, Acc: 0.87183060, F1: 0.92702678 Bal: 0.8137 - Val Loss: 0.39213523, Accuracy: 0.8714, F1: 0.9267 Bal: 0.8100\n","Epoch 086:      TX: Train Loss: 0.1894, Acc: 0.9299, F1: 0.9599 Bal: 0.9235 - Val Loss: 0.2183, Accuracy: 0.9265, F1: 0.9580 Bal: 0.9052\n","           WALLETS: Train Loss: 0.39159855, Acc: 0.87233132, F1: 0.92733198 Bal: 0.8140 - Val Loss: 0.39099476, Accuracy: 0.8716, F1: 0.9268 Bal: 0.8104\n","Epoch 087:      TX: Train Loss: 0.1872, Acc: 0.9311, F1: 0.9606 Bal: 0.9248 - Val Loss: 0.2170, Accuracy: 0.9272, F1: 0.9584 Bal: 0.9056\n","           WALLETS: Train Loss: 0.39047939, Acc: 0.87268690, F1: 0.92754515 Bal: 0.8145 - Val Loss: 0.38986689, Accuracy: 0.8720, F1: 0.9271 Bal: 0.8106\n","Epoch 088:      TX: Train Loss: 0.1850, Acc: 0.9323, F1: 0.9613 Bal: 0.9259 - Val Loss: 0.2157, Accuracy: 0.9280, F1: 0.9590 Bal: 0.9060\n","           WALLETS: Train Loss: 0.38935998, Acc: 0.87309691, F1: 0.92779295 Bal: 0.8149 - Val Loss: 0.38875434, Accuracy: 0.8723, F1: 0.9273 Bal: 0.8105\n","Epoch 089:      TX: Train Loss: 0.1828, Acc: 0.9334, F1: 0.9619 Bal: 0.9271 - Val Loss: 0.2144, Accuracy: 0.9280, F1: 0.9590 Bal: 0.9060\n","           WALLETS: Train Loss: 0.38824201, Acc: 0.87365205, F1: 0.92812797 Bal: 0.8154 - Val Loss: 0.38766101, Accuracy: 0.8725, F1: 0.9274 Bal: 0.8107\n","Epoch 090:      TX: Train Loss: 0.1806, Acc: 0.9347, F1: 0.9627 Bal: 0.9282 - Val Loss: 0.2132, Accuracy: 0.9285, F1: 0.9592 Bal: 0.9063\n","           WALLETS: Train Loss: 0.38712552, Acc: 0.87476234, F1: 0.92880510 Bal: 0.8160 - Val Loss: 0.38658181, Accuracy: 0.8732, F1: 0.9278 Bal: 0.8116\n","Epoch 091:      TX: Train Loss: 0.1783, Acc: 0.9358, F1: 0.9633 Bal: 0.9293 - Val Loss: 0.2120, Accuracy: 0.9291, F1: 0.9596 Bal: 0.9067\n","           WALLETS: Train Loss: 0.38601190, Acc: 0.87509252, F1: 0.92899676 Bal: 0.8169 - Val Loss: 0.38551497, Accuracy: 0.8733, F1: 0.9279 Bal: 0.8123\n","Epoch 092:      TX: Train Loss: 0.1761, Acc: 0.9366, F1: 0.9638 Bal: 0.9305 - Val Loss: 0.2108, Accuracy: 0.9298, F1: 0.9600 Bal: 0.9070\n","           WALLETS: Train Loss: 0.38490602, Acc: 0.87521952, F1: 0.92906997 Bal: 0.8172 - Val Loss: 0.38446185, Accuracy: 0.8735, F1: 0.9280 Bal: 0.8122\n","Epoch 093:      TX: Train Loss: 0.1740, Acc: 0.9380, F1: 0.9647 Bal: 0.9320 - Val Loss: 0.2097, Accuracy: 0.9309, F1: 0.9606 Bal: 0.9076\n","           WALLETS: Train Loss: 0.38380909, Acc: 0.87544448, F1: 0.92920339 Bal: 0.8176 - Val Loss: 0.38342190, Accuracy: 0.8736, F1: 0.9281 Bal: 0.8122\n","Epoch 094:      TX: Train Loss: 0.1718, Acc: 0.9388, F1: 0.9651 Bal: 0.9332 - Val Loss: 0.2086, Accuracy: 0.9324, F1: 0.9616 Bal: 0.9085\n","           WALLETS: Train Loss: 0.38272077, Acc: 0.87571298, F1: 0.92935747 Bal: 0.8185 - Val Loss: 0.38238886, Accuracy: 0.8741, F1: 0.9283 Bal: 0.8129\n","Epoch 095:      TX: Train Loss: 0.1696, Acc: 0.9399, F1: 0.9658 Bal: 0.9344 - Val Loss: 0.2075, Accuracy: 0.9337, F1: 0.9623 Bal: 0.9092\n","           WALLETS: Train Loss: 0.38163662, Acc: 0.87586900, F1: 0.92944833 Bal: 0.8188 - Val Loss: 0.38135535, Accuracy: 0.8745, F1: 0.9286 Bal: 0.8139\n","Epoch 096:      TX: Train Loss: 0.1674, Acc: 0.9410, F1: 0.9664 Bal: 0.9356 - Val Loss: 0.2065, Accuracy: 0.9337, F1: 0.9623 Bal: 0.9092\n","           WALLETS: Train Loss: 0.38055980, Acc: 0.87605405, F1: 0.92956033 Bal: 0.8190 - Val Loss: 0.38031903, Accuracy: 0.8744, F1: 0.9285 Bal: 0.8134\n","Epoch 097:      TX: Train Loss: 0.1653, Acc: 0.9422, F1: 0.9671 Bal: 0.9369 - Val Loss: 0.2055, Accuracy: 0.9342, F1: 0.9626 Bal: 0.9095\n","           WALLETS: Train Loss: 0.37949008, Acc: 0.87619193, F1: 0.92963491 Bal: 0.8197 - Val Loss: 0.37927756, Accuracy: 0.8744, F1: 0.9285 Bal: 0.8134\n","Epoch 098:      TX: Train Loss: 0.1632, Acc: 0.9432, F1: 0.9677 Bal: 0.9384 - Val Loss: 0.2045, Accuracy: 0.9353, F1: 0.9632 Bal: 0.9101\n","           WALLETS: Train Loss: 0.37841651, Acc: 0.87633706, F1: 0.92971334 Bal: 0.8205 - Val Loss: 0.37822640, Accuracy: 0.8744, F1: 0.9285 Bal: 0.8135\n","Epoch 099:      TX: Train Loss: 0.1611, Acc: 0.9441, F1: 0.9682 Bal: 0.9393 - Val Loss: 0.2035, Accuracy: 0.9364, F1: 0.9639 Bal: 0.9117\n","           WALLETS: Train Loss: 0.37733912, Acc: 0.87658742, F1: 0.92985955 Bal: 0.8211 - Val Loss: 0.37717026, Accuracy: 0.8746, F1: 0.9286 Bal: 0.8146\n","Epoch 100:      TX: Train Loss: 0.1590, Acc: 0.9450, F1: 0.9687 Bal: 0.9408 - Val Loss: 0.2026, Accuracy: 0.9381, F1: 0.9649 Bal: 0.9136\n","           WALLETS: Train Loss: 0.37625989, Acc: 0.87669265, F1: 0.92991631 Bal: 0.8217 - Val Loss: 0.37611550, Accuracy: 0.8748, F1: 0.9287 Bal: 0.8158\n","Epoch 101:      TX: Train Loss: 0.1569, Acc: 0.9457, F1: 0.9691 Bal: 0.9421 - Val Loss: 0.2017, Accuracy: 0.9388, F1: 0.9653 Bal: 0.9140\n","           WALLETS: Train Loss: 0.37517726, Acc: 0.87674344, F1: 0.92993998 Bal: 0.8223 - Val Loss: 0.37505907, Accuracy: 0.8744, F1: 0.9285 Bal: 0.8167\n","Epoch 102:      TX: Train Loss: 0.1549, Acc: 0.9465, F1: 0.9696 Bal: 0.9436 - Val Loss: 0.2008, Accuracy: 0.9388, F1: 0.9653 Bal: 0.9130\n","           WALLETS: Train Loss: 0.37408713, Acc: 0.87671442, F1: 0.92991134 Bal: 0.8231 - Val Loss: 0.37399840, Accuracy: 0.8746, F1: 0.9286 Bal: 0.8180\n","Epoch 103:      TX: Train Loss: 0.1528, Acc: 0.9475, F1: 0.9701 Bal: 0.9444 - Val Loss: 0.2000, Accuracy: 0.9388, F1: 0.9653 Bal: 0.9130\n","           WALLETS: Train Loss: 0.37299174, Acc: 0.87670716, F1: 0.92989333 Bal: 0.8241 - Val Loss: 0.37294638, Accuracy: 0.8747, F1: 0.9286 Bal: 0.8190\n","Epoch 104:      TX: Train Loss: 0.1508, Acc: 0.9483, F1: 0.9706 Bal: 0.9451 - Val Loss: 0.1992, Accuracy: 0.9397, F1: 0.9658 Bal: 0.9135\n","           WALLETS: Train Loss: 0.37189093, Acc: 0.87677247, F1: 0.92992556 Bal: 0.8247 - Val Loss: 0.37190431, Accuracy: 0.8747, F1: 0.9286 Bal: 0.8195\n","Epoch 105:      TX: Train Loss: 0.1488, Acc: 0.9491, F1: 0.9711 Bal: 0.9461 - Val Loss: 0.1984, Accuracy: 0.9401, F1: 0.9661 Bal: 0.9137\n","           WALLETS: Train Loss: 0.37078637, Acc: 0.87690672, F1: 0.92999973 Bal: 0.8253 - Val Loss: 0.37086967, Accuracy: 0.8747, F1: 0.9286 Bal: 0.8198\n","Epoch 106:      TX: Train Loss: 0.1468, Acc: 0.9496, F1: 0.9714 Bal: 0.9467 - Val Loss: 0.1978, Accuracy: 0.9410, F1: 0.9666 Bal: 0.9142\n","           WALLETS: Train Loss: 0.36968017, Acc: 0.87690672, F1: 0.92998615 Bal: 0.8263 - Val Loss: 0.36983740, Accuracy: 0.8747, F1: 0.9286 Bal: 0.8207\n","Epoch 107:      TX: Train Loss: 0.1448, Acc: 0.9506, F1: 0.9720 Bal: 0.9484 - Val Loss: 0.1971, Accuracy: 0.9412, F1: 0.9667 Bal: 0.9143\n","           WALLETS: Train Loss: 0.36856943, Acc: 0.87686681, F1: 0.92995521 Bal: 0.8268 - Val Loss: 0.36880666, Accuracy: 0.8745, F1: 0.9285 Bal: 0.8212\n","Epoch 108:      TX: Train Loss: 0.1429, Acc: 0.9511, F1: 0.9722 Bal: 0.9491 - Val Loss: 0.1965, Accuracy: 0.9421, F1: 0.9672 Bal: 0.9158\n","           WALLETS: Train Loss: 0.36745119, Acc: 0.87699743, F1: 0.93002720 Bal: 0.8274 - Val Loss: 0.36777765, Accuracy: 0.8745, F1: 0.9284 Bal: 0.8217\n","Epoch 109:      TX: Train Loss: 0.1409, Acc: 0.9520, F1: 0.9727 Bal: 0.9503 - Val Loss: 0.1959, Accuracy: 0.9425, F1: 0.9675 Bal: 0.9160\n","           WALLETS: Train Loss: 0.36633059, Acc: 0.87695026, F1: 0.92998664 Bal: 0.8283 - Val Loss: 0.36674926, Accuracy: 0.8742, F1: 0.9283 Bal: 0.8221\n","Epoch 110:      TX: Train Loss: 0.1390, Acc: 0.9527, F1: 0.9732 Bal: 0.9512 - Val Loss: 0.1954, Accuracy: 0.9427, F1: 0.9676 Bal: 0.9152\n","           WALLETS: Train Loss: 0.36521038, Acc: 0.87703372, F1: 0.93002618 Bal: 0.8292 - Val Loss: 0.36571071, Accuracy: 0.8745, F1: 0.9284 Bal: 0.8235\n","Epoch 111:      TX: Train Loss: 0.1371, Acc: 0.9533, F1: 0.9735 Bal: 0.9518 - Val Loss: 0.1949, Accuracy: 0.9434, F1: 0.9680 Bal: 0.9146\n","           WALLETS: Train Loss: 0.36408743, Acc: 0.87699017, F1: 0.92998666 Bal: 0.8301 - Val Loss: 0.36466071, Accuracy: 0.8748, F1: 0.9286 Bal: 0.8251\n","Epoch 112:      TX: Train Loss: 0.1352, Acc: 0.9540, F1: 0.9739 Bal: 0.9526 - Val Loss: 0.1944, Accuracy: 0.9443, F1: 0.9685 Bal: 0.9151\n","           WALLETS: Train Loss: 0.36295968, Acc: 0.87701920, F1: 0.92999740 Bal: 0.8306 - Val Loss: 0.36360973, Accuracy: 0.8746, F1: 0.9285 Bal: 0.8255\n","Epoch 113:      TX: Train Loss: 0.1334, Acc: 0.9545, F1: 0.9742 Bal: 0.9530 - Val Loss: 0.1940, Accuracy: 0.9445, F1: 0.9686 Bal: 0.9152\n","           WALLETS: Train Loss: 0.36183009, Acc: 0.87710991, F1: 0.93004369 Bal: 0.8313 - Val Loss: 0.36256200, Accuracy: 0.8747, F1: 0.9285 Bal: 0.8268\n","Epoch 114:      TX: Train Loss: 0.1316, Acc: 0.9548, F1: 0.9744 Bal: 0.9536 - Val Loss: 0.1936, Accuracy: 0.9447, F1: 0.9688 Bal: 0.9143\n","           WALLETS: Train Loss: 0.36069858, Acc: 0.87699743, F1: 0.92997083 Bal: 0.8316 - Val Loss: 0.36151224, Accuracy: 0.8748, F1: 0.9286 Bal: 0.8274\n","Epoch 115:      TX: Train Loss: 0.1298, Acc: 0.9554, F1: 0.9747 Bal: 0.9545 - Val Loss: 0.1932, Accuracy: 0.9452, F1: 0.9690 Bal: 0.9146\n","           WALLETS: Train Loss: 0.35956800, Acc: 0.87691761, F1: 0.92991380 Bal: 0.8322 - Val Loss: 0.36046675, Accuracy: 0.8748, F1: 0.9286 Bal: 0.8274\n","Epoch 116:      TX: Train Loss: 0.1280, Acc: 0.9561, F1: 0.9751 Bal: 0.9551 - Val Loss: 0.1928, Accuracy: 0.9454, F1: 0.9692 Bal: 0.9147\n","           WALLETS: Train Loss: 0.35843298, Acc: 0.87692123, F1: 0.92990906 Bal: 0.8327 - Val Loss: 0.35942954, Accuracy: 0.8748, F1: 0.9285 Bal: 0.8282\n","Epoch 117:      TX: Train Loss: 0.1262, Acc: 0.9570, F1: 0.9756 Bal: 0.9563 - Val Loss: 0.1925, Accuracy: 0.9458, F1: 0.9694 Bal: 0.9159\n","           WALLETS: Train Loss: 0.35729426, Acc: 0.87681238, F1: 0.92983634 Bal: 0.8331 - Val Loss: 0.35839057, Accuracy: 0.8745, F1: 0.9284 Bal: 0.8285\n","Epoch 118:      TX: Train Loss: 0.1245, Acc: 0.9571, F1: 0.9757 Bal: 0.9565 - Val Loss: 0.1922, Accuracy: 0.9462, F1: 0.9697 Bal: 0.9152\n","           WALLETS: Train Loss: 0.35615182, Acc: 0.87687407, F1: 0.92986177 Bal: 0.8341 - Val Loss: 0.35734656, Accuracy: 0.8748, F1: 0.9285 Bal: 0.8304\n","Epoch 119:      TX: Train Loss: 0.1228, Acc: 0.9581, F1: 0.9763 Bal: 0.9581 - Val Loss: 0.1920, Accuracy: 0.9471, F1: 0.9702 Bal: 0.9157\n","           WALLETS: Train Loss: 0.35500586, Acc: 0.87693212, F1: 0.92988672 Bal: 0.8349 - Val Loss: 0.35630193, Accuracy: 0.8751, F1: 0.9287 Bal: 0.8314\n","Epoch 120:      TX: Train Loss: 0.1211, Acc: 0.9587, F1: 0.9766 Bal: 0.9587 - Val Loss: 0.1918, Accuracy: 0.9480, F1: 0.9707 Bal: 0.9171\n","           WALLETS: Train Loss: 0.35385224, Acc: 0.87686318, F1: 0.92983918 Bal: 0.8352 - Val Loss: 0.35525599, Accuracy: 0.8750, F1: 0.9286 Bal: 0.8318\n","Epoch 121:      TX: Train Loss: 0.1195, Acc: 0.9594, F1: 0.9770 Bal: 0.9594 - Val Loss: 0.1917, Accuracy: 0.9480, F1: 0.9707 Bal: 0.9161\n","           WALLETS: Train Loss: 0.35269359, Acc: 0.87692123, F1: 0.92986791 Bal: 0.8358 - Val Loss: 0.35420203, Accuracy: 0.8754, F1: 0.9288 Bal: 0.8334\n","Epoch 122:      TX: Train Loss: 0.1178, Acc: 0.9601, F1: 0.9774 Bal: 0.9601 - Val Loss: 0.1916, Accuracy: 0.9480, F1: 0.9707 Bal: 0.9152\n","           WALLETS: Train Loss: 0.35153440, Acc: 0.87681964, F1: 0.92979521 Bal: 0.8365 - Val Loss: 0.35313883, Accuracy: 0.8754, F1: 0.9288 Bal: 0.8344\n","Epoch 123:      TX: Train Loss: 0.1162, Acc: 0.9605, F1: 0.9777 Bal: 0.9607 - Val Loss: 0.1915, Accuracy: 0.9482, F1: 0.9708 Bal: 0.9143\n","           WALLETS: Train Loss: 0.35038358, Acc: 0.87683415, F1: 0.92979651 Bal: 0.8371 - Val Loss: 0.35208696, Accuracy: 0.8754, F1: 0.9288 Bal: 0.8351\n","Epoch 124:      TX: Train Loss: 0.1147, Acc: 0.9608, F1: 0.9779 Bal: 0.9616 - Val Loss: 0.1915, Accuracy: 0.9484, F1: 0.9710 Bal: 0.9144\n","           WALLETS: Train Loss: 0.34924099, Acc: 0.87695389, F1: 0.92986317 Bal: 0.8376 - Val Loss: 0.35104838, Accuracy: 0.8756, F1: 0.9289 Bal: 0.8355\n","Epoch 125:      TX: Train Loss: 0.1131, Acc: 0.9613, F1: 0.9781 Bal: 0.9621 - Val Loss: 0.1915, Accuracy: 0.9491, F1: 0.9713 Bal: 0.9148\n","           WALLETS: Train Loss: 0.34810379, Acc: 0.87703009, F1: 0.92989892 Bal: 0.8384 - Val Loss: 0.35001457, Accuracy: 0.8753, F1: 0.9287 Bal: 0.8352\n","Epoch 126:      TX: Train Loss: 0.1116, Acc: 0.9620, F1: 0.9785 Bal: 0.9626 - Val Loss: 0.1916, Accuracy: 0.9498, F1: 0.9717 Bal: 0.9161\n","           WALLETS: Train Loss: 0.34697005, Acc: 0.87710628, F1: 0.92993670 Bal: 0.8391 - Val Loss: 0.34897876, Accuracy: 0.8754, F1: 0.9288 Bal: 0.8364\n","Epoch 127:      TX: Train Loss: 0.1101, Acc: 0.9624, F1: 0.9787 Bal: 0.9630 - Val Loss: 0.1917, Accuracy: 0.9500, F1: 0.9718 Bal: 0.9172\n","           WALLETS: Train Loss: 0.34584340, Acc: 0.87701557, F1: 0.92987585 Bal: 0.8395 - Val Loss: 0.34794852, Accuracy: 0.8755, F1: 0.9289 Bal: 0.8369\n","Epoch 128:      TX: Train Loss: 0.1086, Acc: 0.9626, F1: 0.9788 Bal: 0.9633 - Val Loss: 0.1918, Accuracy: 0.9500, F1: 0.9718 Bal: 0.9172\n","           WALLETS: Train Loss: 0.34472358, Acc: 0.87727319, F1: 0.93002230 Bal: 0.8404 - Val Loss: 0.34693059, Accuracy: 0.8758, F1: 0.9290 Bal: 0.8377\n","Epoch 129:      TX: Train Loss: 0.1072, Acc: 0.9630, F1: 0.9791 Bal: 0.9642 - Val Loss: 0.1919, Accuracy: 0.9506, F1: 0.9722 Bal: 0.9176\n","           WALLETS: Train Loss: 0.34360343, Acc: 0.87724779, F1: 0.93000130 Bal: 0.8408 - Val Loss: 0.34590855, Accuracy: 0.8757, F1: 0.9290 Bal: 0.8382\n","Epoch 130:      TX: Train Loss: 0.1057, Acc: 0.9635, F1: 0.9794 Bal: 0.9645 - Val Loss: 0.1922, Accuracy: 0.9513, F1: 0.9726 Bal: 0.9180\n","           WALLETS: Train Loss: 0.34248298, Acc: 0.87724779, F1: 0.92999348 Bal: 0.8414 - Val Loss: 0.34489229, Accuracy: 0.8759, F1: 0.9291 Bal: 0.8392\n","Epoch 131:      TX: Train Loss: 0.1043, Acc: 0.9642, F1: 0.9798 Bal: 0.9657 - Val Loss: 0.1925, Accuracy: 0.9515, F1: 0.9727 Bal: 0.9181\n","           WALLETS: Train Loss: 0.34136805, Acc: 0.87740018, F1: 0.93007837 Bal: 0.8421 - Val Loss: 0.34388825, Accuracy: 0.8764, F1: 0.9294 Bal: 0.8400\n","Epoch 132:      TX: Train Loss: 0.1029, Acc: 0.9647, F1: 0.9800 Bal: 0.9666 - Val Loss: 0.1927, Accuracy: 0.9522, F1: 0.9731 Bal: 0.9185\n","           WALLETS: Train Loss: 0.34025830, Acc: 0.87766868, F1: 0.93023468 Bal: 0.8428 - Val Loss: 0.34289229, Accuracy: 0.8771, F1: 0.9298 Bal: 0.8409\n","Epoch 133:      TX: Train Loss: 0.1016, Acc: 0.9650, F1: 0.9803 Bal: 0.9672 - Val Loss: 0.1929, Accuracy: 0.9522, F1: 0.9731 Bal: 0.9185\n","           WALLETS: Train Loss: 0.33915180, Acc: 0.87764329, F1: 0.93020994 Bal: 0.8434 - Val Loss: 0.34190351, Accuracy: 0.8771, F1: 0.9298 Bal: 0.8414\n","Epoch 134:      TX: Train Loss: 0.1002, Acc: 0.9657, F1: 0.9806 Bal: 0.9678 - Val Loss: 0.1933, Accuracy: 0.9526, F1: 0.9733 Bal: 0.9197\n","           WALLETS: Train Loss: 0.33804706, Acc: 0.87786099, F1: 0.93033584 Bal: 0.8441 - Val Loss: 0.34093201, Accuracy: 0.8774, F1: 0.9300 Bal: 0.8422\n","Epoch 135:      TX: Train Loss: 0.0989, Acc: 0.9662, F1: 0.9809 Bal: 0.9684 - Val Loss: 0.1937, Accuracy: 0.9535, F1: 0.9739 Bal: 0.9202\n","           WALLETS: Train Loss: 0.33695021, Acc: 0.87791904, F1: 0.93035339 Bal: 0.8454 - Val Loss: 0.33996031, Accuracy: 0.8772, F1: 0.9298 Bal: 0.8433\n","Epoch 136:      TX: Train Loss: 0.0976, Acc: 0.9665, F1: 0.9811 Bal: 0.9690 - Val Loss: 0.1941, Accuracy: 0.9541, F1: 0.9742 Bal: 0.9205\n","           WALLETS: Train Loss: 0.33585626, Acc: 0.87846330, F1: 0.93067020 Bal: 0.8469 - Val Loss: 0.33899447, Accuracy: 0.8781, F1: 0.9304 Bal: 0.8454\n","Epoch 137:      TX: Train Loss: 0.0963, Acc: 0.9670, F1: 0.9814 Bal: 0.9696 - Val Loss: 0.1945, Accuracy: 0.9548, F1: 0.9746 Bal: 0.9209\n","           WALLETS: Train Loss: 0.33476511, Acc: 0.87853950, F1: 0.93070778 Bal: 0.8476 - Val Loss: 0.33805212, Accuracy: 0.8781, F1: 0.9303 Bal: 0.8460\n","Epoch 138:      TX: Train Loss: 0.0951, Acc: 0.9674, F1: 0.9816 Bal: 0.9703 - Val Loss: 0.1950, Accuracy: 0.9552, F1: 0.9749 Bal: 0.9211\n","           WALLETS: Train Loss: 0.33368129, Acc: 0.87851047, F1: 0.93067774 Bal: 0.8485 - Val Loss: 0.33711082, Accuracy: 0.8780, F1: 0.9303 Bal: 0.8473\n","Epoch 139:      TX: Train Loss: 0.0938, Acc: 0.9679, F1: 0.9819 Bal: 0.9706 - Val Loss: 0.1955, Accuracy: 0.9555, F1: 0.9750 Bal: 0.9213\n","           WALLETS: Train Loss: 0.33260325, Acc: 0.87826737, F1: 0.93052046 Bal: 0.8490 - Val Loss: 0.33617023, Accuracy: 0.8778, F1: 0.9301 Bal: 0.8478\n","Epoch 140:      TX: Train Loss: 0.0926, Acc: 0.9684, F1: 0.9822 Bal: 0.9710 - Val Loss: 0.1960, Accuracy: 0.9552, F1: 0.9749 Bal: 0.9202\n","           WALLETS: Train Loss: 0.33154058, Acc: 0.87845605, F1: 0.93062672 Bal: 0.8498 - Val Loss: 0.33525854, Accuracy: 0.8779, F1: 0.9302 Bal: 0.8486\n","Epoch 141:      TX: Train Loss: 0.0914, Acc: 0.9685, F1: 0.9823 Bal: 0.9713 - Val Loss: 0.1965, Accuracy: 0.9555, F1: 0.9750 Bal: 0.9203\n","           WALLETS: Train Loss: 0.33050144, Acc: 0.87846330, F1: 0.93061793 Bal: 0.8508 - Val Loss: 0.33436158, Accuracy: 0.8779, F1: 0.9302 Bal: 0.8492\n","Epoch 142:      TX: Train Loss: 0.0902, Acc: 0.9692, F1: 0.9826 Bal: 0.9718 - Val Loss: 0.1971, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9204\n","           WALLETS: Train Loss: 0.32947639, Acc: 0.87846693, F1: 0.93060749 Bal: 0.8517 - Val Loss: 0.33348757, Accuracy: 0.8779, F1: 0.9302 Bal: 0.8491\n","Epoch 143:      TX: Train Loss: 0.0891, Acc: 0.9695, F1: 0.9828 Bal: 0.9722 - Val Loss: 0.1977, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9204\n","           WALLETS: Train Loss: 0.32845807, Acc: 0.87872092, F1: 0.93075194 Bal: 0.8526 - Val Loss: 0.33261865, Accuracy: 0.8783, F1: 0.9304 Bal: 0.8503\n","Epoch 144:      TX: Train Loss: 0.0879, Acc: 0.9698, F1: 0.9830 Bal: 0.9728 - Val Loss: 0.1982, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9204\n","           WALLETS: Train Loss: 0.32744759, Acc: 0.87860118, F1: 0.93067223 Bal: 0.8531 - Val Loss: 0.33174676, Accuracy: 0.8782, F1: 0.9304 Bal: 0.8512\n","Epoch 145:      TX: Train Loss: 0.0868, Acc: 0.9705, F1: 0.9834 Bal: 0.9735 - Val Loss: 0.1987, Accuracy: 0.9559, F1: 0.9753 Bal: 0.9195\n","           WALLETS: Train Loss: 0.32644299, Acc: 0.87863747, F1: 0.93068663 Bal: 0.8537 - Val Loss: 0.33088908, Accuracy: 0.8784, F1: 0.9305 Bal: 0.8522\n","Epoch 146:      TX: Train Loss: 0.0857, Acc: 0.9710, F1: 0.9837 Bal: 0.9739 - Val Loss: 0.1993, Accuracy: 0.9566, F1: 0.9756 Bal: 0.9199\n","           WALLETS: Train Loss: 0.32544866, Acc: 0.87872455, F1: 0.93073005 Bal: 0.8544 - Val Loss: 0.33002639, Accuracy: 0.8783, F1: 0.9304 Bal: 0.8525\n","Epoch 147:      TX: Train Loss: 0.0846, Acc: 0.9715, F1: 0.9840 Bal: 0.9747 - Val Loss: 0.1998, Accuracy: 0.9566, F1: 0.9756 Bal: 0.9199\n","           WALLETS: Train Loss: 0.32447070, Acc: 0.87878623, F1: 0.93076054 Bal: 0.8550 - Val Loss: 0.32917288, Accuracy: 0.8783, F1: 0.9304 Bal: 0.8531\n","Epoch 148:      TX: Train Loss: 0.0835, Acc: 0.9719, F1: 0.9842 Bal: 0.9751 - Val Loss: 0.2004, Accuracy: 0.9566, F1: 0.9756 Bal: 0.9199\n","           WALLETS: Train Loss: 0.32350546, Acc: 0.87890597, F1: 0.93082650 Bal: 0.8556 - Val Loss: 0.32834026, Accuracy: 0.8782, F1: 0.9303 Bal: 0.8535\n","Epoch 149:      TX: Train Loss: 0.0825, Acc: 0.9722, F1: 0.9843 Bal: 0.9754 - Val Loss: 0.2010, Accuracy: 0.9568, F1: 0.9758 Bal: 0.9210\n","           WALLETS: Train Loss: 0.32255715, Acc: 0.87907287, F1: 0.93091984 Bal: 0.8563 - Val Loss: 0.32750511, Accuracy: 0.8785, F1: 0.9305 Bal: 0.8538\n","Epoch 150:      TX: Train Loss: 0.0814, Acc: 0.9726, F1: 0.9846 Bal: 0.9758 - Val Loss: 0.2016, Accuracy: 0.9570, F1: 0.9759 Bal: 0.9211\n","           WALLETS: Train Loss: 0.32161647, Acc: 0.87930509, F1: 0.93105650 Bal: 0.8569 - Val Loss: 0.32671359, Accuracy: 0.8787, F1: 0.9306 Bal: 0.8546\n","Epoch 151:      TX: Train Loss: 0.0804, Acc: 0.9729, F1: 0.9847 Bal: 0.9760 - Val Loss: 0.2022, Accuracy: 0.9568, F1: 0.9758 Bal: 0.9210\n","           WALLETS: Train Loss: 0.32068142, Acc: 0.87917447, F1: 0.93096357 Bal: 0.8578 - Val Loss: 0.32590148, Accuracy: 0.8786, F1: 0.9305 Bal: 0.8558\n","Epoch 152:      TX: Train Loss: 0.0794, Acc: 0.9731, F1: 0.9848 Bal: 0.9762 - Val Loss: 0.2028, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9213\n","           WALLETS: Train Loss: 0.31975251, Acc: 0.87991829, F1: 0.93140918 Bal: 0.8588 - Val Loss: 0.32513401, Accuracy: 0.8791, F1: 0.9308 Bal: 0.8560\n","Epoch 153:      TX: Train Loss: 0.0784, Acc: 0.9734, F1: 0.9850 Bal: 0.9766 - Val Loss: 0.2035, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9203\n","           WALLETS: Train Loss: 0.31883135, Acc: 0.87989289, F1: 0.93138202 Bal: 0.8597 - Val Loss: 0.32435015, Accuracy: 0.8791, F1: 0.9308 Bal: 0.8564\n","Epoch 154:      TX: Train Loss: 0.0775, Acc: 0.9736, F1: 0.9851 Bal: 0.9769 - Val Loss: 0.2043, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9204\n","           WALLETS: Train Loss: 0.31792027, Acc: 0.88009245, F1: 0.93149873 Bal: 0.8602 - Val Loss: 0.32360497, Accuracy: 0.8792, F1: 0.9309 Bal: 0.8562\n","Epoch 155:      TX: Train Loss: 0.0765, Acc: 0.9738, F1: 0.9853 Bal: 0.9774 - Val Loss: 0.2049, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9204\n","           WALLETS: Train Loss: 0.31701860, Acc: 0.88027750, F1: 0.93159976 Bal: 0.8612 - Val Loss: 0.32286233, Accuracy: 0.8791, F1: 0.9309 Bal: 0.8567\n","Epoch 156:      TX: Train Loss: 0.0756, Acc: 0.9742, F1: 0.9855 Bal: 0.9779 - Val Loss: 0.2057, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9204\n","           WALLETS: Train Loss: 0.31612983, Acc: 0.88052060, F1: 0.93174276 Bal: 0.8617 - Val Loss: 0.32213831, Accuracy: 0.8793, F1: 0.9310 Bal: 0.8571\n","Epoch 157:      TX: Train Loss: 0.0746, Acc: 0.9746, F1: 0.9857 Bal: 0.9783 - Val Loss: 0.2065, Accuracy: 0.9581, F1: 0.9765 Bal: 0.9208\n","           WALLETS: Train Loss: 0.31525218, Acc: 0.88086530, F1: 0.93194687 Bal: 0.8624 - Val Loss: 0.32143337, Accuracy: 0.8794, F1: 0.9310 Bal: 0.8573\n","Epoch 158:      TX: Train Loss: 0.0737, Acc: 0.9748, F1: 0.9858 Bal: 0.9784 - Val Loss: 0.2074, Accuracy: 0.9585, F1: 0.9768 Bal: 0.9210\n","           WALLETS: Train Loss: 0.31438512, Acc: 0.88107575, F1: 0.93207131 Bal: 0.8628 - Val Loss: 0.32072723, Accuracy: 0.8796, F1: 0.9311 Bal: 0.8579\n","Epoch 159:      TX: Train Loss: 0.0728, Acc: 0.9749, F1: 0.9859 Bal: 0.9785 - Val Loss: 0.2081, Accuracy: 0.9585, F1: 0.9768 Bal: 0.9200\n","           WALLETS: Train Loss: 0.31352612, Acc: 0.88151115, F1: 0.93233376 Bal: 0.8633 - Val Loss: 0.32005590, Accuracy: 0.8800, F1: 0.9313 Bal: 0.8578\n","Epoch 160:      TX: Train Loss: 0.0719, Acc: 0.9752, F1: 0.9860 Bal: 0.9788 - Val Loss: 0.2088, Accuracy: 0.9592, F1: 0.9771 Bal: 0.9204\n","           WALLETS: Train Loss: 0.31267530, Acc: 0.88152930, F1: 0.93233921 Bal: 0.8637 - Val Loss: 0.31936094, Accuracy: 0.8797, F1: 0.9312 Bal: 0.8581\n","Epoch 161:      TX: Train Loss: 0.0711, Acc: 0.9756, F1: 0.9863 Bal: 0.9790 - Val Loss: 0.2096, Accuracy: 0.9596, F1: 0.9774 Bal: 0.9206\n","           WALLETS: Train Loss: 0.31183508, Acc: 0.88205541, F1: 0.93265824 Bal: 0.8641 - Val Loss: 0.31873664, Accuracy: 0.8807, F1: 0.9318 Bal: 0.8583\n","Epoch 162:      TX: Train Loss: 0.0702, Acc: 0.9760, F1: 0.9865 Bal: 0.9794 - Val Loss: 0.2103, Accuracy: 0.9599, F1: 0.9775 Bal: 0.9208\n","           WALLETS: Train Loss: 0.31100693, Acc: 0.88156921, F1: 0.93235205 Bal: 0.8646 - Val Loss: 0.31803513, Accuracy: 0.8800, F1: 0.9314 Bal: 0.8586\n","Epoch 163:      TX: Train Loss: 0.0694, Acc: 0.9761, F1: 0.9866 Bal: 0.9796 - Val Loss: 0.2110, Accuracy: 0.9601, F1: 0.9777 Bal: 0.9209\n","           WALLETS: Train Loss: 0.31019020, Acc: 0.88282826, F1: 0.93312209 Bal: 0.8651 - Val Loss: 0.31749642, Accuracy: 0.8815, F1: 0.9323 Bal: 0.8590\n","Epoch 164:      TX: Train Loss: 0.0685, Acc: 0.9766, F1: 0.9868 Bal: 0.9802 - Val Loss: 0.2118, Accuracy: 0.9596, F1: 0.9774 Bal: 0.9197\n","           WALLETS: Train Loss: 0.30939013, Acc: 0.88074193, F1: 0.93181846 Bal: 0.8663 - Val Loss: 0.31669065, Accuracy: 0.8794, F1: 0.9309 Bal: 0.8600\n","Epoch 165:      TX: Train Loss: 0.0677, Acc: 0.9768, F1: 0.9870 Bal: 0.9805 - Val Loss: 0.2126, Accuracy: 0.9594, F1: 0.9773 Bal: 0.9195\n","           WALLETS: Train Loss: 0.30860975, Acc: 0.88424696, F1: 0.93398475 Bal: 0.8659 - Val Loss: 0.31637189, Accuracy: 0.8829, F1: 0.9331 Bal: 0.8590\n","Epoch 166:      TX: Train Loss: 0.0669, Acc: 0.9772, F1: 0.9872 Bal: 0.9809 - Val Loss: 0.2133, Accuracy: 0.9594, F1: 0.9773 Bal: 0.9195\n","           WALLETS: Train Loss: 0.30782968, Acc: 0.87998723, F1: 0.93133600 Bal: 0.8675 - Val Loss: 0.31540504, Accuracy: 0.8785, F1: 0.9304 Bal: 0.8609\n","Epoch 167:      TX: Train Loss: 0.0661, Acc: 0.9775, F1: 0.9874 Bal: 0.9814 - Val Loss: 0.2140, Accuracy: 0.9599, F1: 0.9775 Bal: 0.9188\n","           WALLETS: Train Loss: 0.30702725, Acc: 0.88443201, F1: 0.93408770 Bal: 0.8667 - Val Loss: 0.31517571, Accuracy: 0.8833, F1: 0.9334 Bal: 0.8602\n","Epoch 168:      TX: Train Loss: 0.0653, Acc: 0.9779, F1: 0.9876 Bal: 0.9818 - Val Loss: 0.2149, Accuracy: 0.9601, F1: 0.9777 Bal: 0.9189\n","           WALLETS: Train Loss: 0.30621228, Acc: 0.88170346, F1: 0.93238936 Bal: 0.8681 - Val Loss: 0.31430635, Accuracy: 0.8805, F1: 0.9316 Bal: 0.8615\n","Epoch 169:      TX: Train Loss: 0.0646, Acc: 0.9781, F1: 0.9877 Bal: 0.9821 - Val Loss: 0.2156, Accuracy: 0.9605, F1: 0.9779 Bal: 0.9192\n","           WALLETS: Train Loss: 0.30544004, Acc: 0.88204090, F1: 0.93259103 Bal: 0.8686 - Val Loss: 0.31375295, Accuracy: 0.8809, F1: 0.9319 Bal: 0.8620\n","Epoch 170:      TX: Train Loss: 0.0638, Acc: 0.9785, F1: 0.9879 Bal: 0.9826 - Val Loss: 0.2164, Accuracy: 0.9605, F1: 0.9779 Bal: 0.9192\n","           WALLETS: Train Loss: 0.30470634, Acc: 0.88434493, F1: 0.93401373 Bal: 0.8683 - Val Loss: 0.31342146, Accuracy: 0.8834, F1: 0.9334 Bal: 0.8617\n","Epoch 171:      TX: Train Loss: 0.0631, Acc: 0.9786, F1: 0.9880 Bal: 0.9829 - Val Loss: 0.2172, Accuracy: 0.9612, F1: 0.9783 Bal: 0.9195\n","           WALLETS: Train Loss: 0.30396768, Acc: 0.88128256, F1: 0.93209050 Bal: 0.8711 - Val Loss: 0.31256291, Accuracy: 0.8800, F1: 0.9313 Bal: 0.8641\n","Epoch 172:      TX: Train Loss: 0.0623, Acc: 0.9788, F1: 0.9881 Bal: 0.9831 - Val Loss: 0.2179, Accuracy: 0.9614, F1: 0.9784 Bal: 0.9196\n","           WALLETS: Train Loss: 0.30320933, Acc: 0.88475857, F1: 0.93423829 Bal: 0.8707 - Val Loss: 0.31230453, Accuracy: 0.8835, F1: 0.9335 Bal: 0.8627\n","Epoch 173:      TX: Train Loss: 0.0616, Acc: 0.9791, F1: 0.9883 Bal: 0.9836 - Val Loss: 0.2186, Accuracy: 0.9614, F1: 0.9784 Bal: 0.9187\n","           WALLETS: Train Loss: 0.30245987, Acc: 0.88367005, F1: 0.93355137 Bal: 0.8720 - Val Loss: 0.31161228, Accuracy: 0.8823, F1: 0.9327 Bal: 0.8639\n","Epoch 174:      TX: Train Loss: 0.0609, Acc: 0.9793, F1: 0.9884 Bal: 0.9838 - Val Loss: 0.2194, Accuracy: 0.9618, F1: 0.9787 Bal: 0.9189\n","           WALLETS: Train Loss: 0.30175027, Acc: 0.88330358, F1: 0.93330375 Bal: 0.8737 - Val Loss: 0.31102210, Accuracy: 0.8819, F1: 0.9325 Bal: 0.8664\n","Epoch 175:      TX: Train Loss: 0.0602, Acc: 0.9795, F1: 0.9885 Bal: 0.9842 - Val Loss: 0.2201, Accuracy: 0.9620, F1: 0.9788 Bal: 0.9190\n","           WALLETS: Train Loss: 0.30105942, Acc: 0.88617364, F1: 0.93507106 Bal: 0.8736 - Val Loss: 0.31076270, Accuracy: 0.8847, F1: 0.9342 Bal: 0.8652\n","Epoch 176:      TX: Train Loss: 0.0595, Acc: 0.9798, F1: 0.9886 Bal: 0.9845 - Val Loss: 0.2206, Accuracy: 0.9620, F1: 0.9788 Bal: 0.9190\n","           WALLETS: Train Loss: 0.30035526, Acc: 0.88291534, F1: 0.93305367 Bal: 0.8745 - Val Loss: 0.30995625, Accuracy: 0.8818, F1: 0.9323 Bal: 0.8677\n","Epoch 177:      TX: Train Loss: 0.0589, Acc: 0.9800, F1: 0.9888 Bal: 0.9847 - Val Loss: 0.2214, Accuracy: 0.9620, F1: 0.9788 Bal: 0.9190\n","           WALLETS: Train Loss: 0.29963923, Acc: 0.88583983, F1: 0.93485287 Bal: 0.8747 - Val Loss: 0.30968174, Accuracy: 0.8843, F1: 0.9339 Bal: 0.8666\n","Epoch 178:      TX: Train Loss: 0.0582, Acc: 0.9802, F1: 0.9889 Bal: 0.9851 - Val Loss: 0.2222, Accuracy: 0.9620, F1: 0.9788 Bal: 0.9190\n","           WALLETS: Train Loss: 0.29893750, Acc: 0.88489282, F1: 0.93426522 Bal: 0.8751 - Val Loss: 0.30910963, Accuracy: 0.8834, F1: 0.9334 Bal: 0.8670\n","Epoch 179:      TX: Train Loss: 0.0575, Acc: 0.9804, F1: 0.9890 Bal: 0.9854 - Val Loss: 0.2229, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9192\n","           WALLETS: Train Loss: 0.29826081, Acc: 0.88402563, F1: 0.93372453 Bal: 0.8756 - Val Loss: 0.30853620, Accuracy: 0.8824, F1: 0.9327 Bal: 0.8673\n","Epoch 180:      TX: Train Loss: 0.0569, Acc: 0.9806, F1: 0.9891 Bal: 0.9856 - Val Loss: 0.2236, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9192\n","           WALLETS: Train Loss: 0.29759517, Acc: 0.88699366, F1: 0.93554602 Bal: 0.8759 - Val Loss: 0.30830741, Accuracy: 0.8853, F1: 0.9345 Bal: 0.8673\n","Epoch 181:      TX: Train Loss: 0.0562, Acc: 0.9810, F1: 0.9893 Bal: 0.9859 - Val Loss: 0.2244, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9192\n","           WALLETS: Train Loss: 0.29691967, Acc: 0.88390227, F1: 0.93364028 Bal: 0.8762 - Val Loss: 0.30754772, Accuracy: 0.8820, F1: 0.9325 Bal: 0.8675\n","Epoch 182:      TX: Train Loss: 0.0556, Acc: 0.9812, F1: 0.9895 Bal: 0.9861 - Val Loss: 0.2253, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9192\n","           WALLETS: Train Loss: 0.29623792, Acc: 0.88678321, F1: 0.93541075 Bal: 0.8764 - Val Loss: 0.30731863, Accuracy: 0.8848, F1: 0.9342 Bal: 0.8675\n","Epoch 183:      TX: Train Loss: 0.0550, Acc: 0.9813, F1: 0.9895 Bal: 0.9861 - Val Loss: 0.2260, Accuracy: 0.9625, F1: 0.9790 Bal: 0.9193\n","           WALLETS: Train Loss: 0.29556400, Acc: 0.88557496, F1: 0.93466208 Bal: 0.8769 - Val Loss: 0.30671832, Accuracy: 0.8834, F1: 0.9334 Bal: 0.8677\n","Epoch 184:      TX: Train Loss: 0.0544, Acc: 0.9815, F1: 0.9896 Bal: 0.9863 - Val Loss: 0.2268, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9192\n","           WALLETS: Train Loss: 0.29490691, Acc: 0.88574912, F1: 0.93476558 Bal: 0.8772 - Val Loss: 0.30627742, Accuracy: 0.8837, F1: 0.9335 Bal: 0.8679\n","Epoch 185:      TX: Train Loss: 0.0538, Acc: 0.9819, F1: 0.9898 Bal: 0.9867 - Val Loss: 0.2277, Accuracy: 0.9625, F1: 0.9790 Bal: 0.9193\n","           WALLETS: Train Loss: 0.29426351, Acc: 0.88733110, F1: 0.93573197 Bal: 0.8776 - Val Loss: 0.30597869, Accuracy: 0.8852, F1: 0.9344 Bal: 0.8683\n","Epoch 186:      TX: Train Loss: 0.0532, Acc: 0.9821, F1: 0.9900 Bal: 0.9868 - Val Loss: 0.2284, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9182\n","           WALLETS: Train Loss: 0.29362580, Acc: 0.88520486, F1: 0.93441869 Bal: 0.8781 - Val Loss: 0.30530703, Accuracy: 0.8831, F1: 0.9331 Bal: 0.8692\n","Epoch 187:      TX: Train Loss: 0.0526, Acc: 0.9824, F1: 0.9901 Bal: 0.9871 - Val Loss: 0.2293, Accuracy: 0.9625, F1: 0.9791 Bal: 0.9183\n","           WALLETS: Train Loss: 0.29298729, Acc: 0.88820917, F1: 0.93626686 Bal: 0.8779 - Val Loss: 0.30515164, Accuracy: 0.8861, F1: 0.9350 Bal: 0.8688\n","Epoch 188:      TX: Train Loss: 0.0520, Acc: 0.9825, F1: 0.9902 Bal: 0.9872 - Val Loss: 0.2302, Accuracy: 0.9627, F1: 0.9792 Bal: 0.9184\n","           WALLETS: Train Loss: 0.29234540, Acc: 0.88561124, F1: 0.93466060 Bal: 0.8788 - Val Loss: 0.30442059, Accuracy: 0.8831, F1: 0.9331 Bal: 0.8694\n","Epoch 189:      TX: Train Loss: 0.0514, Acc: 0.9827, F1: 0.9903 Bal: 0.9874 - Val Loss: 0.2310, Accuracy: 0.9629, F1: 0.9793 Bal: 0.9185\n","           WALLETS: Train Loss: 0.29170603, Acc: 0.88803138, F1: 0.93615111 Bal: 0.8785 - Val Loss: 0.30424550, Accuracy: 0.8859, F1: 0.9349 Bal: 0.8695\n","Epoch 190:      TX: Train Loss: 0.0509, Acc: 0.9829, F1: 0.9904 Bal: 0.9876 - Val Loss: 0.2317, Accuracy: 0.9627, F1: 0.9792 Bal: 0.9184\n","           WALLETS: Train Loss: 0.29106984, Acc: 0.88639497, F1: 0.93513489 Bal: 0.8794 - Val Loss: 0.30362183, Accuracy: 0.8839, F1: 0.9336 Bal: 0.8693\n","Epoch 191:      TX: Train Loss: 0.0503, Acc: 0.9830, F1: 0.9905 Bal: 0.9878 - Val Loss: 0.2325, Accuracy: 0.9629, F1: 0.9793 Bal: 0.9185\n","           WALLETS: Train Loss: 0.29043669, Acc: 0.88753429, F1: 0.93583671 Bal: 0.8792 - Val Loss: 0.30331749, Accuracy: 0.8853, F1: 0.9345 Bal: 0.8698\n","Epoch 192:      TX: Train Loss: 0.0498, Acc: 0.9832, F1: 0.9906 Bal: 0.9879 - Val Loss: 0.2335, Accuracy: 0.9629, F1: 0.9793 Bal: 0.9176\n","           WALLETS: Train Loss: 0.28980812, Acc: 0.88739641, F1: 0.93574635 Bal: 0.8797 - Val Loss: 0.30285496, Accuracy: 0.8851, F1: 0.9343 Bal: 0.8698\n","Epoch 193:      TX: Train Loss: 0.0492, Acc: 0.9832, F1: 0.9906 Bal: 0.9880 - Val Loss: 0.2343, Accuracy: 0.9629, F1: 0.9793 Bal: 0.9176\n","           WALLETS: Train Loss: 0.28918713, Acc: 0.88733836, F1: 0.93570736 Bal: 0.8800 - Val Loss: 0.30242607, Accuracy: 0.8850, F1: 0.9343 Bal: 0.8701\n","Epoch 194:      TX: Train Loss: 0.0487, Acc: 0.9834, F1: 0.9907 Bal: 0.9882 - Val Loss: 0.2351, Accuracy: 0.9631, F1: 0.9794 Bal: 0.9177\n","           WALLETS: Train Loss: 0.28857389, Acc: 0.88818740, F1: 0.93622860 Bal: 0.8799 - Val Loss: 0.30211627, Accuracy: 0.8857, F1: 0.9347 Bal: 0.8693\n","Epoch 195:      TX: Train Loss: 0.0482, Acc: 0.9835, F1: 0.9908 Bal: 0.9882 - Val Loss: 0.2360, Accuracy: 0.9636, F1: 0.9797 Bal: 0.9179\n","           WALLETS: Train Loss: 0.28796843, Acc: 0.88693923, F1: 0.93545260 Bal: 0.8807 - Val Loss: 0.30156642, Accuracy: 0.8844, F1: 0.9339 Bal: 0.8701\n","Epoch 196:      TX: Train Loss: 0.0477, Acc: 0.9837, F1: 0.9909 Bal: 0.9885 - Val Loss: 0.2367, Accuracy: 0.9631, F1: 0.9794 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28737625, Acc: 0.88931583, F1: 0.93691462 Bal: 0.8804 - Val Loss: 0.30143985, Accuracy: 0.8868, F1: 0.9354 Bal: 0.8697\n","Epoch 197:      TX: Train Loss: 0.0472, Acc: 0.9838, F1: 0.9909 Bal: 0.9887 - Val Loss: 0.2375, Accuracy: 0.9631, F1: 0.9794 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28680179, Acc: 0.88640223, F1: 0.93511430 Bal: 0.8814 - Val Loss: 0.30071726, Accuracy: 0.8839, F1: 0.9336 Bal: 0.8704\n","Epoch 198:      TX: Train Loss: 0.0467, Acc: 0.9843, F1: 0.9912 Bal: 0.9891 - Val Loss: 0.2386, Accuracy: 0.9631, F1: 0.9794 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28625807, Acc: 0.89149287, F1: 0.93824331 Bal: 0.8806 - Val Loss: 0.30094931, Accuracy: 0.8892, F1: 0.9369 Bal: 0.8697\n","Epoch 199:      TX: Train Loss: 0.0462, Acc: 0.9844, F1: 0.9913 Bal: 0.9893 - Val Loss: 0.2394, Accuracy: 0.9631, F1: 0.9794 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28575388, Acc: 0.88464971, F1: 0.93402701 Bal: 0.8820 - Val Loss: 0.29989478, Accuracy: 0.8819, F1: 0.9324 Bal: 0.8709\n","Epoch 200:      TX: Train Loss: 0.0457, Acc: 0.9847, F1: 0.9914 Bal: 0.9899 - Val Loss: 0.2399, Accuracy: 0.9629, F1: 0.9793 Bal: 0.9166\n","           WALLETS: Train Loss: 0.28525445, Acc: 0.89394203, F1: 0.93973569 Bal: 0.8807 - Val Loss: 0.30063862, Accuracy: 0.8909, F1: 0.9379 Bal: 0.8687\n","Epoch 201:      TX: Train Loss: 0.0452, Acc: 0.9848, F1: 0.9915 Bal: 0.9900 - Val Loss: 0.2407, Accuracy: 0.9631, F1: 0.9794 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28465888, Acc: 0.88429050, F1: 0.93380027 Bal: 0.8824 - Val Loss: 0.29919273, Accuracy: 0.8817, F1: 0.9322 Bal: 0.8714\n","Epoch 202:      TX: Train Loss: 0.0447, Acc: 0.9850, F1: 0.9916 Bal: 0.9901 - Val Loss: 0.2421, Accuracy: 0.9634, F1: 0.9796 Bal: 0.9168\n","           WALLETS: Train Loss: 0.28394839, Acc: 0.89200810, F1: 0.93854442 Bal: 0.8818 - Val Loss: 0.29954520, Accuracy: 0.8894, F1: 0.9370 Bal: 0.8696\n","Epoch 203:      TX: Train Loss: 0.0442, Acc: 0.9851, F1: 0.9916 Bal: 0.9903 - Val Loss: 0.2424, Accuracy: 0.9634, F1: 0.9796 Bal: 0.9168\n","           WALLETS: Train Loss: 0.28331792, Acc: 0.88965327, F1: 0.93709849 Bal: 0.8823 - Val Loss: 0.29886663, Accuracy: 0.8868, F1: 0.9354 Bal: 0.8700\n","Epoch 204:      TX: Train Loss: 0.0438, Acc: 0.9852, F1: 0.9917 Bal: 0.9903 - Val Loss: 0.2430, Accuracy: 0.9636, F1: 0.9797 Bal: 0.9169\n","           WALLETS: Train Loss: 0.28284484, Acc: 0.88670339, F1: 0.93528028 Bal: 0.8829 - Val Loss: 0.29827508, Accuracy: 0.8842, F1: 0.9337 Bal: 0.8723\n","Epoch 205:      TX: Train Loss: 0.0433, Acc: 0.9856, F1: 0.9920 Bal: 0.9906 - Val Loss: 0.2443, Accuracy: 0.9636, F1: 0.9797 Bal: 0.9169\n","           WALLETS: Train Loss: 0.28237578, Acc: 0.89370619, F1: 0.93957265 Bal: 0.8824 - Val Loss: 0.29883459, Accuracy: 0.8910, F1: 0.9380 Bal: 0.8699\n","Epoch 206:      TX: Train Loss: 0.0429, Acc: 0.9858, F1: 0.9920 Bal: 0.9907 - Val Loss: 0.2448, Accuracy: 0.9638, F1: 0.9798 Bal: 0.9171\n","           WALLETS: Train Loss: 0.28178096, Acc: 0.88649294, F1: 0.93514501 Bal: 0.8834 - Val Loss: 0.29763624, Accuracy: 0.8836, F1: 0.9334 Bal: 0.8720\n","Epoch 207:      TX: Train Loss: 0.0424, Acc: 0.9859, F1: 0.9921 Bal: 0.9907 - Val Loss: 0.2454, Accuracy: 0.9638, F1: 0.9798 Bal: 0.9171\n","           WALLETS: Train Loss: 0.28114593, Acc: 0.89116994, F1: 0.93801612 Bal: 0.8832 - Val Loss: 0.29775026, Accuracy: 0.8879, F1: 0.9361 Bal: 0.8704\n","Epoch 208:      TX: Train Loss: 0.0420, Acc: 0.9861, F1: 0.9922 Bal: 0.9909 - Val Loss: 0.2467, Accuracy: 0.9640, F1: 0.9799 Bal: 0.9172\n","           WALLETS: Train Loss: 0.28062621, Acc: 0.89173234, F1: 0.93835720 Bal: 0.8834 - Val Loss: 0.29752475, Accuracy: 0.8888, F1: 0.9366 Bal: 0.8707\n","Epoch 209:      TX: Train Loss: 0.0416, Acc: 0.9862, F1: 0.9923 Bal: 0.9909 - Val Loss: 0.2474, Accuracy: 0.9642, F1: 0.9801 Bal: 0.9173\n","           WALLETS: Train Loss: 0.28017050, Acc: 0.88733836, F1: 0.93565327 Bal: 0.8843 - Val Loss: 0.29674485, Accuracy: 0.8843, F1: 0.9338 Bal: 0.8722\n","Epoch 210:      TX: Train Loss: 0.0411, Acc: 0.9863, F1: 0.9923 Bal: 0.9911 - Val Loss: 0.2478, Accuracy: 0.9642, F1: 0.9801 Bal: 0.9173\n","           WALLETS: Train Loss: 0.27964303, Acc: 0.89376424, F1: 0.93959294 Bal: 0.8837 - Val Loss: 0.29720706, Accuracy: 0.8905, F1: 0.9377 Bal: 0.8706\n","Epoch 211:      TX: Train Loss: 0.0407, Acc: 0.9865, F1: 0.9925 Bal: 0.9914 - Val Loss: 0.2485, Accuracy: 0.9642, F1: 0.9801 Bal: 0.9173\n","           WALLETS: Train Loss: 0.27904719, Acc: 0.88920335, F1: 0.93679522 Bal: 0.8846 - Val Loss: 0.29624915, Accuracy: 0.8861, F1: 0.9349 Bal: 0.8723\n","Epoch 212:      TX: Train Loss: 0.0403, Acc: 0.9867, F1: 0.9926 Bal: 0.9916 - Val Loss: 0.2493, Accuracy: 0.9645, F1: 0.9802 Bal: 0.9174\n","           WALLETS: Train Loss: 0.27849680, Acc: 0.89069099, F1: 0.93770678 Bal: 0.8845 - Val Loss: 0.29608977, Accuracy: 0.8877, F1: 0.9360 Bal: 0.8724\n","Epoch 213:      TX: Train Loss: 0.0399, Acc: 0.9869, F1: 0.9927 Bal: 0.9917 - Val Loss: 0.2498, Accuracy: 0.9647, F1: 0.9803 Bal: 0.9175\n","           WALLETS: Train Loss: 0.27802044, Acc: 0.89344494, F1: 0.93938637 Bal: 0.8847 - Val Loss: 0.29615232, Accuracy: 0.8901, F1: 0.9374 Bal: 0.8717\n","Epoch 214:      TX: Train Loss: 0.0395, Acc: 0.9870, F1: 0.9927 Bal: 0.9919 - Val Loss: 0.2505, Accuracy: 0.9642, F1: 0.9801 Bal: 0.9173\n","           WALLETS: Train Loss: 0.27753919, Acc: 0.88878608, F1: 0.93652868 Bal: 0.8854 - Val Loss: 0.29528785, Accuracy: 0.8857, F1: 0.9347 Bal: 0.8731\n","Epoch 215:      TX: Train Loss: 0.0391, Acc: 0.9873, F1: 0.9929 Bal: 0.9920 - Val Loss: 0.2517, Accuracy: 0.9647, F1: 0.9803 Bal: 0.9175\n","           WALLETS: Train Loss: 0.27700031, Acc: 0.89391301, F1: 0.93966595 Bal: 0.8852 - Val Loss: 0.29561964, Accuracy: 0.8904, F1: 0.9376 Bal: 0.8721\n","Epoch 216:      TX: Train Loss: 0.0387, Acc: 0.9874, F1: 0.9929 Bal: 0.9921 - Val Loss: 0.2523, Accuracy: 0.9647, F1: 0.9803 Bal: 0.9175\n","           WALLETS: Train Loss: 0.27644971, Acc: 0.89104657, F1: 0.93790813 Bal: 0.8859 - Val Loss: 0.29491314, Accuracy: 0.8880, F1: 0.9361 Bal: 0.8733\n","Epoch 217:      TX: Train Loss: 0.0383, Acc: 0.9875, F1: 0.9930 Bal: 0.9922 - Val Loss: 0.2531, Accuracy: 0.9649, F1: 0.9804 Bal: 0.9186\n","           WALLETS: Train Loss: 0.27594504, Acc: 0.89141667, F1: 0.93813235 Bal: 0.8861 - Val Loss: 0.29467365, Accuracy: 0.8884, F1: 0.9364 Bal: 0.8737\n","Epoch 218:      TX: Train Loss: 0.0379, Acc: 0.9877, F1: 0.9931 Bal: 0.9923 - Val Loss: 0.2540, Accuracy: 0.9651, F1: 0.9806 Bal: 0.9188\n","           WALLETS: Train Loss: 0.27547455, Acc: 0.89419965, F1: 0.93983133 Bal: 0.8860 - Val Loss: 0.29478207, Accuracy: 0.8905, F1: 0.9377 Bal: 0.8726\n","Epoch 219:      TX: Train Loss: 0.0375, Acc: 0.9878, F1: 0.9932 Bal: 0.9925 - Val Loss: 0.2544, Accuracy: 0.9651, F1: 0.9806 Bal: 0.9188\n","           WALLETS: Train Loss: 0.27499548, Acc: 0.89038621, F1: 0.93749328 Bal: 0.8868 - Val Loss: 0.29400834, Accuracy: 0.8874, F1: 0.9357 Bal: 0.8739\n","Epoch 220:      TX: Train Loss: 0.0371, Acc: 0.9881, F1: 0.9933 Bal: 0.9926 - Val Loss: 0.2552, Accuracy: 0.9653, F1: 0.9807 Bal: 0.9189\n","           WALLETS: Train Loss: 0.27448833, Acc: 0.89486002, F1: 0.94022538 Bal: 0.8867 - Val Loss: 0.29431960, Accuracy: 0.8913, F1: 0.9381 Bal: 0.8733\n","Epoch 221:      TX: Train Loss: 0.0368, Acc: 0.9883, F1: 0.9935 Bal: 0.9927 - Val Loss: 0.2560, Accuracy: 0.9656, F1: 0.9808 Bal: 0.9190\n","           WALLETS: Train Loss: 0.27397010, Acc: 0.89202987, F1: 0.93849458 Bal: 0.8871 - Val Loss: 0.29364389, Accuracy: 0.8889, F1: 0.9366 Bal: 0.8744\n","Epoch 222:      TX: Train Loss: 0.0364, Acc: 0.9884, F1: 0.9935 Bal: 0.9928 - Val Loss: 0.2568, Accuracy: 0.9658, F1: 0.9809 Bal: 0.9191\n","           WALLETS: Train Loss: 0.27347201, Acc: 0.89356105, F1: 0.93942541 Bal: 0.8874 - Val Loss: 0.29355127, Accuracy: 0.8901, F1: 0.9374 Bal: 0.8743\n","Epoch 223:      TX: Train Loss: 0.0360, Acc: 0.9885, F1: 0.9936 Bal: 0.9928 - Val Loss: 0.2575, Accuracy: 0.9658, F1: 0.9809 Bal: 0.9191\n","           WALLETS: Train Loss: 0.27299827, Acc: 0.89462054, F1: 0.94007045 Bal: 0.8875 - Val Loss: 0.29345787, Accuracy: 0.8911, F1: 0.9380 Bal: 0.8736\n","Epoch 224:      TX: Train Loss: 0.0357, Acc: 0.9886, F1: 0.9936 Bal: 0.9929 - Val Loss: 0.2584, Accuracy: 0.9658, F1: 0.9809 Bal: 0.9191\n","           WALLETS: Train Loss: 0.27253649, Acc: 0.89238182, F1: 0.93869798 Bal: 0.8881 - Val Loss: 0.29287887, Accuracy: 0.8893, F1: 0.9369 Bal: 0.8746\n","Epoch 225:      TX: Train Loss: 0.0353, Acc: 0.9887, F1: 0.9937 Bal: 0.9929 - Val Loss: 0.2593, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9193\n","           WALLETS: Train Loss: 0.27207261, Acc: 0.89604650, F1: 0.94093295 Bal: 0.8880 - Val Loss: 0.29316133, Accuracy: 0.8923, F1: 0.9387 Bal: 0.8729\n","Epoch 226:      TX: Train Loss: 0.0350, Acc: 0.9887, F1: 0.9937 Bal: 0.9929 - Val Loss: 0.2596, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9193\n","           WALLETS: Train Loss: 0.27159792, Acc: 0.89246890, F1: 0.93874809 Bal: 0.8884 - Val Loss: 0.29236972, Accuracy: 0.8891, F1: 0.9368 Bal: 0.8748\n","Epoch 227:      TX: Train Loss: 0.0346, Acc: 0.9889, F1: 0.9938 Bal: 0.9931 - Val Loss: 0.2607, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9193\n","           WALLETS: Train Loss: 0.27111208, Acc: 0.89613721, F1: 0.94098145 Bal: 0.8885 - Val Loss: 0.29262331, Accuracy: 0.8926, F1: 0.9389 Bal: 0.8734\n","Epoch 228:      TX: Train Loss: 0.0343, Acc: 0.9891, F1: 0.9939 Bal: 0.9931 - Val Loss: 0.2614, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9193\n","           WALLETS: Train Loss: 0.27062729, Acc: 0.89362636, F1: 0.93944983 Bal: 0.8888 - Val Loss: 0.29198650, Accuracy: 0.8905, F1: 0.9376 Bal: 0.8751\n","Epoch 229:      TX: Train Loss: 0.0340, Acc: 0.9892, F1: 0.9940 Bal: 0.9932 - Val Loss: 0.2618, Accuracy: 0.9662, F1: 0.9812 Bal: 0.9194\n","           WALLETS: Train Loss: 0.27015078, Acc: 0.89546959, F1: 0.94056924 Bal: 0.8891 - Val Loss: 0.29196280, Accuracy: 0.8921, F1: 0.9386 Bal: 0.8743\n","Epoch 230:      TX: Train Loss: 0.0336, Acc: 0.9893, F1: 0.9940 Bal: 0.9933 - Val Loss: 0.2627, Accuracy: 0.9662, F1: 0.9812 Bal: 0.9194\n","           WALLETS: Train Loss: 0.26968622, Acc: 0.89525188, F1: 0.94043244 Bal: 0.8895 - Val Loss: 0.29166251, Accuracy: 0.8920, F1: 0.9385 Bal: 0.8752\n","Epoch 231:      TX: Train Loss: 0.0333, Acc: 0.9896, F1: 0.9942 Bal: 0.9934 - Val Loss: 0.2638, Accuracy: 0.9662, F1: 0.9812 Bal: 0.9194\n","           WALLETS: Train Loss: 0.26923206, Acc: 0.89483462, F1: 0.94017775 Bal: 0.8895 - Val Loss: 0.29132494, Accuracy: 0.8916, F1: 0.9383 Bal: 0.8751\n","Epoch 232:      TX: Train Loss: 0.0330, Acc: 0.9897, F1: 0.9942 Bal: 0.9935 - Val Loss: 0.2644, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9193\n","           WALLETS: Train Loss: 0.26878589, Acc: 0.89658350, F1: 0.94123953 Bal: 0.8897 - Val Loss: 0.29134077, Accuracy: 0.8930, F1: 0.9391 Bal: 0.8749\n","Epoch 233:      TX: Train Loss: 0.0327, Acc: 0.9898, F1: 0.9943 Bal: 0.9935 - Val Loss: 0.2650, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9193\n","           WALLETS: Train Loss: 0.26834628, Acc: 0.89429036, F1: 0.93983736 Bal: 0.8903 - Val Loss: 0.29073492, Accuracy: 0.8908, F1: 0.9378 Bal: 0.8753\n","Epoch 234:      TX: Train Loss: 0.0324, Acc: 0.9898, F1: 0.9943 Bal: 0.9935 - Val Loss: 0.2660, Accuracy: 0.9667, F1: 0.9814 Bal: 0.9196\n","           WALLETS: Train Loss: 0.26791769, Acc: 0.89804575, F1: 0.94212156 Bal: 0.8902 - Val Loss: 0.29109418, Accuracy: 0.8942, F1: 0.9399 Bal: 0.8746\n","Epoch 235:      TX: Train Loss: 0.0320, Acc: 0.9899, F1: 0.9944 Bal: 0.9936 - Val Loss: 0.2666, Accuracy: 0.9669, F1: 0.9816 Bal: 0.9197\n","           WALLETS: Train Loss: 0.26750541, Acc: 0.89347397, F1: 0.93933101 Bal: 0.8910 - Val Loss: 0.29018757, Accuracy: 0.8900, F1: 0.9373 Bal: 0.8758\n","Epoch 236:      TX: Train Loss: 0.0317, Acc: 0.9901, F1: 0.9944 Bal: 0.9937 - Val Loss: 0.2676, Accuracy: 0.9669, F1: 0.9816 Bal: 0.9197\n","           WALLETS: Train Loss: 0.26711500, Acc: 0.89993977, F1: 0.94326339 Bal: 0.8907 - Val Loss: 0.29099452, Accuracy: 0.8960, F1: 0.9410 Bal: 0.8739\n","Epoch 237:      TX: Train Loss: 0.0314, Acc: 0.9901, F1: 0.9944 Bal: 0.9937 - Val Loss: 0.2681, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9210\n","           WALLETS: Train Loss: 0.26675409, Acc: 0.89210244, F1: 0.93849040 Bal: 0.8912 - Val Loss: 0.28964156, Accuracy: 0.8886, F1: 0.9365 Bal: 0.8765\n","Epoch 238:      TX: Train Loss: 0.0311, Acc: 0.9901, F1: 0.9945 Bal: 0.9937 - Val Loss: 0.2692, Accuracy: 0.9675, F1: 0.9819 Bal: 0.9211\n","           WALLETS: Train Loss: 0.26638991, Acc: 0.90181928, F1: 0.94440072 Bal: 0.8906 - Val Loss: 0.29098868, Accuracy: 0.8979, F1: 0.9421 Bal: 0.8740\n","Epoch 239:      TX: Train Loss: 0.0308, Acc: 0.9903, F1: 0.9946 Bal: 0.9939 - Val Loss: 0.2697, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9210\n","           WALLETS: Train Loss: 0.26596993, Acc: 0.89172146, F1: 0.93825469 Bal: 0.8914 - Val Loss: 0.28919020, Accuracy: 0.8879, F1: 0.9360 Bal: 0.8759\n","Epoch 240:      TX: Train Loss: 0.0305, Acc: 0.9903, F1: 0.9946 Bal: 0.9940 - Val Loss: 0.2708, Accuracy: 0.9675, F1: 0.9819 Bal: 0.9211\n","           WALLETS: Train Loss: 0.26543921, Acc: 0.90138387, F1: 0.94413188 Bal: 0.8912 - Val Loss: 0.29037017, Accuracy: 0.8975, F1: 0.9419 Bal: 0.8750\n","Epoch 241:      TX: Train Loss: 0.0303, Acc: 0.9904, F1: 0.9947 Bal: 0.9940 - Val Loss: 0.2713, Accuracy: 0.9671, F1: 0.9817 Bal: 0.9208\n","           WALLETS: Train Loss: 0.26487720, Acc: 0.89527365, F1: 0.94041507 Bal: 0.8921 - Val Loss: 0.28901383, Accuracy: 0.8911, F1: 0.9380 Bal: 0.8758\n","Epoch 242:      TX: Train Loss: 0.0300, Acc: 0.9906, F1: 0.9948 Bal: 0.9941 - Val Loss: 0.2722, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9210\n","           WALLETS: Train Loss: 0.26440206, Acc: 0.89756680, F1: 0.94180747 Bal: 0.8923 - Val Loss: 0.28908408, Accuracy: 0.8935, F1: 0.9394 Bal: 0.8753\n","Epoch 243:      TX: Train Loss: 0.0297, Acc: 0.9907, F1: 0.9948 Bal: 0.9942 - Val Loss: 0.2731, Accuracy: 0.9671, F1: 0.9817 Bal: 0.9199\n","           WALLETS: Train Loss: 0.26404458, Acc: 0.90020464, F1: 0.94340927 Bal: 0.8921 - Val Loss: 0.28934771, Accuracy: 0.8960, F1: 0.9410 Bal: 0.8750\n","Epoch 244:      TX: Train Loss: 0.0294, Acc: 0.9908, F1: 0.9949 Bal: 0.9944 - Val Loss: 0.2735, Accuracy: 0.9671, F1: 0.9817 Bal: 0.9199\n","           WALLETS: Train Loss: 0.26373002, Acc: 0.89425770, F1: 0.93978881 Bal: 0.8927 - Val Loss: 0.28826591, Accuracy: 0.8905, F1: 0.9376 Bal: 0.8767\n","Epoch 245:      TX: Train Loss: 0.0291, Acc: 0.9910, F1: 0.9950 Bal: 0.9945 - Val Loss: 0.2747, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9200\n","           WALLETS: Train Loss: 0.26335478, Acc: 0.90242885, F1: 0.94475273 Bal: 0.8921 - Val Loss: 0.28941891, Accuracy: 0.8982, F1: 0.9423 Bal: 0.8754\n","Epoch 246:      TX: Train Loss: 0.0289, Acc: 0.9910, F1: 0.9950 Bal: 0.9945 - Val Loss: 0.2749, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9200\n","           WALLETS: Train Loss: 0.26288611, Acc: 0.89505958, F1: 0.94027292 Bal: 0.8932 - Val Loss: 0.28794453, Accuracy: 0.8908, F1: 0.9378 Bal: 0.8764\n","Epoch 247:      TX: Train Loss: 0.0286, Acc: 0.9911, F1: 0.9951 Bal: 0.9946 - Val Loss: 0.2759, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9200\n","           WALLETS: Train Loss: 0.26238531, Acc: 0.90040420, F1: 0.94351837 Bal: 0.8931 - Val Loss: 0.28851625, Accuracy: 0.8960, F1: 0.9409 Bal: 0.8756\n","Epoch 248:      TX: Train Loss: 0.0283, Acc: 0.9912, F1: 0.9951 Bal: 0.9946 - Val Loss: 0.2767, Accuracy: 0.9675, F1: 0.9819 Bal: 0.9201\n","           WALLETS: Train Loss: 0.26195431, Acc: 0.89919958, F1: 0.94278647 Bal: 0.8934 - Val Loss: 0.28807524, Accuracy: 0.8946, F1: 0.9401 Bal: 0.8758\n","Epoch 249:      TX: Train Loss: 0.0281, Acc: 0.9913, F1: 0.9952 Bal: 0.9947 - Val Loss: 0.2773, Accuracy: 0.9677, F1: 0.9820 Bal: 0.9212\n","           WALLETS: Train Loss: 0.26160544, Acc: 0.89681935, F1: 0.94133907 Bal: 0.8936 - Val Loss: 0.28751901, Accuracy: 0.8921, F1: 0.9386 Bal: 0.8758\n","Epoch 250:      TX: Train Loss: 0.0278, Acc: 0.9914, F1: 0.9952 Bal: 0.9947 - Val Loss: 0.2782, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9213\n","           WALLETS: Train Loss: 0.26127586, Acc: 0.90229097, F1: 0.94465761 Bal: 0.8932 - Val Loss: 0.28827524, Accuracy: 0.8975, F1: 0.9419 Bal: 0.8755\n","Epoch 251:      TX: Train Loss: 0.0275, Acc: 0.9914, F1: 0.9952 Bal: 0.9947 - Val Loss: 0.2789, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9213\n","           WALLETS: Train Loss: 0.26090837, Acc: 0.89576712, F1: 0.94069411 Bal: 0.8940 - Val Loss: 0.28700861, Accuracy: 0.8914, F1: 0.9381 Bal: 0.8772\n","Epoch 252:      TX: Train Loss: 0.0273, Acc: 0.9915, F1: 0.9953 Bal: 0.9948 - Val Loss: 0.2801, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.26047802, Acc: 0.90225468, F1: 0.94463068 Bal: 0.8937 - Val Loss: 0.28788123, Accuracy: 0.8974, F1: 0.9418 Bal: 0.8756\n","Epoch 253:      TX: Train Loss: 0.0270, Acc: 0.9916, F1: 0.9953 Bal: 0.9949 - Val Loss: 0.2804, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9213\n","           WALLETS: Train Loss: 0.26003253, Acc: 0.89795504, F1: 0.94202313 Bal: 0.8941 - Val Loss: 0.28690311, Accuracy: 0.8933, F1: 0.9393 Bal: 0.8763\n","Epoch 254:      TX: Train Loss: 0.0268, Acc: 0.9916, F1: 0.9953 Bal: 0.9950 - Val Loss: 0.2813, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25962099, Acc: 0.89996517, F1: 0.94324007 Bal: 0.8943 - Val Loss: 0.28700337, Accuracy: 0.8952, F1: 0.9405 Bal: 0.8761\n","Epoch 255:      TX: Train Loss: 0.0265, Acc: 0.9917, F1: 0.9954 Bal: 0.9950 - Val Loss: 0.2823, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25925785, Acc: 0.90130767, F1: 0.94405249 Bal: 0.8943 - Val Loss: 0.28704640, Accuracy: 0.8962, F1: 0.9411 Bal: 0.8758\n","Epoch 256:      TX: Train Loss: 0.0263, Acc: 0.9918, F1: 0.9954 Bal: 0.9950 - Val Loss: 0.2828, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25892368, Acc: 0.89791512, F1: 0.94199005 Bal: 0.8949 - Val Loss: 0.28628489, Accuracy: 0.8933, F1: 0.9393 Bal: 0.8779\n","Epoch 257:      TX: Train Loss: 0.0261, Acc: 0.9919, F1: 0.9955 Bal: 0.9951 - Val Loss: 0.2839, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25857967, Acc: 0.90317630, F1: 0.94517776 Bal: 0.8945 - Val Loss: 0.28708133, Accuracy: 0.8980, F1: 0.9422 Bal: 0.8756\n","Epoch 258:      TX: Train Loss: 0.0258, Acc: 0.9920, F1: 0.9955 Bal: 0.9952 - Val Loss: 0.2847, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25821033, Acc: 0.89750149, F1: 0.94173434 Bal: 0.8953 - Val Loss: 0.28589797, Accuracy: 0.8929, F1: 0.9391 Bal: 0.8782\n","Epoch 259:      TX: Train Loss: 0.0256, Acc: 0.9921, F1: 0.9956 Bal: 0.9952 - Val Loss: 0.2856, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25781146, Acc: 0.90319807, F1: 0.94518626 Bal: 0.8950 - Val Loss: 0.28671226, Accuracy: 0.8980, F1: 0.9422 Bal: 0.8754\n","Epoch 260:      TX: Train Loss: 0.0253, Acc: 0.9921, F1: 0.9956 Bal: 0.9952 - Val Loss: 0.2861, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9215\n","           WALLETS: Train Loss: 0.25740024, Acc: 0.89908347, F1: 0.94269177 Bal: 0.8956 - Val Loss: 0.28572571, Accuracy: 0.8944, F1: 0.9399 Bal: 0.8777\n","Epoch 261:      TX: Train Loss: 0.0251, Acc: 0.9923, F1: 0.9957 Bal: 0.9953 - Val Loss: 0.2873, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25699911, Acc: 0.90184105, F1: 0.94436207 Bal: 0.8955 - Val Loss: 0.28600556, Accuracy: 0.8968, F1: 0.9414 Bal: 0.8771\n","Epoch 262:      TX: Train Loss: 0.0249, Acc: 0.9923, F1: 0.9957 Bal: 0.9953 - Val Loss: 0.2880, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25662237, Acc: 0.90130767, F1: 0.94403730 Bal: 0.8957 - Val Loss: 0.28570729, Accuracy: 0.8962, F1: 0.9411 Bal: 0.8771\n","Epoch 263:      TX: Train Loss: 0.0247, Acc: 0.9923, F1: 0.9957 Bal: 0.9954 - Val Loss: 0.2886, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25626642, Acc: 0.90036792, F1: 0.94346638 Bal: 0.8959 - Val Loss: 0.28537977, Accuracy: 0.8955, F1: 0.9406 Bal: 0.8778\n","Epoch 264:      TX: Train Loss: 0.0244, Acc: 0.9924, F1: 0.9958 Bal: 0.9954 - Val Loss: 0.2896, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25592324, Acc: 0.90307107, F1: 0.94510228 Bal: 0.8957 - Val Loss: 0.28575030, Accuracy: 0.8981, F1: 0.9422 Bal: 0.8764\n","Epoch 265:      TX: Train Loss: 0.0242, Acc: 0.9925, F1: 0.9958 Bal: 0.9956 - Val Loss: 0.2902, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25558716, Acc: 0.89946445, F1: 0.94291365 Bal: 0.8964 - Val Loss: 0.28492317, Accuracy: 0.8947, F1: 0.9401 Bal: 0.8783\n","Epoch 266:      TX: Train Loss: 0.0240, Acc: 0.9926, F1: 0.9959 Bal: 0.9956 - Val Loss: 0.2912, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25525206, Acc: 0.90476916, F1: 0.94612160 Bal: 0.8960 - Val Loss: 0.28575987, Accuracy: 0.8996, F1: 0.9431 Bal: 0.8761\n","Epoch 267:      TX: Train Loss: 0.0238, Acc: 0.9926, F1: 0.9959 Bal: 0.9956 - Val Loss: 0.2917, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25492281, Acc: 0.89884762, F1: 0.94253523 Bal: 0.8968 - Val Loss: 0.28451338, Accuracy: 0.8940, F1: 0.9397 Bal: 0.8786\n","Epoch 268:      TX: Train Loss: 0.0236, Acc: 0.9927, F1: 0.9960 Bal: 0.9957 - Val Loss: 0.2926, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25458840, Acc: 0.90574157, F1: 0.94670480 Bal: 0.8962 - Val Loss: 0.28565308, Accuracy: 0.9002, F1: 0.9435 Bal: 0.8763\n","Epoch 269:      TX: Train Loss: 0.0234, Acc: 0.9927, F1: 0.9960 Bal: 0.9957 - Val Loss: 0.2931, Accuracy: 0.9686, F1: 0.9825 Bal: 0.9217\n","           WALLETS: Train Loss: 0.25425476, Acc: 0.89853195, F1: 0.94234009 Bal: 0.8971 - Val Loss: 0.28413758, Accuracy: 0.8937, F1: 0.9395 Bal: 0.8786\n","Epoch 270:      TX: Train Loss: 0.0232, Acc: 0.9928, F1: 0.9960 Bal: 0.9958 - Val Loss: 0.2944, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9218\n","           WALLETS: Train Loss: 0.25390294, Acc: 0.90623140, F1: 0.94699709 Bal: 0.8964 - Val Loss: 0.28545725, Accuracy: 0.9006, F1: 0.9437 Bal: 0.8762\n","Epoch 271:      TX: Train Loss: 0.0230, Acc: 0.9928, F1: 0.9960 Bal: 0.9958 - Val Loss: 0.2948, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9218\n","           WALLETS: Train Loss: 0.25353870, Acc: 0.89907621, F1: 0.94266686 Bal: 0.8974 - Val Loss: 0.28387812, Accuracy: 0.8939, F1: 0.9396 Bal: 0.8785\n","Epoch 272:      TX: Train Loss: 0.0227, Acc: 0.9929, F1: 0.9960 Bal: 0.9958 - Val Loss: 0.2959, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9218\n","           WALLETS: Train Loss: 0.25315168, Acc: 0.90605361, F1: 0.94688559 Bal: 0.8969 - Val Loss: 0.28503019, Accuracy: 0.9002, F1: 0.9435 Bal: 0.8765\n","Epoch 273:      TX: Train Loss: 0.0225, Acc: 0.9929, F1: 0.9960 Bal: 0.9958 - Val Loss: 0.2966, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25275832, Acc: 0.90063279, F1: 0.94360542 Bal: 0.8979 - Val Loss: 0.28375426, Accuracy: 0.8953, F1: 0.9405 Bal: 0.8780\n","Epoch 274:      TX: Train Loss: 0.0223, Acc: 0.9930, F1: 0.9961 Bal: 0.9958 - Val Loss: 0.2976, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25236973, Acc: 0.90473651, F1: 0.94608708 Bal: 0.8975 - Val Loss: 0.28437644, Accuracy: 0.8991, F1: 0.9428 Bal: 0.8765\n","Epoch 275:      TX: Train Loss: 0.0222, Acc: 0.9930, F1: 0.9961 Bal: 0.9959 - Val Loss: 0.2983, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25199977, Acc: 0.90254858, F1: 0.94476345 Bal: 0.8979 - Val Loss: 0.28378457, Accuracy: 0.8971, F1: 0.9416 Bal: 0.8774\n","Epoch 276:      TX: Train Loss: 0.0220, Acc: 0.9930, F1: 0.9961 Bal: 0.9959 - Val Loss: 0.2994, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25164789, Acc: 0.90324524, F1: 0.94518560 Bal: 0.8977 - Val Loss: 0.28377649, Accuracy: 0.8978, F1: 0.9420 Bal: 0.8775\n","Epoch 277:      TX: Train Loss: 0.0218, Acc: 0.9932, F1: 0.9962 Bal: 0.9960 - Val Loss: 0.3003, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25130919, Acc: 0.90455509, F1: 0.94597220 Bal: 0.8980 - Val Loss: 0.28388363, Accuracy: 0.8988, F1: 0.9426 Bal: 0.8769\n","Epoch 278:      TX: Train Loss: 0.0216, Acc: 0.9932, F1: 0.9962 Bal: 0.9960 - Val Loss: 0.3009, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25098330, Acc: 0.90213858, F1: 0.94451017 Bal: 0.8984 - Val Loss: 0.28327858, Accuracy: 0.8963, F1: 0.9411 Bal: 0.8772\n","Epoch 279:      TX: Train Loss: 0.0214, Acc: 0.9934, F1: 0.9963 Bal: 0.9961 - Val Loss: 0.3021, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9219\n","           WALLETS: Train Loss: 0.25066981, Acc: 0.90631849, F1: 0.94703132 Bal: 0.8982 - Val Loss: 0.28396139, Accuracy: 0.9005, F1: 0.9437 Bal: 0.8765\n","Epoch 280:      TX: Train Loss: 0.0212, Acc: 0.9935, F1: 0.9964 Bal: 0.9961 - Val Loss: 0.3026, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.25037652, Acc: 0.90087952, F1: 0.94374081 Bal: 0.8992 - Val Loss: 0.28279945, Accuracy: 0.8950, F1: 0.9403 Bal: 0.8777\n","Epoch 281:      TX: Train Loss: 0.0210, Acc: 0.9937, F1: 0.9965 Bal: 0.9962 - Val Loss: 0.3035, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.25010729, Acc: 0.90811091, F1: 0.94810823 Bal: 0.8981 - Val Loss: 0.28421447, Accuracy: 0.9025, F1: 0.9449 Bal: 0.8769\n","Epoch 282:      TX: Train Loss: 0.0208, Acc: 0.9937, F1: 0.9965 Bal: 0.9962 - Val Loss: 0.3040, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.24987987, Acc: 0.89922860, F1: 0.94273644 Bal: 0.8995 - Val Loss: 0.28237665, Accuracy: 0.8937, F1: 0.9395 Bal: 0.8787\n","Epoch 283:      TX: Train Loss: 0.0207, Acc: 0.9937, F1: 0.9965 Bal: 0.9962 - Val Loss: 0.3055, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.24964923, Acc: 0.90998316, F1: 0.94923128 Bal: 0.8978 - Val Loss: 0.28462115, Accuracy: 0.9041, F1: 0.9458 Bal: 0.8765\n","Epoch 284:      TX: Train Loss: 0.0205, Acc: 0.9937, F1: 0.9965 Bal: 0.9962 - Val Loss: 0.3054, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.24938263, Acc: 0.89824894, F1: 0.94213703 Bal: 0.8999 - Val Loss: 0.28206474, Accuracy: 0.8926, F1: 0.9388 Bal: 0.8789\n","Epoch 285:      TX: Train Loss: 0.0203, Acc: 0.9938, F1: 0.9965 Bal: 0.9963 - Val Loss: 0.3069, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.24896005, Acc: 0.91012830, F1: 0.94931334 Bal: 0.8983 - Val Loss: 0.28435484, Accuracy: 0.9043, F1: 0.9460 Bal: 0.8766\n","Epoch 286:      TX: Train Loss: 0.0201, Acc: 0.9939, F1: 0.9966 Bal: 0.9965 - Val Loss: 0.3073, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9230\n","           WALLETS: Train Loss: 0.24844579, Acc: 0.90097023, F1: 0.94378477 Bal: 0.9002 - Val Loss: 0.28205308, Accuracy: 0.8947, F1: 0.9402 Bal: 0.8780\n","Epoch 287:      TX: Train Loss: 0.0200, Acc: 0.9940, F1: 0.9967 Bal: 0.9966 - Val Loss: 0.3083, Accuracy: 0.9695, F1: 0.9830 Bal: 0.9232\n","           WALLETS: Train Loss: 0.24796209, Acc: 0.90659062, F1: 0.94717359 Bal: 0.9003 - Val Loss: 0.28289369, Accuracy: 0.9004, F1: 0.9436 Bal: 0.8775\n","Epoch 288:      TX: Train Loss: 0.0198, Acc: 0.9942, F1: 0.9968 Bal: 0.9966 - Val Loss: 0.3089, Accuracy: 0.9695, F1: 0.9830 Bal: 0.9232\n","           WALLETS: Train Loss: 0.24763782, Acc: 0.90655796, F1: 0.94715241 Bal: 0.9004 - Val Loss: 0.28276321, Accuracy: 0.9001, F1: 0.9434 Bal: 0.8774\n","Epoch 289:      TX: Train Loss: 0.0196, Acc: 0.9942, F1: 0.9968 Bal: 0.9967 - Val Loss: 0.3096, Accuracy: 0.9695, F1: 0.9830 Bal: 0.9232\n","           WALLETS: Train Loss: 0.24743418, Acc: 0.90193176, F1: 0.94436119 Bal: 0.9007 - Val Loss: 0.28178912, Accuracy: 0.8956, F1: 0.9407 Bal: 0.8787\n","Epoch 290:      TX: Train Loss: 0.0195, Acc: 0.9944, F1: 0.9969 Bal: 0.9967 - Val Loss: 0.3110, Accuracy: 0.9695, F1: 0.9830 Bal: 0.9232\n","           WALLETS: Train Loss: 0.24723017, Acc: 0.90995777, F1: 0.94919543 Bal: 0.9000 - Val Loss: 0.28351837, Accuracy: 0.9040, F1: 0.9458 Bal: 0.8776\n","Epoch 291:      TX: Train Loss: 0.0193, Acc: 0.9943, F1: 0.9968 Bal: 0.9967 - Val Loss: 0.3108, Accuracy: 0.9695, F1: 0.9830 Bal: 0.9232\n","           WALLETS: Train Loss: 0.24693975, Acc: 0.90095572, F1: 0.94376495 Bal: 0.9012 - Val Loss: 0.28141168, Accuracy: 0.8947, F1: 0.9401 Bal: 0.8791\n","Epoch 292:      TX: Train Loss: 0.0191, Acc: 0.9944, F1: 0.9969 Bal: 0.9967 - Val Loss: 0.3123, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9233\n","           WALLETS: Train Loss: 0.24652109, Acc: 0.90956590, F1: 0.94895468 Bal: 0.9006 - Val Loss: 0.28305805, Accuracy: 0.9034, F1: 0.9454 Bal: 0.8774\n","Epoch 293:      TX: Train Loss: 0.0190, Acc: 0.9944, F1: 0.9969 Bal: 0.9968 - Val Loss: 0.3128, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9233\n","           WALLETS: Train Loss: 0.24608563, Acc: 0.90407614, F1: 0.94564862 Bal: 0.9014 - Val Loss: 0.28158069, Accuracy: 0.8978, F1: 0.9420 Bal: 0.8787\n","Epoch 294:      TX: Train Loss: 0.0188, Acc: 0.9945, F1: 0.9969 Bal: 0.9969 - Val Loss: 0.3133, Accuracy: 0.9699, F1: 0.9833 Bal: 0.9234\n","           WALLETS: Train Loss: 0.24573171, Acc: 0.90586857, F1: 0.94672534 Bal: 0.9017 - Val Loss: 0.28180316, Accuracy: 0.8996, F1: 0.9431 Bal: 0.8778\n","Epoch 295:      TX: Train Loss: 0.0186, Acc: 0.9945, F1: 0.9969 Bal: 0.9969 - Val Loss: 0.3143, Accuracy: 0.9704, F1: 0.9835 Bal: 0.9237\n","           WALLETS: Train Loss: 0.24546997, Acc: 0.90849189, F1: 0.94830060 Bal: 0.9017 - Val Loss: 0.28223929, Accuracy: 0.9021, F1: 0.9446 Bal: 0.8779\n","Epoch 296:      TX: Train Loss: 0.0185, Acc: 0.9945, F1: 0.9969 Bal: 0.9969 - Val Loss: 0.3148, Accuracy: 0.9704, F1: 0.9835 Bal: 0.9237\n","           WALLETS: Train Loss: 0.24524277, Acc: 0.90304567, F1: 0.94501976 Bal: 0.9020 - Val Loss: 0.28101173, Accuracy: 0.8968, F1: 0.9414 Bal: 0.8798\n","Epoch 297:      TX: Train Loss: 0.0183, Acc: 0.9946, F1: 0.9970 Bal: 0.9970 - Val Loss: 0.3159, Accuracy: 0.9704, F1: 0.9835 Bal: 0.9237\n","           WALLETS: Train Loss: 0.24497335, Acc: 0.91034963, F1: 0.94941115 Bal: 0.9019 - Val Loss: 0.28259584, Accuracy: 0.9043, F1: 0.9460 Bal: 0.8785\n","Epoch 298:      TX: Train Loss: 0.0182, Acc: 0.9946, F1: 0.9970 Bal: 0.9970 - Val Loss: 0.3159, Accuracy: 0.9704, F1: 0.9835 Bal: 0.9237\n","           WALLETS: Train Loss: 0.24463983, Acc: 0.90329241, F1: 0.94516351 Bal: 0.9025 - Val Loss: 0.28082982, Accuracy: 0.8969, F1: 0.9415 Bal: 0.8792\n","Epoch 299:      TX: Train Loss: 0.0180, Acc: 0.9946, F1: 0.9970 Bal: 0.9970 - Val Loss: 0.3172, Accuracy: 0.9704, F1: 0.9835 Bal: 0.9237\n","           WALLETS: Train Loss: 0.24425855, Acc: 0.90935182, F1: 0.94881131 Bal: 0.9022 - Val Loss: 0.28202748, Accuracy: 0.9029, F1: 0.9451 Bal: 0.8784\n","Epoch 300:      TX: Train Loss: 0.0179, Acc: 0.9947, F1: 0.9971 Bal: 0.9971 - Val Loss: 0.3174, Accuracy: 0.9706, F1: 0.9837 Bal: 0.9238\n","           WALLETS: Train Loss: 0.24389142, Acc: 0.90600282, F1: 0.94679619 Bal: 0.9026 - Val Loss: 0.28102005, Accuracy: 0.8997, F1: 0.9431 Bal: 0.8792\n","Final_result for tx\n","{'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'SAGE', 'p': 5, 'factor': 0.2, 'eta_min': None, 'T_max': None, 'aggr': 'mean', 'lr_scheduler': 'ReduceLROnPlateau', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99, 'epoch': 300}\n","Epoch 300:\n","  TX:\n","   Train: Loss=0.0179, Acc=0.9947, F1=0.9971, Bal. Acc=0.9971\n","   Val:   Loss=0.3174, Acc=0.9706, F1=0.9837, Bal. Acc=0.9238\n","   Test:  Loss=0.3480, Acc=0.9664, F1=0.9813, Bal. Acc=0.9129\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.87      0.85       453\n","           1       0.99      0.98      0.98      4105\n","\n","    accuracy                           0.97      4558\n","   macro avg       0.91      0.92      0.92      4558\n","weighted avg       0.97      0.97      0.97      4558\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.85      0.83       665\n","           1       0.98      0.98      0.98      6172\n","\n","    accuracy                           0.97      6837\n","   macro avg       0.90      0.91      0.91      6837\n","weighted avg       0.97      0.97      0.97      6837\n","\n","  WALLETS:\n","   Train: Loss=0.24389142, Acc=0.90600282, F1=0.94679619, Bal. Acc=0.9026\n","   Val:   Loss=0.28102005, Acc=0.8997, F1=0.9431, Bal. Acc=0.8792\n","   Test:  Loss=0.28093967, Acc=0.8996, F1=0.9431, Bal. Acc=0.8777\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.85      0.57      2906\n","           1       0.99      0.90      0.94     33841\n","\n","    accuracy                           0.90     36747\n","   macro avg       0.71      0.88      0.76     36747\n","weighted avg       0.94      0.90      0.91     36747\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.85      0.57      4340\n","           1       0.99      0.90      0.94     50781\n","\n","    accuracy                           0.90     55121\n","   macro avg       0.71      0.88      0.76     55121\n","weighted avg       0.94      0.90      0.91     55121\n","\n","\n"]}],"source":["hyperparams = {\n","    \"hidden_channels\": [128],\n","    'num_head': ['/'],\n","    \"num_layers\": [2],\n","    \"num_epoch\": [300],\n","    \"patience\": [50],\n","    \"lr\": [0.001],\n","    \"weight_decay\": [1e-5],\n","    \"dropout\": [0],\n","    \"conv_type\": ['SAGE'],\n","    \"p\": [5],\n","    \"factor\": [0.5],\n","    \"eta_min\": ['/'],\n","    \"T_max\": ['/'], #10 15\n","    \"aggr\": ['mean'], #,\n","    'lr_scheduler':['ReduceLROnPlateau'],\n","    'optimizer': ['Adam'], #Adam\n","    'type_model':['HeteroGNN'],\n","}\n","\n","scaler = ['standard_l2']\n","dim_reduction=['pca']\n","pca_threshold=[0.99]\n","best_model = train_grid(data, hyperparams, scaler, dim_reduction, pca_threshold)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIoomnaeh74t1AKH1zYUzUr42JRiCeq-","timestamp":1739893838211}],"collapsed_sections":["fPrQ3dOEx291"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}