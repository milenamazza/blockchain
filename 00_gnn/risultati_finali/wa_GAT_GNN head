{"cells":[{"cell_type":"markdown","source":["# Addestramento\n","addestramento del migliore modello GAT per ottenere le performance di classificazione binari adei wallet per il test set"],"metadata":{"id":"jFTKGWdI4ZXT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iidXjXM7pXO"},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","!pip install torch_geometric\n","#!pip install torch-sparse\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","import os.path as osp\n","import torch\n","import warnings\n","from torch_geometric.data import Data, HeteroData\n","from torch_geometric.transforms import RandomNodeSplit\n","from torch_geometric.nn import GCNConv, GATConv, SAGEConv, ChebConv\n","import torch_geometric.nn as pyg_nn\n","import torch.nn as nn\n","import torch_geometric.utils as pyg_utils\n","from torch.nn import Module, Linear\n","import torch.nn.functional as F\n","from sklearn.metrics import precision_recall_fscore_support, f1_score, classification_report\n","from torch_geometric.seed import seed_everything\n","import joblib\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","warnings.simplefilter(action='ignore')\n","SEED = 51\n","FILEPATH_TX = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/tx_result_4.csv\"\n","FILEPATH_WALLET = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/w_result_4.csv\"\n","\n","base_path = \"/content/drive/MyDrive/blockchain/E++/\"\n","path_comb = '/content/drive/MyDrive/blockchain/00_gnn/combination_do.csv'\n","\n","type_classification = 'w'"]},{"cell_type":"markdown","metadata":{"id":"XOk9zPoYDC5I"},"source":["## Crea db vuoto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCrI1kO2CYxi"},"outputs":[],"source":["def create_df():\n","  # Crea DataFrame vuoti\n","  df_tx = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","  df_wallet = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","\n","  # Salva i DataFrame come file CSV\n","  df_tx.to_csv(FILEPATH_TX, index=False)\n","  df_wallet.to_csv(FILEPATH_WALLET, index=False)\n","\n","#create_df()"]},{"cell_type":"markdown","metadata":{"id":"W1Syx8hzDPvM"},"source":["##Carica dati"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qobTcbrVqpwG"},"outputs":[],"source":["def load_data():\n","    # Loading transactions\n","\n","    #Reading edges, features and classes from transaction files (as done with the original dataset)\n","    df_edges_tx = pd.read_csv(osp.join(base_path, \"txs_edgelist.csv\"))\n","    df_features_tx = pd.read_csv(osp.join(base_path, \"txs_features.csv\"), header=None)\n","    df_classes_tx = pd.read_csv(osp.join(base_path, \"txs_classes.csv\"))\n","\n","    #Columns naming based on index\n","    colNames1_tx = {'0': 'txId', 1: \"Time step\"}\n","    colNames2_tx = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(94)}\n","    colNames3_tx = {str(ii+96): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n","\n","    colNames_tx = dict(colNames1_tx, **colNames2_tx, **colNames3_tx)\n","    colNames_tx = {int(jj): item_kk for jj, item_kk in colNames_tx.items()}\n","\n","    # Rename feature columns\n","    df_features_tx = df_features_tx.rename(columns=colNames_tx)\n","\n","    # Map unknown class to '3'\n","    df_classes_tx.loc[df_classes_tx['class'] == 'unknown', 'class'] = '3'\n","\n","    # Merge classes and features in one Dataframe\n","    df_class_feature_tx = pd.merge(df_classes_tx, df_features_tx)\n","\n","    # Exclude records with unknown class transaction\n","    df_class_feature_tx = df_class_feature_tx[df_class_feature_tx['class'] != 3]\n","\n","    # Build Dataframe with head and tail of transactions (edges)\n","    known_txs = df_class_feature_tx[\"txId\"].values\n","    df_edges_tx = df_edges_tx[(df_edges_tx[\"txId1\"].isin(known_txs)) & (df_edges_tx[\"txId2\"].isin(known_txs))]\n","\n","    # Build indices for features and edge types\n","    features_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"txId\"].unique()))}\n","    class_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_tx[\"txId\"] = df_class_feature_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_class_feature_tx[\"class\"] = df_class_feature_tx[\"class\"].apply(lambda name: class_idx_tx[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_tx[\"txId1\"] = df_edges_tx[\"txId1\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx[\"txId2\"] = df_edges_tx[\"txId2\"].apply(lambda name: features_idx_tx[name])\n","\n","    # Loading wallets\n","\n","    # From file\n","    df_edges_wallet = pd.read_csv(osp.join(base_path, \"AddrAddr_edgelist.csv\"))\n","    df_class_feature_wallet = pd.read_csv(osp.join(base_path, \"wallets_features_classes_combined.csv\"))\n","\n","    # Exclude records with unknown class transaction\n","    #print(df_class_feature_wallet.shape)\n","    df_class_feature_wallet = df_class_feature_wallet[df_class_feature_wallet[\"class\"] != 3]\n","    #print(df_class_feature_wallet.shape)\n","\n","    # Build Dataframe with head and tail of AddrToAddr (edges)\n","    known_wallets = df_class_feature_wallet[\"address\"].values\n","    df_edges_wallet = df_edges_wallet[(df_edges_wallet[\"input_address\"].isin(known_wallets)) & (df_edges_wallet[\"output_address\"].isin(known_wallets))]\n","\n","    # Building indices for features and edge types\n","    features_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"address\"].unique()))}\n","    class_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_wallet[\"address\"] = df_class_feature_wallet[\"address\"].apply(lambda name: features_idx_wallet[name])\n","    df_class_feature_wallet[\"class\"] = df_class_feature_wallet[\"class\"].apply(lambda name: class_idx_wallet[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet[\"input_address\"] = df_edges_wallet[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet[\"output_address\"] = df_edges_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    # Loading AddrTx and TxAddr\n","\n","    # From file\n","    df_edges_wallet_tx = pd.read_csv(osp.join(base_path, \"AddrTx_edgelist.csv\"))\n","    df_edges_tx_wallet = pd.read_csv(osp.join(base_path, \"TxAddr_edgelist.csv\"))\n","\n","    # Build Dataframe with head and tail of AddrTx (edges)\n","    df_edges_wallet_tx = df_edges_wallet_tx[(df_edges_wallet_tx[\"input_address\"].isin(known_wallets)) & df_edges_wallet_tx[\"txId\"].isin(known_txs)]\n","    df_edges_tx_wallet = df_edges_tx_wallet[(df_edges_tx_wallet[\"txId\"].isin(known_txs)) & df_edges_tx_wallet[\"output_address\"].isin(known_wallets)]\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet_tx[\"input_address\"] = df_edges_wallet_tx[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet_tx[\"txId\"] = df_edges_wallet_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"txId\"] = df_edges_tx_wallet[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"output_address\"] = df_edges_tx_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    return df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet\n","\n","def data_to_pyg(df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet):\n","    data = HeteroData()\n","\n","    # Defining PyG objects for transactions\n","    df_class_feature_tx = df_class_feature_tx.fillna(0)\n","    data['tx'].x = torch.tensor(df_class_feature_tx.iloc[:, 3:].values, dtype=torch.float)\n","    data['tx'].y = torch.tensor(df_class_feature_tx[\"class\"].values, dtype=torch.long)\n","    data['tx','is_related_to','tx'].edge_index = torch.tensor([df_edges_tx[\"txId1\"].values,\n","                            df_edges_tx[\"txId2\"].values], dtype=torch.int64)\n","    #data['tx'] = random_node_split(num_val=0.15, num_test=0.2)(data['tx'])\n","    # Defining PyG objects for wallets\n","    data['wallet'].x = torch.tensor(df_class_feature_wallet.iloc[:, 3:].values, dtype=torch.float)\n","    data['wallet'].y = torch.tensor(df_class_feature_wallet[\"class\"].values, dtype=torch.long)\n","    data['wallet','interacts_with','wallet'].edge_index = torch.tensor([df_edges_wallet[\"input_address\"].values,\n","                            df_edges_wallet[\"output_address\"].values], dtype=torch.int64)\n","    #data['wallet'] = random_node_split(num_val=0.15, num_test=0.2)(data['wallet'])\n","    # Defining PyG objects for cross-edges\n","    data['wallet','performs','tx'].edge_index = torch.tensor([df_edges_wallet_tx[\"input_address\"].values,\n","                                         df_edges_wallet_tx[\"txId\"].values], dtype=torch.int64)\n","\n","    data['tx', 'flows_into', 'wallet'].edge_index = torch.tensor([df_edges_tx_wallet[\"txId\"].values,\n","                                         df_edges_tx_wallet[\"output_address\"].values], dtype=torch.int64)\n","\n","    # Impostare il seed per la divisione del dataset\n","    return RandomNodeSplit(num_val=0.10, num_test=0.15)(data)"]},{"cell_type":"markdown","metadata":{"id":"OhFPkCpExzOL"},"source":["##Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQfEkZ1nx44T"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do2dQUF-x7hj"},"outputs":[],"source":["#NEW -> SU RANGE\n","# Utility per conversione a tensor\n","def to_tensor(arr):\n","    return torch.tensor(arr, dtype=torch.float).to(device)\n","\n","def scale_features(data, method=\"standard\"):\n","  if method == 'no':\n","    return data\n","\n","  # Scaling per training\n","  data['tx'].x[data['tx'].train_mask] = to_tensor(\n","      scale_train_data(data['tx'].x[data['tx'].train_mask].cpu().numpy(), method, 'tx')\n","  )\n","  data['wallet'].x[data['wallet'].train_mask] = to_tensor(\n","      scale_train_data(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), method, 'wallet')\n","  )\n","\n","  # Scaling per validation\n","  data['tx'].x[data['tx'].val_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].val_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].val_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), method, 'wallet'))\n","\n","  data['tx'].x[data['tx'].test_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].test_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].test_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), method, 'wallet'))\n","  return data\n","\n","def scale_train_data(train, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = StandardScaler()\n","        scaled_train = scaler.fit_transform(train)  # Scala tutte le colonne\n","        joblib.dump(scaler, f\"scaler_standard_{df}.pkl\")\n","\n","        if 'l2' in scaling_method:\n","            norm = Normalizer(norm='l2')\n","            scaled_train = norm.fit_transform(scaled_train)\n","            joblib.dump(norm, f\"scaler_l2_{df}.pkl\")\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_train\n","\n","def scale_validation_data(val, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = joblib.load(f\"scaler_standard_{df}.pkl\")\n","        scaled_val = scaler.transform(val)  # Scala tutte le colonne\n","\n","        if 'l2' in scaling_method:\n","            norm = joblib.load(f\"scaler_l2_{df}.pkl\")\n","            scaled_val = norm.transform(scaled_val)\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_val"]},{"cell_type":"code","source":["def dimentional_reduction(data, dim_reduction, pca_threshold):\n","    if dim_reduction == 'no':\n","        return data\n","    elif dim_reduction == 'pca':\n","        data1 = copy.deepcopy(data)\n","\n","        transformed_tx_data = apply_pca_train(data['tx'].x[data['tx'].train_mask].cpu().numpy(), 'tx', pca_threshold)\n","        transformed_wallet_data = apply_pca_train(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), 'wallet', pca_threshold)\n","\n","        data1['tx'].x = torch.zeros((data['tx'].x.shape[0], transformed_tx_data.shape[1]), dtype=torch.float, device=device)\n","        data1['wallet'].x = torch.zeros((data['wallet'].x.shape[0], transformed_wallet_data.shape[1]), dtype=torch.float, device=device)\n","\n","        data1['tx'].x[data['tx'].train_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].train_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].val_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].val_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].val_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].test_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].test_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].test_mask] = to_tensor(transformed_wallet_data)\n","\n","        return data1\n","\n","def apply_pca_train(train, df, pca_threshold=0.99):\n","    pca = PCA(random_state=SEED)\n","    pca.fit(train)\n","\n","    # Selezione componenti principali\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","\n","    pca = PCA(n_components=n_components, random_state=SEED)\n","    transformed_data = pca.fit_transform(train).astype(np.float32)\n","\n","    joblib.dump(pca, f\"pca_model_{df}.pkl\")\n","    print(f\"  Numero di componenti principali per {df}: {pca.n_components_}\")\n","\n","    return transformed_data\n","\n","def apply_pca_validation(val, df):\n","    pca = joblib.load(f\"pca_model_{df}.pkl\")\n","    transformed_data = pca.transform(val).astype(np.float32)\n","    return transformed_data"],"metadata":{"id":"PVRMmabn21eP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CaNNtrNLDSdI"},"source":["##Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbfXKtdynFBL"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, Dropout\n","from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, TransformerConv\n","import random\n","from itertools import product\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxfbqge_p64q"},"outputs":[],"source":["def set_seed(seed = 51):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # Per più GPU\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    seed_everything(seed)\n","device = \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiBF-BZYfq7b"},"outputs":[],"source":["class ResidualHeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0.5, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.skips = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1 # Define heads\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","            self.skips.append(Linear(hidden_channels * heads, hidden_channels * heads)) # Fix: Linear layer expects the output of conv\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv, skip in zip(self.convs, self.skips):\n","            x_dict_new = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(F.relu(x + skip(x_dict_new[key]))) for key, x in x_dict_new.items()}  # Residual + Dropout\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdcebmslq5D0"},"outputs":[],"source":["class HeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1  # Definiamo i heads solo se necessario\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv in self.convs:\n","            x_dict = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJEJVt5l04Eb"},"outputs":[],"source":["def is_combination_tested(filepath, new_row):\n","    existing_results = pd.read_csv(filepath)\n","\n","    # Identifica le colonne comuni tra il dataset e new_row\n","    dataset_columns = set(existing_results.columns)\n","    comparison_columns = [col for col in new_row.keys() if col not in ['end', 'num_epoch', 'epoch']]\n","\n","    # Filtra le combinazioni\n","    filtered_results = existing_results.copy()\n","    #filtered_results = filtered_results[filtered_results['end'] == True]\n","    filtered_results = filtered_results[filtered_results['num_epoch'] >= new_row['num_epoch']]\n","\n","    for col in comparison_columns:\n","        if col in dataset_columns:\n","            # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","            filtered_results = filtered_results[\n","                (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","            ]\n","\n","    return not filtered_results.empty\n","\n","def append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, end=False):\n","  def append_and_save_result(filepath, new_row, end=False):\n","    new_row['end'] = end\n","    # Leggi i risultati esistenti\n","    results_df = pd.read_csv(filepath)\n","    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","    results_df.to_csv(filepath, index=False)\n","\n","  append_and_save_result(FILEPATH_TX, params_tx, end)\n","  append_and_save_result(FILEPATH_WALLET, params_wallet, end)\n","  if end:\n","    df_comb = pd.read_csv(path_comb)\n","    filtered_params = {key: params_tx[key] for key in params_tx if \"train\" not in key and \"val\" not in key}\n","    print(filtered_params)\n","    df_comb = pd.concat([df_comb, pd.DataFrame([filtered_params])], ignore_index=True)\n","    df_comb.to_csv(path_comb, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vo1zTF6zxQR"},"outputs":[],"source":["def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def eval(model, data, out_tx, out_wallet, params):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalità di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  loss = params_tx['train_loss'] + params_wallet['train_loss']\n","\n","  with torch.no_grad():\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for wallets\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    print(f\"Epoch {str(params['epoch']).zfill(3)}:      TX: Train Loss: {params_tx['train_loss']:.4f}, Acc: {params_tx['train_acc']:.4f}, F1: {params_tx['train_f1']:.4f} Bal: {params_tx['train_balanced_acc']:.4f} - Val Loss: {params_tx['val_loss']:.4f}, Accuracy: {params_tx['val_acc']:.4f}, F1: {params_tx['val_f1']:.4f} Bal: {params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"           WALLETS: Train Loss: {params_wallet['train_loss']:.8f}, Acc: {params_wallet['train_acc']:.8f}, F1: {params_wallet['train_f1']:.8f} Bal: {params_wallet['train_balanced_acc']:.4f} - Val Loss: {params_wallet['val_loss']:.8f}, Accuracy: {params_wallet['val_acc']:.4f}, F1: {params_wallet['val_f1']:.4f} Bal: {params_wallet['val_balanced_acc']:.4f}\")\n","\n","  return loss, params_tx, params_wallet\n","\n","\n","def eval_total(model, data, out_tx, out_wallet, params, best_epoch):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalitÃ  di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","  tx_mask_test = data['tx'].test_mask\n","  wallet_mask_test = data['wallet'].test_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  with torch.no_grad():\n","    # Calculate metrics for validation\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    report_tx_val = classification_report(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    report_wallet_val = classification_report(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for test\n","    params_tx['test_loss'] = criterion(out_tx[tx_mask_test], data['tx'].y[tx_mask_test].cpu())  # Convert to scalar\n","    params_tx['test_acc'] = accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_precision'] = precision_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_recall'] = recall_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_f1'] = f1_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    report_tx_test = classification_report(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","\n","    params_wallet['test_loss'] = criterion(out_wallet[wallet_mask_test], data['wallet'].y[wallet_mask_test].cpu())  # Convert to scalar\n","    params_wallet['test_acc'] = accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_precision'] = precision_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_recall'] = recall_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_f1'] = f1_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    report_wallet_test = classification_report(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","\n","    # Stampa delle metriche con formattazione migliorata\n","    print('Final_result for '+type_classification)\n","    print(params)\n","    print(f\"Epoch {best_epoch}:\")\n","    print(\"  TX:\")\n","    print(f\"   Train: Loss={params_tx['train_loss']:.4f}, Acc={params_tx['train_acc']:.4f}, F1={params_tx['train_f1']:.4f}, Bal. Acc={params_tx['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_tx['val_loss']:.4f}, Acc={params_tx['val_acc']:.4f}, F1={params_tx['val_f1']:.4f}, Bal. Acc={params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_tx['test_loss']:.4f}, Acc={params_tx['test_acc']:.4f}, F1={params_tx['test_f1']:.4f}, Bal. Acc={params_tx['test_balanced_acc']:.4f}\")\n","    print(report_tx_val)\n","    print(report_tx_test)\n","    print(\"  WALLETS:\")\n","    print(f\"   Train: Loss={params_wallet['train_loss']:.8f}, Acc={params_wallet['train_acc']:.8f}, F1={params_wallet['train_f1']:.8f}, Bal. Acc={params_wallet['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_wallet['val_loss']:.8f}, Acc={params_wallet['val_acc']:.4f}, F1={params_wallet['val_f1']:.4f}, Bal. Acc={params_wallet['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_wallet['test_loss']:.8f}, Acc={params_wallet['test_acc']:.4f}, F1={params_wallet['test_f1']:.4f}, Bal. Acc={params_wallet['test_balanced_acc']:.4f}\")\n","    print(report_wallet_val)\n","    print(report_wallet_test)\n","    print()\n","\n","def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def train(model, data, params):\n","    best_model = None\n","    best_epoch = None\n","\n","    if params['optimizer'] == 'Adam':\n","      optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    elif params['optimizer'] == 'AdamW':\n","      optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    else:\n","      optimizer = None\n","\n","    if params['lr_scheduler'] == 'ReduceLROnPlateau':\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=params['factor'], patience=params['p'])\n","    elif params['lr_scheduler'] == 'CosineAnnealingLR':\n","      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=params['eta_min'])\n","    elif params['lr_scheduler'] == 'StepLR':\n","      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n","    else:\n","      scheduler = None\n","\n","    print(f'Combinazione: {params}')\n","    model.train()\n","    tx_mask = data['tx'].train_mask\n","    wallet_mask = data['wallet'].train_mask\n","    tx_mask_val =  data['tx'].val_mask\n","    wallet_mask_val = data['wallet'].val_mask\n","\n","    best_val_tx_acc = 0\n","    best_val_wallet_acc = 0\n","\n","    best_val_tx_loss = float('inf')\n","    best_val_wallet_loss = float('inf')\n","    patience = params['patience']\n","    epochs_since_best = 0\n","\n","    for epoch in range(params['num_epoch']):\n","        params['epoch'] = epoch+1\n","        optimizer.zero_grad()\n","        out_tx, out_wallet = model(data.x_dict, data.edge_index_dict)\n","        loss, params_tx, params_wallet = eval(model, data, out_tx, out_wallet, params)\n","\n","        val_tx_loss = params_tx['val_loss']\n","        val_wallet_loss = params_wallet['val_loss']\n","        val_tx_acc = params_tx['val_balanced_acc']\n","        val_wallet_acc = params_wallet['val_balanced_acc']\n","\n","        # Check if validation loss has improved\n","        if val_tx_loss < best_val_tx_loss or val_wallet_loss < best_val_wallet_loss:\n","            best_val_tx_loss = val_tx_loss\n","            best_val_wallet_loss = val_wallet_loss\n","            epochs_since_best = 0\n","        else:\n","            epochs_since_best += 1\n","\n","        # Check if early stopping criteria is met\n","        if epochs_since_best >= patience:\n","            print(f'Early stopping at epoch {epoch}')\n","            append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, True)\n","            out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","            eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","            return best_model\n","\n","        if type_classification == 'w':\n","          if best_val_wallet_acc < val_wallet_acc:\n","            best_val_wallet_acc = val_wallet_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        elif type_classification == 'tx':\n","          if best_val_tx_acc < val_tx_acc:\n","            best_val_tx_acc = val_tx_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        else:\n","          print('Definisci modello da considerare')\n","          raise ValueError\n","\n","        loss.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","        scheduler.step(loss)\n","\n","        append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, params['epoch']==params['num_epoch'])\n","\n","    out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","    eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","    return model\n","\n","\n","def train_grid(data_full, param_grid, scalers, dim_reductions, pca_thresholds):\n","    best_model = None\n","    best_f1 = 0\n","\n","    keys, values = zip(*param_grid.items())\n","    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","    combination_counter = 0\n","    total_combinations = len(param_combinations) * len(scalers) * len(dim_reductions) * len(pca_thresholds)\n","\n","    for scaler in scalers:\n","      data = scale_features(data_full.clone(), scaler)\n","      for dim_reduction in dim_reductions:\n","        for pca_threshold in pca_thresholds:\n","          data = dimentional_reduction(data, dim_reduction, pca_threshold)\n","\n","          for params in param_combinations:\n","            combination_counter += 1\n","\n","            set_seed(SEED)\n","            params['scaler'] = scaler\n","            params['dim_reduction'] = dim_reduction # Fixed the typo here: 'dim_reduction' instead of 'dim_reducition'\n","\n","            if params['lr_scheduler'] != 'ReduceLROnPlateau':\n","              params['p'] = None\n","              params['factor'] = None\n","            elif params['lr_scheduler'] != 'CosineAnnealingLR':\n","              params['T_max'] = None\n","              params['eta_min'] = None\n","\n","            if params['conv_type'] != 'Transformer':\n","              params['num_head'] = None\n","\n","            if dim_reduction == 'no':\n","              params['pca_threshold'] = None\n","            else:\n","              params['pca_threshold'] = pca_threshold\n","\n","            if True: #not is_combination_tested(path_comb, params):\n","              print(f\"  Combinazione {combination_counter}/{total_combinations}\")  # Print the counter\n","              model = None\n","              if params[ 'type_model'] == 'HeteroGNN':\n","                model = HeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              elif params[ 'type_model'] == 'ResidualHeteroGNN':\n","                model = ResidualHeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              model = train(model, data, params)\n","            else:\n","              print(f\"  Configurazione {combination_counter}/{total_combinations} già testata, salto...\")\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOYfyiGS4aMO"},"outputs":[],"source":["set_seed(SEED)\n","data = data_to_pyg(*load_data())"]},{"cell_type":"markdown","source":["# Risultati"],"metadata":{"id":"AaQjXm5L44JY"}},{"cell_type":"code","source":["hyperparams = {\n","    \"hidden_channels\": [64],\n","    'num_head': ['/'],\n","    \"num_layers\": [2],\n","    \"num_epoch\": [300],\n","    \"patience\": [50],\n","    \"lr\": [0.001],\n","    \"weight_decay\": [0],\n","    \"dropout\": [0],\n","    \"conv_type\": ['GAT'],\n","    \"p\": ['/'],\n","    \"factor\": ['/'],\n","    \"eta_min\": [1e-5],\n","    \"T_max\": [15], #10 15\n","    \"aggr\": ['sum'], #,\n","    'lr_scheduler':['CosineAnnealingLR'],\n","    'optimizer': ['Adam'], #Adam\n","    'type_model':['HeteroGNN'],\n","}\n","\n","scaler = ['standard']\n","dim_reduction=['no']\n","pca_threshold=[0.99]\n","best_model = train_grid(data, hyperparams, scaler, dim_reduction, pca_threshold)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMeCUciZNqIt","outputId":"7a2f2a5e-dac7-41c1-b1d3-d2d03c0ed3b2","executionInfo":{"status":"ok","timestamp":1742725328747,"user_tz":-60,"elapsed":1043842,"user":{"displayName":"Claudia Brunetti","userId":"17627296144817418720"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Combinazione 1/1\n","Combinazione: {'hidden_channels': 64, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 0, 'dropout': 0, 'conv_type': 'GAT', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 15, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard', 'dim_reduction': 'no', 'pca_threshold': None}\n","Epoch 001:      TX: Train Loss: 0.7398, Acc: 0.7725, F1: 0.8695 Bal: 0.4941 - Val Loss: 0.7362, Accuracy: 0.7720, F1: 0.8689 Bal: 0.5023\n","           WALLETS: Train Loss: 0.75240606, Acc: 0.64695360, F1: 0.77778818 Bal: 0.5226 - Val Loss: 0.74791676, Accuracy: 0.6529, F1: 0.7822 Bal: 0.5258\n","Epoch 002:      TX: Train Loss: 0.7176, Acc: 0.8066, F1: 0.8916 Bal: 0.4977 - Val Loss: 0.7255, Accuracy: 0.8065, F1: 0.8915 Bal: 0.4998\n","           WALLETS: Train Loss: 0.71643406, Acc: 0.68893775, F1: 0.81022522 Bal: 0.5207 - Val Loss: 0.71420318, Accuracy: 0.6920, F1: 0.8123 Bal: 0.5223\n","Epoch 003:      TX: Train Loss: 0.7075, Acc: 0.8193, F1: 0.8996 Bal: 0.5008 - Val Loss: 0.7201, Accuracy: 0.8186, F1: 0.8990 Bal: 0.5026\n","           WALLETS: Train Loss: 0.70229244, Acc: 0.76651282, F1: 0.86531740 Bal: 0.5127 - Val Loss: 0.70193207, Accuracy: 0.7677, F1: 0.8660 Bal: 0.5146\n","Epoch 004:      TX: Train Loss: 0.7013, Acc: 0.8011, F1: 0.8880 Bal: 0.5017 - Val Loss: 0.7150, Accuracy: 0.7988, F1: 0.8867 Bal: 0.4955\n","           WALLETS: Train Loss: 0.69846565, Acc: 0.84138837, F1: 0.91305022 Bal: 0.5069 - Val Loss: 0.69928008, Accuracy: 0.8401, F1: 0.9123 Bal: 0.5061\n","Epoch 005:      TX: Train Loss: 0.6972, Acc: 0.7532, F1: 0.8560 Bal: 0.5068 - Val Loss: 0.7099, Accuracy: 0.7536, F1: 0.8562 Bal: 0.5077\n","           WALLETS: Train Loss: 0.69853550, Acc: 0.87179794, F1: 0.93111630 Bal: 0.5040 - Val Loss: 0.70004690, Accuracy: 0.8713, F1: 0.9308 Bal: 0.5042\n","Epoch 006:      TX: Train Loss: 0.6956, Acc: 0.7114, F1: 0.8263 Bal: 0.5062 - Val Loss: 0.7071, Accuracy: 0.7056, F1: 0.8224 Bal: 0.4987\n","           WALLETS: Train Loss: 0.69902998, Acc: 0.89148198, F1: 0.94242726 Bal: 0.5027 - Val Loss: 0.70074886, Accuracy: 0.8911, F1: 0.9422 Bal: 0.5022\n","Epoch 007:      TX: Train Loss: 0.6946, Acc: 0.6842, F1: 0.8059 Bal: 0.5068 - Val Loss: 0.7058, Accuracy: 0.6790, F1: 0.8025 Bal: 0.4978\n","           WALLETS: Train Loss: 0.69837826, Acc: 0.89979100, F1: 0.94712032 Bal: 0.5017 - Val Loss: 0.70019490, Accuracy: 0.8990, F1: 0.9467 Bal: 0.5009\n","Epoch 008:      TX: Train Loss: 0.6932, Acc: 0.6780, F1: 0.8008 Bal: 0.5135 - Val Loss: 0.7055, Accuracy: 0.6707, F1: 0.7961 Bal: 0.4961\n","           WALLETS: Train Loss: 0.69657183, Acc: 0.90386206, F1: 0.94939687 Bal: 0.5018 - Val Loss: 0.69838536, Accuracy: 0.9032, F1: 0.9490 Bal: 0.5004\n","Epoch 009:      TX: Train Loss: 0.6914, Acc: 0.6889, F1: 0.8090 Bal: 0.5158 - Val Loss: 0.7065, Accuracy: 0.6801, F1: 0.8035 Bal: 0.4954\n","           WALLETS: Train Loss: 0.69414908, Acc: 0.90579237, F1: 0.95047250 Bal: 0.5018 - Val Loss: 0.69589710, Accuracy: 0.9051, F1: 0.9501 Bal: 0.4997\n","Epoch 010:      TX: Train Loss: 0.6899, Acc: 0.7063, F1: 0.8220 Bal: 0.5179 - Val Loss: 0.7085, Accuracy: 0.6972, F1: 0.8163 Bal: 0.4971\n","           WALLETS: Train Loss: 0.69172305, Acc: 0.90420313, F1: 0.94958468 Bal: 0.5020 - Val Loss: 0.69335765, Accuracy: 0.9032, F1: 0.9491 Bal: 0.4998\n","Epoch 011:      TX: Train Loss: 0.6888, Acc: 0.7267, F1: 0.8368 Bal: 0.5186 - Val Loss: 0.7109, Accuracy: 0.7154, F1: 0.8298 Bal: 0.4954\n","           WALLETS: Train Loss: 0.68986249, Acc: 0.89961321, F1: 0.94701037 Bal: 0.5028 - Val Loss: 0.69137740, Accuracy: 0.8993, F1: 0.9469 Bal: 0.5013\n","Epoch 012:      TX: Train Loss: 0.6877, Acc: 0.7409, F1: 0.8467 Bal: 0.5236 - Val Loss: 0.7122, Accuracy: 0.7269, F1: 0.8379 Bal: 0.4978\n","           WALLETS: Train Loss: 0.68880701, Acc: 0.89109011, F1: 0.94219088 Bal: 0.5041 - Val Loss: 0.69020802, Accuracy: 0.8911, F1: 0.9422 Bal: 0.5035\n","Epoch 013:      TX: Train Loss: 0.6864, Acc: 0.7482, F1: 0.8518 Bal: 0.5261 - Val Loss: 0.7121, Accuracy: 0.7334, F1: 0.8427 Bal: 0.4946\n","           WALLETS: Train Loss: 0.68839222, Acc: 0.87667450, F1: 0.93393845 Bal: 0.5043 - Val Loss: 0.68972260, Accuracy: 0.8765, F1: 0.9339 Bal: 0.5034\n","Epoch 014:      TX: Train Loss: 0.6851, Acc: 0.7491, F1: 0.8523 Bal: 0.5291 - Val Loss: 0.7110, Accuracy: 0.7321, F1: 0.8419 Bal: 0.4909\n","           WALLETS: Train Loss: 0.68828756, Acc: 0.86925444, F1: 0.92960927 Bal: 0.5061 - Val Loss: 0.68959415, Accuracy: 0.8691, F1: 0.9295 Bal: 0.5043\n","Epoch 015:      TX: Train Loss: 0.6841, Acc: 0.7319, F1: 0.8400 Bal: 0.5311 - Val Loss: 0.7096, Accuracy: 0.7227, F1: 0.8349 Bal: 0.4965\n","           WALLETS: Train Loss: 0.68819630, Acc: 0.85442519, F1: 0.92084511 Bal: 0.5083 - Val Loss: 0.68951696, Accuracy: 0.8536, F1: 0.9204 Bal: 0.5039\n","Epoch 016:      TX: Train Loss: 0.6832, Acc: 0.7277, F1: 0.8370 Bal: 0.5316 - Val Loss: 0.7080, Accuracy: 0.7185, F1: 0.8317 Bal: 0.5010\n","           WALLETS: Train Loss: 0.68802220, Acc: 0.84869958, F1: 0.91742069 Bal: 0.5088 - Val Loss: 0.68937767, Accuracy: 0.8484, F1: 0.9173 Bal: 0.5058\n","Epoch 017:      TX: Train Loss: 0.6823, Acc: 0.5034, F1: 0.6431 Bal: 0.5280 - Val Loss: 0.7064, Accuracy: 0.4998, F1: 0.6434 Bal: 0.4945\n","           WALLETS: Train Loss: 0.68762743, Acc: 0.84881206, F1: 0.91749303 Bal: 0.5085 - Val Loss: 0.68904537, Accuracy: 0.8493, F1: 0.9178 Bal: 0.5067\n","Epoch 018:      TX: Train Loss: 0.6813, Acc: 0.3800, F1: 0.4964 Bal: 0.5423 - Val Loss: 0.7055, Accuracy: 0.3780, F1: 0.4996 Bal: 0.5123\n","           WALLETS: Train Loss: 0.68705100, Acc: 0.85141362, F1: 0.91904597 Bal: 0.5087 - Val Loss: 0.68854499, Accuracy: 0.8517, F1: 0.9193 Bal: 0.5058\n","Epoch 019:      TX: Train Loss: 0.6803, Acc: 0.3854, F1: 0.5025 Bal: 0.5474 - Val Loss: 0.7058, Accuracy: 0.3802, F1: 0.5013 Bal: 0.5184\n","           WALLETS: Train Loss: 0.68638647, Acc: 0.85326773, F1: 0.92016204 Bal: 0.5080 - Val Loss: 0.68798828, Accuracy: 0.8533, F1: 0.9202 Bal: 0.5056\n","Epoch 020:      TX: Train Loss: 0.6795, Acc: 0.3845, F1: 0.5007 Bal: 0.5514 - Val Loss: 0.7072, Accuracy: 0.3796, F1: 0.4996 Bal: 0.5230\n","           WALLETS: Train Loss: 0.68575513, Acc: 0.85747304, F1: 0.92266287 Bal: 0.5077 - Val Loss: 0.68749410, Accuracy: 0.8567, F1: 0.9223 Bal: 0.5035\n","Epoch 021:      TX: Train Loss: 0.6788, Acc: 0.3888, F1: 0.5060 Bal: 0.5533 - Val Loss: 0.7092, Accuracy: 0.3831, F1: 0.5041 Bal: 0.5239\n","           WALLETS: Train Loss: 0.68525183, Acc: 0.87633706, F1: 0.93372716 Bal: 0.5056 - Val Loss: 0.68715590, Accuracy: 0.8756, F1: 0.9333 Bal: 0.5031\n","Epoch 022:      TX: Train Loss: 0.6781, Acc: 0.3871, F1: 0.5031 Bal: 0.5567 - Val Loss: 0.7111, Accuracy: 0.3787, F1: 0.4984 Bal: 0.5235\n","           WALLETS: Train Loss: 0.68491715, Acc: 0.88377890, F1: 0.93801463 Bal: 0.5050 - Val Loss: 0.68700421, Accuracy: 0.8830, F1: 0.9376 Bal: 0.5014\n","Epoch 023:      TX: Train Loss: 0.6773, Acc: 0.3828, F1: 0.4973 Bal: 0.5575 - Val Loss: 0.7124, Accuracy: 0.3730, F1: 0.4913 Bal: 0.5213\n","           WALLETS: Train Loss: 0.68474001, Acc: 0.89044789, F1: 0.94181715 Bal: 0.5049 - Val Loss: 0.68701458, Accuracy: 0.8895, F1: 0.9413 Bal: 0.5020\n","Epoch 024:      TX: Train Loss: 0.6765, Acc: 0.3776, F1: 0.4900 Bal: 0.5590 - Val Loss: 0.7130, Accuracy: 0.3664, F1: 0.4828 Bal: 0.5196\n","           WALLETS: Train Loss: 0.68465507, Acc: 0.89719307, F1: 0.94563282 Bal: 0.5047 - Val Loss: 0.68709624, Accuracy: 0.8954, F1: 0.9447 Bal: 0.5002\n","Epoch 025:      TX: Train Loss: 0.6757, Acc: 0.3716, F1: 0.4820 Bal: 0.5589 - Val Loss: 0.7130, Accuracy: 0.3611, F1: 0.4759 Bal: 0.5186\n","           WALLETS: Train Loss: 0.68458706, Acc: 0.90105731, F1: 0.94780863 Bal: 0.5040 - Val Loss: 0.68717229, Accuracy: 0.8994, F1: 0.9469 Bal: 0.5007\n","Epoch 026:      TX: Train Loss: 0.6749, Acc: 0.3682, F1: 0.4769 Bal: 0.5611 - Val Loss: 0.7132, Accuracy: 0.3552, F1: 0.4679 Bal: 0.5183\n","           WALLETS: Train Loss: 0.68447816, Acc: 0.90337586, F1: 0.94910830 Bal: 0.5037 - Val Loss: 0.68719631, Accuracy: 0.9014, F1: 0.9480 Bal: 0.4996\n","Epoch 027:      TX: Train Loss: 0.6741, Acc: 0.3684, F1: 0.4771 Bal: 0.5618 - Val Loss: 0.7138, Accuracy: 0.3570, F1: 0.4707 Bal: 0.5163\n","           WALLETS: Train Loss: 0.68429828, Acc: 0.90425756, F1: 0.94960360 Bal: 0.5033 - Val Loss: 0.68713361, Accuracy: 0.9025, F1: 0.9487 Bal: 0.5004\n","Epoch 028:      TX: Train Loss: 0.6733, Acc: 0.3718, F1: 0.4814 Bal: 0.5630 - Val Loss: 0.7148, Accuracy: 0.3624, F1: 0.4773 Bal: 0.5204\n","           WALLETS: Train Loss: 0.68405664, Acc: 0.90454057, F1: 0.94976063 Bal: 0.5034 - Val Loss: 0.68699908, Accuracy: 0.9029, F1: 0.9489 Bal: 0.5000\n","Epoch 029:      TX: Train Loss: 0.6725, Acc: 0.3736, F1: 0.4838 Bal: 0.5637 - Val Loss: 0.7164, Accuracy: 0.3613, F1: 0.4771 Bal: 0.5139\n","           WALLETS: Train Loss: 0.68377906, Acc: 0.90475828, F1: 0.94988057 Bal: 0.5036 - Val Loss: 0.68682092, Accuracy: 0.9031, F1: 0.9490 Bal: 0.5005\n","Epoch 030:      TX: Train Loss: 0.6717, Acc: 0.3763, F1: 0.4874 Bal: 0.5639 - Val Loss: 0.7187, Accuracy: 0.3644, F1: 0.4813 Bal: 0.5136\n","           WALLETS: Train Loss: 0.68350285, Acc: 0.90348471, F1: 0.94916777 Bal: 0.5038 - Val Loss: 0.68662530, Accuracy: 0.9020, F1: 0.9484 Bal: 0.5009\n","Epoch 031:      TX: Train Loss: 0.6710, Acc: 0.3790, F1: 0.4909 Bal: 0.5642 - Val Loss: 0.7214, Accuracy: 0.3673, F1: 0.4852 Bal: 0.5132\n","           WALLETS: Train Loss: 0.68326080, Acc: 0.90173582, F1: 0.94818528 Bal: 0.5044 - Val Loss: 0.68643743, Accuracy: 0.9000, F1: 0.9473 Bal: 0.5009\n","Epoch 032:      TX: Train Loss: 0.6702, Acc: 0.3799, F1: 0.4920 Bal: 0.5651 - Val Loss: 0.7243, Accuracy: 0.3684, F1: 0.4865 Bal: 0.5138\n","           WALLETS: Train Loss: 0.68307257, Acc: 0.89987809, F1: 0.94713874 Bal: 0.5050 - Val Loss: 0.68627179, Accuracy: 0.8980, F1: 0.9461 Bal: 0.5009\n","Epoch 033:      TX: Train Loss: 0.6694, Acc: 0.3793, F1: 0.4910 Bal: 0.5659 - Val Loss: 0.7291, Accuracy: 0.3681, F1: 0.4868 Bal: 0.5108\n","           WALLETS: Train Loss: 0.68293744, Acc: 0.89706245, F1: 0.94554912 Bal: 0.5057 - Val Loss: 0.68614185, Accuracy: 0.8950, F1: 0.9444 Bal: 0.5004\n","Epoch 034:      TX: Train Loss: 0.6686, Acc: 0.3804, F1: 0.4920 Bal: 0.5688 - Val Loss: 0.7332, Accuracy: 0.3695, F1: 0.4882 Bal: 0.5125\n","           WALLETS: Train Loss: 0.68283987, Acc: 0.89374610, F1: 0.94367552 Bal: 0.5059 - Val Loss: 0.68605256, Accuracy: 0.8916, F1: 0.9425 Bal: 0.5017\n","Epoch 035:      TX: Train Loss: 0.6677, Acc: 0.3802, F1: 0.4914 Bal: 0.5702 - Val Loss: 0.7328, Accuracy: 0.3677, F1: 0.4861 Bal: 0.5115\n","           WALLETS: Train Loss: 0.68275762, Acc: 0.89150738, F1: 0.94240382 Bal: 0.5063 - Val Loss: 0.68596417, Accuracy: 0.8897, F1: 0.9414 Bal: 0.5015\n","Epoch 036:      TX: Train Loss: 0.6669, Acc: 0.3810, F1: 0.4924 Bal: 0.5712 - Val Loss: 0.7305, Accuracy: 0.3684, F1: 0.4867 Bal: 0.5128\n","           WALLETS: Train Loss: 0.68266934, Acc: 0.89041523, F1: 0.94178085 Bal: 0.5066 - Val Loss: 0.68589628, Accuracy: 0.8885, F1: 0.9408 Bal: 0.5016\n","Epoch 037:      TX: Train Loss: 0.6661, Acc: 0.3834, F1: 0.4953 Bal: 0.5724 - Val Loss: 0.7292, Accuracy: 0.3684, F1: 0.4869 Bal: 0.5119\n","           WALLETS: Train Loss: 0.68256181, Acc: 0.88981655, F1: 0.94143829 Bal: 0.5069 - Val Loss: 0.68583399, Accuracy: 0.8876, F1: 0.9402 Bal: 0.5008\n","Epoch 038:      TX: Train Loss: 0.6653, Acc: 0.3859, F1: 0.4985 Bal: 0.5734 - Val Loss: 0.7295, Accuracy: 0.3706, F1: 0.4896 Bal: 0.5131\n","           WALLETS: Train Loss: 0.68243742, Acc: 0.88972947, F1: 0.94138703 Bal: 0.5070 - Val Loss: 0.68577486, Accuracy: 0.8875, F1: 0.9402 Bal: 0.5009\n","Epoch 039:      TX: Train Loss: 0.6645, Acc: 0.3878, F1: 0.5008 Bal: 0.5747 - Val Loss: 0.7305, Accuracy: 0.3723, F1: 0.4923 Bal: 0.5111\n","           WALLETS: Train Loss: 0.68230397, Acc: 0.89019753, F1: 0.94165034 Bal: 0.5073 - Val Loss: 0.68573314, Accuracy: 0.8878, F1: 0.9404 Bal: 0.5011\n","Epoch 040:      TX: Train Loss: 0.6636, Acc: 0.3924, F1: 0.5067 Bal: 0.5756 - Val Loss: 0.7322, Accuracy: 0.3774, F1: 0.4986 Bal: 0.5129\n","           WALLETS: Train Loss: 0.68216807, Acc: 0.89220040, F1: 0.94278330 Bal: 0.5076 - Val Loss: 0.68571389, Accuracy: 0.8899, F1: 0.9415 Bal: 0.5016\n","Epoch 041:      TX: Train Loss: 0.6628, Acc: 0.3945, F1: 0.5093 Bal: 0.5765 - Val Loss: 0.7352, Accuracy: 0.3809, F1: 0.5028 Bal: 0.5149\n","           WALLETS: Train Loss: 0.68204021, Acc: 0.89348486, F1: 0.94351270 Bal: 0.5074 - Val Loss: 0.68571883, Accuracy: 0.8910, F1: 0.9422 Bal: 0.5012\n","Epoch 042:      TX: Train Loss: 0.6620, Acc: 0.3947, F1: 0.5092 Bal: 0.5777 - Val Loss: 0.7386, Accuracy: 0.3780, F1: 0.4994 Bal: 0.5133\n","           WALLETS: Train Loss: 0.68193001, Acc: 0.89492170, F1: 0.94432675 Bal: 0.5072 - Val Loss: 0.68574464, Accuracy: 0.8923, F1: 0.9429 Bal: 0.5008\n","Epoch 043:      TX: Train Loss: 0.6611, Acc: 0.3955, F1: 0.5101 Bal: 0.5789 - Val Loss: 0.7419, Accuracy: 0.3774, F1: 0.4984 Bal: 0.5139\n","           WALLETS: Train Loss: 0.68184137, Acc: 0.89598482, F1: 0.94492760 Bal: 0.5072 - Val Loss: 0.68578535, Accuracy: 0.8931, F1: 0.9434 Bal: 0.5000\n","Epoch 044:      TX: Train Loss: 0.6602, Acc: 0.3934, F1: 0.5072 Bal: 0.5800 - Val Loss: 0.7445, Accuracy: 0.3752, F1: 0.4954 Bal: 0.5147\n","           WALLETS: Train Loss: 0.68176973, Acc: 0.89701891, F1: 0.94551123 Bal: 0.5071 - Val Loss: 0.68583620, Accuracy: 0.8940, F1: 0.9439 Bal: 0.5003\n","Epoch 045:      TX: Train Loss: 0.6593, Acc: 0.3919, F1: 0.5051 Bal: 0.5803 - Val Loss: 0.7466, Accuracy: 0.3717, F1: 0.4913 Bal: 0.5117\n","           WALLETS: Train Loss: 0.68170762, Acc: 0.89782079, F1: 0.94596031 Bal: 0.5073 - Val Loss: 0.68587881, Accuracy: 0.8943, F1: 0.9440 Bal: 0.5000\n","Epoch 046:      TX: Train Loss: 0.6584, Acc: 0.3913, F1: 0.5043 Bal: 0.5811 - Val Loss: 0.7484, Accuracy: 0.3717, F1: 0.4909 Bal: 0.5137\n","           WALLETS: Train Loss: 0.68164515, Acc: 0.89793327, F1: 0.94602301 Bal: 0.5074 - Val Loss: 0.68590641, Accuracy: 0.8944, F1: 0.9441 Bal: 0.5001\n","Epoch 047:      TX: Train Loss: 0.6575, Acc: 0.3922, F1: 0.5055 Bal: 0.5811 - Val Loss: 0.7500, Accuracy: 0.3728, F1: 0.4923 Bal: 0.5143\n","           WALLETS: Train Loss: 0.68157876, Acc: 0.89797318, F1: 0.94604422 Bal: 0.5075 - Val Loss: 0.68591470, Accuracy: 0.8943, F1: 0.9441 Bal: 0.5002\n","Epoch 048:      TX: Train Loss: 0.6566, Acc: 0.3974, F1: 0.5121 Bal: 0.5821 - Val Loss: 0.7517, Accuracy: 0.3789, F1: 0.4999 Bal: 0.5167\n","           WALLETS: Train Loss: 0.68150812, Acc: 0.89760671, F1: 0.94583743 Bal: 0.5076 - Val Loss: 0.68590492, Accuracy: 0.8941, F1: 0.9439 Bal: 0.5004\n","Epoch 049:      TX: Train Loss: 0.6557, Acc: 0.3998, F1: 0.5150 Bal: 0.5833 - Val Loss: 0.7532, Accuracy: 0.3806, F1: 0.5020 Bal: 0.5177\n","           WALLETS: Train Loss: 0.68143564, Acc: 0.89685563, F1: 0.94541426 Bal: 0.5076 - Val Loss: 0.68588161, Accuracy: 0.8933, F1: 0.9435 Bal: 0.5003\n","Epoch 050:      TX: Train Loss: 0.6548, Acc: 0.3982, F1: 0.5127 Bal: 0.5845 - Val Loss: 0.7547, Accuracy: 0.3793, F1: 0.5001 Bal: 0.5189\n","           WALLETS: Train Loss: 0.68136293, Acc: 0.89466408, F1: 0.94417556 Bal: 0.5078 - Val Loss: 0.68585443, Accuracy: 0.8914, F1: 0.9424 Bal: 0.5003\n","Epoch 051:      TX: Train Loss: 0.6539, Acc: 0.6237, F1: 0.7514 Bal: 0.5915 - Val Loss: 0.7562, Accuracy: 0.5983, F1: 0.7343 Bal: 0.5256\n","           WALLETS: Train Loss: 0.68129230, Acc: 0.89378601, F1: 0.94367642 Bal: 0.5081 - Val Loss: 0.68582684, Accuracy: 0.8907, F1: 0.9420 Bal: 0.5003\n","Epoch 052:      TX: Train Loss: 0.6529, Acc: 0.6227, F1: 0.7505 Bal: 0.5912 - Val Loss: 0.7576, Accuracy: 0.5989, F1: 0.7348 Bal: 0.5269\n","           WALLETS: Train Loss: 0.68122566, Acc: 0.89284263, F1: 0.94313983 Bal: 0.5083 - Val Loss: 0.68579686, Accuracy: 0.8897, F1: 0.9414 Bal: 0.4999\n","Epoch 053:      TX: Train Loss: 0.6519, Acc: 0.6240, F1: 0.7516 Bal: 0.5923 - Val Loss: 0.7592, Accuracy: 0.5992, F1: 0.7349 Bal: 0.5271\n","           WALLETS: Train Loss: 0.68116415, Acc: 0.89199358, F1: 0.94265575 Bal: 0.5086 - Val Loss: 0.68577307, Accuracy: 0.8891, F1: 0.9411 Bal: 0.5002\n","Epoch 054:      TX: Train Loss: 0.6510, Acc: 0.6265, F1: 0.7537 Bal: 0.5929 - Val Loss: 0.7610, Accuracy: 0.6020, F1: 0.7373 Bal: 0.5287\n","           WALLETS: Train Loss: 0.68110687, Acc: 0.89151827, F1: 0.94238352 Bal: 0.5089 - Val Loss: 0.68575436, Accuracy: 0.8886, F1: 0.9408 Bal: 0.5005\n","Epoch 055:      TX: Train Loss: 0.6500, Acc: 0.6282, F1: 0.7550 Bal: 0.5940 - Val Loss: 0.7628, Accuracy: 0.6031, F1: 0.7383 Bal: 0.5283\n","           WALLETS: Train Loss: 0.68104881, Acc: 0.89146747, F1: 0.94235276 Bal: 0.5091 - Val Loss: 0.68574005, Accuracy: 0.8886, F1: 0.9408 Bal: 0.5007\n","Epoch 056:      TX: Train Loss: 0.6490, Acc: 0.6306, F1: 0.7571 Bal: 0.5949 - Val Loss: 0.7648, Accuracy: 0.6057, F1: 0.7406 Bal: 0.5288\n","           WALLETS: Train Loss: 0.68098783, Acc: 0.89161986, F1: 0.94244102 Bal: 0.5089 - Val Loss: 0.68573320, Accuracy: 0.8887, F1: 0.9409 Bal: 0.5009\n","Epoch 057:      TX: Train Loss: 0.6480, Acc: 0.6326, F1: 0.7587 Bal: 0.5963 - Val Loss: 0.7669, Accuracy: 0.6066, F1: 0.7413 Bal: 0.5292\n","           WALLETS: Train Loss: 0.68092883, Acc: 0.89183756, F1: 0.94256483 Bal: 0.5089 - Val Loss: 0.68574071, Accuracy: 0.8889, F1: 0.9410 Bal: 0.5010\n","Epoch 058:      TX: Train Loss: 0.6470, Acc: 0.6340, F1: 0.7598 Bal: 0.5980 - Val Loss: 0.7688, Accuracy: 0.6075, F1: 0.7421 Bal: 0.5288\n","           WALLETS: Train Loss: 0.68087441, Acc: 0.89224757, F1: 0.94279655 Bal: 0.5089 - Val Loss: 0.68576568, Accuracy: 0.8893, F1: 0.9412 Bal: 0.5012\n","Epoch 059:      TX: Train Loss: 0.6460, Acc: 0.6363, F1: 0.7617 Bal: 0.5990 - Val Loss: 0.7706, Accuracy: 0.6090, F1: 0.7434 Bal: 0.5296\n","           WALLETS: Train Loss: 0.68082565, Acc: 0.89260316, F1: 0.94299894 Bal: 0.5089 - Val Loss: 0.68580431, Accuracy: 0.8897, F1: 0.9414 Bal: 0.5014\n","Epoch 060:      TX: Train Loss: 0.6449, Acc: 0.6360, F1: 0.7613 Bal: 0.6007 - Val Loss: 0.7723, Accuracy: 0.6090, F1: 0.7433 Bal: 0.5306\n","           WALLETS: Train Loss: 0.68077815, Acc: 0.89319458, F1: 0.94333346 Bal: 0.5089 - Val Loss: 0.68584228, Accuracy: 0.8904, F1: 0.9418 Bal: 0.5015\n","Epoch 061:      TX: Train Loss: 0.6439, Acc: 0.6355, F1: 0.7608 Bal: 0.6010 - Val Loss: 0.7740, Accuracy: 0.6088, F1: 0.7431 Bal: 0.5305\n","           WALLETS: Train Loss: 0.68073177, Acc: 0.89366990, F1: 0.94360226 Bal: 0.5089 - Val Loss: 0.68588018, Accuracy: 0.8909, F1: 0.9421 Bal: 0.5016\n","Epoch 062:      TX: Train Loss: 0.6428, Acc: 0.6362, F1: 0.7614 Bal: 0.6018 - Val Loss: 0.7759, Accuracy: 0.6093, F1: 0.7434 Bal: 0.5317\n","           WALLETS: Train Loss: 0.68068624, Acc: 0.89398920, F1: 0.94378330 Bal: 0.5089 - Val Loss: 0.68591911, Accuracy: 0.8911, F1: 0.9422 Bal: 0.5016\n","Epoch 063:      TX: Train Loss: 0.6417, Acc: 0.6377, F1: 0.7626 Bal: 0.6033 - Val Loss: 0.7779, Accuracy: 0.6108, F1: 0.7450 Bal: 0.5286\n","           WALLETS: Train Loss: 0.68064171, Acc: 0.89363725, F1: 0.94358483 Bal: 0.5088 - Val Loss: 0.68595868, Accuracy: 0.8909, F1: 0.9421 Bal: 0.5016\n","Epoch 064:      TX: Train Loss: 0.6407, Acc: 0.6396, F1: 0.7640 Bal: 0.6043 - Val Loss: 0.7801, Accuracy: 0.6119, F1: 0.7460 Bal: 0.5273\n","           WALLETS: Train Loss: 0.68059874, Acc: 0.89352477, F1: 0.94352245 Bal: 0.5087 - Val Loss: 0.68599206, Accuracy: 0.8907, F1: 0.9420 Bal: 0.5014\n","Epoch 065:      TX: Train Loss: 0.6396, Acc: 0.6407, F1: 0.7649 Bal: 0.6055 - Val Loss: 0.7822, Accuracy: 0.6110, F1: 0.7454 Bal: 0.5258\n","           WALLETS: Train Loss: 0.68055606, Acc: 0.89321635, F1: 0.94334719 Bal: 0.5088 - Val Loss: 0.68601632, Accuracy: 0.8904, F1: 0.9418 Bal: 0.5014\n","Epoch 066:      TX: Train Loss: 0.6385, Acc: 0.6422, F1: 0.7661 Bal: 0.6070 - Val Loss: 0.7842, Accuracy: 0.6119, F1: 0.7462 Bal: 0.5243\n","           WALLETS: Train Loss: 0.68051189, Acc: 0.89279546, F1: 0.94310703 Bal: 0.5089 - Val Loss: 0.68603319, Accuracy: 0.8901, F1: 0.9417 Bal: 0.5017\n","Epoch 067:      TX: Train Loss: 0.6374, Acc: 0.6433, F1: 0.7669 Bal: 0.6085 - Val Loss: 0.7861, Accuracy: 0.6119, F1: 0.7462 Bal: 0.5243\n","           WALLETS: Train Loss: 0.68046874, Acc: 0.89235642, F1: 0.94285676 Bal: 0.5091 - Val Loss: 0.68604505, Accuracy: 0.8897, F1: 0.9414 Bal: 0.5016\n","Epoch 068:      TX: Train Loss: 0.6363, Acc: 0.6440, F1: 0.7675 Bal: 0.6090 - Val Loss: 0.7881, Accuracy: 0.6134, F1: 0.7476 Bal: 0.5242\n","           WALLETS: Train Loss: 0.68042898, Acc: 0.89198633, F1: 0.94264527 Bal: 0.5093 - Val Loss: 0.68605542, Accuracy: 0.8892, F1: 0.9411 Bal: 0.5015\n","Epoch 069:      TX: Train Loss: 0.6351, Acc: 0.6450, F1: 0.7682 Bal: 0.6101 - Val Loss: 0.7900, Accuracy: 0.6134, F1: 0.7476 Bal: 0.5232\n","           WALLETS: Train Loss: 0.68039256, Acc: 0.89141667, F1: 0.94232044 Bal: 0.5094 - Val Loss: 0.68606657, Accuracy: 0.8885, F1: 0.9408 Bal: 0.5018\n","Epoch 070:      TX: Train Loss: 0.6340, Acc: 0.6459, F1: 0.7690 Bal: 0.6109 - Val Loss: 0.7922, Accuracy: 0.6141, F1: 0.7481 Bal: 0.5246\n","           WALLETS: Train Loss: 0.68035704, Acc: 0.89111914, F1: 0.94214924 Bal: 0.5096 - Val Loss: 0.68607980, Accuracy: 0.8883, F1: 0.9406 Bal: 0.5024\n","Epoch 071:      TX: Train Loss: 0.6328, Acc: 0.6486, F1: 0.7711 Bal: 0.6123 - Val Loss: 0.7945, Accuracy: 0.6154, F1: 0.7493 Bal: 0.5233\n","           WALLETS: Train Loss: 0.68032032, Acc: 0.89098126, F1: 0.94206705 Bal: 0.5100 - Val Loss: 0.68609232, Accuracy: 0.8884, F1: 0.9407 Bal: 0.5028\n","Epoch 072:      TX: Train Loss: 0.6316, Acc: 0.6494, F1: 0.7718 Bal: 0.6128 - Val Loss: 0.7967, Accuracy: 0.6169, F1: 0.7506 Bal: 0.5232\n","           WALLETS: Train Loss: 0.68028182, Acc: 0.89068011, F1: 0.94189413 Bal: 0.5102 - Val Loss: 0.68611068, Accuracy: 0.8882, F1: 0.9405 Bal: 0.5031\n","Epoch 073:      TX: Train Loss: 0.6305, Acc: 0.6493, F1: 0.7716 Bal: 0.6143 - Val Loss: 0.7987, Accuracy: 0.6163, F1: 0.7500 Bal: 0.5248\n","           WALLETS: Train Loss: 0.68024361, Acc: 0.89091958, F1: 0.94203103 Bal: 0.5101 - Val Loss: 0.68613517, Accuracy: 0.8884, F1: 0.9407 Bal: 0.5030\n","Epoch 074:      TX: Train Loss: 0.6293, Acc: 0.6498, F1: 0.7719 Bal: 0.6151 - Val Loss: 0.8007, Accuracy: 0.6156, F1: 0.7494 Bal: 0.5244\n","           WALLETS: Train Loss: 0.68020660, Acc: 0.89104657, F1: 0.94210309 Bal: 0.5101 - Val Loss: 0.68616796, Accuracy: 0.8886, F1: 0.9408 Bal: 0.5027\n","Epoch 075:      TX: Train Loss: 0.6281, Acc: 0.6507, F1: 0.7726 Bal: 0.6159 - Val Loss: 0.8028, Accuracy: 0.6163, F1: 0.7500 Bal: 0.5238\n","           WALLETS: Train Loss: 0.68016976, Acc: 0.89132596, F1: 0.94226280 Bal: 0.5100 - Val Loss: 0.68620116, Accuracy: 0.8888, F1: 0.9409 Bal: 0.5027\n","Epoch 076:      TX: Train Loss: 0.6269, Acc: 0.6518, F1: 0.7735 Bal: 0.6165 - Val Loss: 0.8051, Accuracy: 0.6176, F1: 0.7511 Bal: 0.5245\n","           WALLETS: Train Loss: 0.68013322, Acc: 0.89192465, F1: 0.94260355 Bal: 0.5099 - Val Loss: 0.68622535, Accuracy: 0.8893, F1: 0.9412 Bal: 0.5028\n","Epoch 077:      TX: Train Loss: 0.6257, Acc: 0.6527, F1: 0.7742 Bal: 0.6175 - Val Loss: 0.8074, Accuracy: 0.6183, F1: 0.7517 Bal: 0.5239\n","           WALLETS: Train Loss: 0.68009710, Acc: 0.89217863, F1: 0.94274727 Bal: 0.5099 - Val Loss: 0.68623960, Accuracy: 0.8897, F1: 0.9414 Bal: 0.5030\n","Epoch 078:      TX: Train Loss: 0.6244, Acc: 0.6537, F1: 0.7749 Bal: 0.6183 - Val Loss: 0.8096, Accuracy: 0.6178, F1: 0.7514 Bal: 0.5237\n","           WALLETS: Train Loss: 0.68006182, Acc: 0.89229111, F1: 0.94280953 Bal: 0.5101 - Val Loss: 0.68625987, Accuracy: 0.8898, F1: 0.9415 Bal: 0.5031\n","Epoch 079:      TX: Train Loss: 0.6232, Acc: 0.6540, F1: 0.7752 Bal: 0.6188 - Val Loss: 0.8117, Accuracy: 0.6183, F1: 0.7517 Bal: 0.5239\n","           WALLETS: Train Loss: 0.68002748, Acc: 0.89230926, F1: 0.94282015 Bal: 0.5101 - Val Loss: 0.68628424, Accuracy: 0.8899, F1: 0.9415 Bal: 0.5031\n","Epoch 080:      TX: Train Loss: 0.6220, Acc: 0.6559, F1: 0.7767 Bal: 0.6201 - Val Loss: 0.8143, Accuracy: 0.6196, F1: 0.7529 Bal: 0.5227\n","           WALLETS: Train Loss: 0.67999262, Acc: 0.89218589, F1: 0.94274892 Bal: 0.5102 - Val Loss: 0.68630016, Accuracy: 0.8896, F1: 0.9413 Bal: 0.5025\n","Epoch 081:      TX: Train Loss: 0.6207, Acc: 0.6569, F1: 0.7775 Bal: 0.6208 - Val Loss: 0.8171, Accuracy: 0.6220, F1: 0.7548 Bal: 0.5250\n","           WALLETS: Train Loss: 0.67995787, Acc: 0.89212784, F1: 0.94271345 Bal: 0.5104 - Val Loss: 0.68630832, Accuracy: 0.8895, F1: 0.9413 Bal: 0.5026\n","Epoch 082:      TX: Train Loss: 0.6194, Acc: 0.6575, F1: 0.7779 Bal: 0.6213 - Val Loss: 0.8195, Accuracy: 0.6226, F1: 0.7554 Bal: 0.5244\n","           WALLETS: Train Loss: 0.67992389, Acc: 0.89198633, F1: 0.94263112 Bal: 0.5106 - Val Loss: 0.68631625, Accuracy: 0.8891, F1: 0.9411 Bal: 0.5021\n","Epoch 083:      TX: Train Loss: 0.6181, Acc: 0.6580, F1: 0.7783 Bal: 0.6226 - Val Loss: 0.8217, Accuracy: 0.6229, F1: 0.7555 Bal: 0.5255\n","           WALLETS: Train Loss: 0.67989171, Acc: 0.89146384, F1: 0.94233316 Bal: 0.5108 - Val Loss: 0.68632221, Accuracy: 0.8886, F1: 0.9408 Bal: 0.5026\n","Epoch 084:      TX: Train Loss: 0.6168, Acc: 0.6611, F1: 0.7807 Bal: 0.6245 - Val Loss: 0.8243, Accuracy: 0.6262, F1: 0.7582 Bal: 0.5263\n","           WALLETS: Train Loss: 0.67985821, Acc: 0.89062568, F1: 0.94185073 Bal: 0.5114 - Val Loss: 0.68632287, Accuracy: 0.8875, F1: 0.9402 Bal: 0.5022\n","Epoch 085:      TX: Train Loss: 0.6155, Acc: 0.6626, F1: 0.7819 Bal: 0.6252 - Val Loss: 0.8269, Accuracy: 0.6283, F1: 0.7599 Bal: 0.5295\n","           WALLETS: Train Loss: 0.67982543, Acc: 0.89049506, F1: 0.94177724 Bal: 0.5113 - Val Loss: 0.68633133, Accuracy: 0.8873, F1: 0.9401 Bal: 0.5019\n","Epoch 086:      TX: Train Loss: 0.6142, Acc: 0.6636, F1: 0.7827 Bal: 0.6259 - Val Loss: 0.8294, Accuracy: 0.6283, F1: 0.7600 Bal: 0.5276\n","           WALLETS: Train Loss: 0.67979270, Acc: 0.88956619, F1: 0.94124574 Bal: 0.5116 - Val Loss: 0.68634081, Accuracy: 0.8865, F1: 0.9396 Bal: 0.5021\n","Epoch 087:      TX: Train Loss: 0.6129, Acc: 0.6642, F1: 0.7831 Bal: 0.6267 - Val Loss: 0.8319, Accuracy: 0.6297, F1: 0.7608 Bal: 0.5312\n","           WALLETS: Train Loss: 0.67975962, Acc: 0.88951539, F1: 0.94121644 Bal: 0.5117 - Val Loss: 0.68634993, Accuracy: 0.8864, F1: 0.9396 Bal: 0.5019\n","Epoch 088:      TX: Train Loss: 0.6115, Acc: 0.6653, F1: 0.7839 Bal: 0.6281 - Val Loss: 0.8344, Accuracy: 0.6288, F1: 0.7601 Bal: 0.5307\n","           WALLETS: Train Loss: 0.67972803, Acc: 0.88933760, F1: 0.94111423 Bal: 0.5118 - Val Loss: 0.68636376, Accuracy: 0.8863, F1: 0.9395 Bal: 0.5020\n","Epoch 089:      TX: Train Loss: 0.6102, Acc: 0.6667, F1: 0.7850 Bal: 0.6289 - Val Loss: 0.8374, Accuracy: 0.6301, F1: 0.7612 Bal: 0.5315\n","           WALLETS: Train Loss: 0.67969501, Acc: 0.88892759, F1: 0.94087837 Bal: 0.5120 - Val Loss: 0.68637204, Accuracy: 0.8857, F1: 0.9392 Bal: 0.5018\n","Epoch 090:      TX: Train Loss: 0.6088, Acc: 0.6676, F1: 0.7857 Bal: 0.6295 - Val Loss: 0.8403, Accuracy: 0.6321, F1: 0.7627 Bal: 0.5336\n","           WALLETS: Train Loss: 0.67966157, Acc: 0.88823094, F1: 0.94048272 Bal: 0.5119 - Val Loss: 0.68638372, Accuracy: 0.8851, F1: 0.9388 Bal: 0.5015\n","Early stopping at epoch 89\n","{'hidden_channels': 64, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 0, 'dropout': 0, 'conv_type': 'GAT', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 15, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard', 'dim_reduction': 'no', 'pca_threshold': None, 'epoch': 90, 'end': True}\n","Final_result for w\n","{'hidden_channels': 64, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 0, 'dropout': 0, 'conv_type': 'GAT', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 15, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard', 'dim_reduction': 'no', 'pca_threshold': None, 'epoch': 90}\n","Epoch 1:\n","  TX:\n","   Train: Loss=0.7398, Acc=0.7725, F1=0.8695, Bal. Acc=0.4941\n","   Val:   Loss=0.7362, Acc=0.7720, F1=0.8689, Bal. Acc=0.5023\n","   Test:  Loss=0.7232, Acc=0.7704, F1=0.8683, Bal. Acc=0.4891\n","              precision    recall  f1-score   support\n","\n","           0       0.10      0.17      0.13       453\n","           1       0.90      0.84      0.87      4105\n","\n","    accuracy                           0.77      4558\n","   macro avg       0.50      0.50      0.50      4558\n","weighted avg       0.82      0.77      0.80      4558\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.14      0.11       665\n","           1       0.90      0.84      0.87      6172\n","\n","    accuracy                           0.77      6837\n","   macro avg       0.49      0.49      0.49      6837\n","weighted avg       0.82      0.77      0.79      6837\n","\n","  WALLETS:\n","   Train: Loss=0.75240606, Acc=0.64695360, F1=0.77778818, Bal. Acc=0.5226\n","   Val:   Loss=0.74791676, Acc=0.6529, F1=0.7822, Bal. Acc=0.5258\n","   Test:  Loss=0.75265497, Acc=0.6447, F1=0.7761, Bal. Acc=0.5182\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.37      0.15      2906\n","           1       0.93      0.68      0.78     33841\n","\n","    accuracy                           0.65     36747\n","   macro avg       0.51      0.53      0.46     36747\n","weighted avg       0.86      0.65      0.73     36747\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.37      0.14      4340\n","           1       0.93      0.67      0.78     50781\n","\n","    accuracy                           0.64     55121\n","   macro avg       0.51      0.52      0.46     55121\n","weighted avg       0.86      0.64      0.73     55121\n","\n","\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIoomnaeh74t1AKH1zYUzUr42JRiCeq-","timestamp":1739893838211}],"collapsed_sections":["jFTKGWdI4ZXT"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}