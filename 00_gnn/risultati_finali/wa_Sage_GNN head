{"cells":[{"cell_type":"markdown","source":["# Addestramento\n","Addestramento del migliore modello SAGE per calcolare le performance sul test set per la classificazione dei wallet"],"metadata":{"id":"_QWrR51eyGrX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iidXjXM7pXO"},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","!pip install torch_geometric\n","#!pip install torch-sparse\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","import os.path as osp\n","import torch\n","import warnings\n","from torch_geometric.data import Data, HeteroData\n","from torch_geometric.transforms import RandomNodeSplit\n","from torch_geometric.nn import GCNConv, GATConv, SAGEConv, ChebConv\n","import torch_geometric.nn as pyg_nn\n","import torch.nn as nn\n","import torch_geometric.utils as pyg_utils\n","from torch.nn import Module, Linear\n","import torch.nn.functional as F\n","from sklearn.metrics import precision_recall_fscore_support, f1_score, classification_report\n","from torch_geometric.seed import seed_everything\n","import joblib\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","warnings.simplefilter(action='ignore')\n","SEED = 51\n","FILEPATH_TX = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/tx_result_2.csv\"\n","FILEPATH_WALLET = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/w_result_2.csv\"\n","\n","base_path = \"/content/drive/MyDrive/blockchain/E++/\"\n","path_comb = '/content/drive/MyDrive/blockchain/00_gnn/combination_do.csv'\n","\n","type_classification = 'w'"]},{"cell_type":"markdown","metadata":{"id":"XOk9zPoYDC5I"},"source":["##Crea db vuoto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCrI1kO2CYxi"},"outputs":[],"source":["def create_df():\n","  # Crea DataFrame vuoti\n","  df_tx = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","  df_wallet = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","\n","  # Salva i DataFrame come file CSV\n","  df_tx.to_csv(FILEPATH_TX, index=False)\n","  df_wallet.to_csv(FILEPATH_WALLET, index=False)\n","\n","#create_df()"]},{"cell_type":"markdown","metadata":{"id":"W1Syx8hzDPvM"},"source":["##Carica dati"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qobTcbrVqpwG"},"outputs":[],"source":["def load_data():\n","    # Loading transactions\n","\n","    #Reading edges, features and classes from transaction files (as done with the original dataset)\n","    df_edges_tx = pd.read_csv(osp.join(base_path, \"txs_edgelist.csv\"))\n","    df_features_tx = pd.read_csv(osp.join(base_path, \"txs_features.csv\"), header=None)\n","    df_classes_tx = pd.read_csv(osp.join(base_path, \"txs_classes.csv\"))\n","\n","    #Columns naming based on index\n","    colNames1_tx = {'0': 'txId', 1: \"Time step\"}\n","    colNames2_tx = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(94)}\n","    colNames3_tx = {str(ii+96): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n","\n","    colNames_tx = dict(colNames1_tx, **colNames2_tx, **colNames3_tx)\n","    colNames_tx = {int(jj): item_kk for jj, item_kk in colNames_tx.items()}\n","\n","    # Rename feature columns\n","    df_features_tx = df_features_tx.rename(columns=colNames_tx)\n","\n","    # Map unknown class to '3'\n","    df_classes_tx.loc[df_classes_tx['class'] == 'unknown', 'class'] = '3'\n","\n","    # Merge classes and features in one Dataframe\n","    df_class_feature_tx = pd.merge(df_classes_tx, df_features_tx)\n","\n","    # Exclude records with unknown class transaction\n","    df_class_feature_tx = df_class_feature_tx[df_class_feature_tx['class'] != 3]\n","\n","    # Build Dataframe with head and tail of transactions (edges)\n","    known_txs = df_class_feature_tx[\"txId\"].values\n","    df_edges_tx = df_edges_tx[(df_edges_tx[\"txId1\"].isin(known_txs)) & (df_edges_tx[\"txId2\"].isin(known_txs))]\n","\n","    # Build indices for features and edge types\n","    features_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"txId\"].unique()))}\n","    class_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_tx[\"txId\"] = df_class_feature_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_class_feature_tx[\"class\"] = df_class_feature_tx[\"class\"].apply(lambda name: class_idx_tx[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_tx[\"txId1\"] = df_edges_tx[\"txId1\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx[\"txId2\"] = df_edges_tx[\"txId2\"].apply(lambda name: features_idx_tx[name])\n","\n","    # Loading wallets\n","\n","    # From file\n","    df_edges_wallet = pd.read_csv(osp.join(base_path, \"AddrAddr_edgelist.csv\"))\n","    df_class_feature_wallet = pd.read_csv(osp.join(base_path, \"wallets_features_classes_combined.csv\"))\n","\n","    # Exclude records with unknown class transaction\n","    #print(df_class_feature_wallet.shape)\n","    df_class_feature_wallet = df_class_feature_wallet[df_class_feature_wallet[\"class\"] != 3]\n","    #print(df_class_feature_wallet.shape)\n","\n","    # Build Dataframe with head and tail of AddrToAddr (edges)\n","    known_wallets = df_class_feature_wallet[\"address\"].values\n","    df_edges_wallet = df_edges_wallet[(df_edges_wallet[\"input_address\"].isin(known_wallets)) & (df_edges_wallet[\"output_address\"].isin(known_wallets))]\n","\n","    # Building indices for features and edge types\n","    features_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"address\"].unique()))}\n","    class_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_wallet[\"address\"] = df_class_feature_wallet[\"address\"].apply(lambda name: features_idx_wallet[name])\n","    df_class_feature_wallet[\"class\"] = df_class_feature_wallet[\"class\"].apply(lambda name: class_idx_wallet[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet[\"input_address\"] = df_edges_wallet[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet[\"output_address\"] = df_edges_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    # Loading AddrTx and TxAddr\n","\n","    # From file\n","    df_edges_wallet_tx = pd.read_csv(osp.join(base_path, \"AddrTx_edgelist.csv\"))\n","    df_edges_tx_wallet = pd.read_csv(osp.join(base_path, \"TxAddr_edgelist.csv\"))\n","\n","    # Build Dataframe with head and tail of AddrTx (edges)\n","    df_edges_wallet_tx = df_edges_wallet_tx[(df_edges_wallet_tx[\"input_address\"].isin(known_wallets)) & df_edges_wallet_tx[\"txId\"].isin(known_txs)]\n","    df_edges_tx_wallet = df_edges_tx_wallet[(df_edges_tx_wallet[\"txId\"].isin(known_txs)) & df_edges_tx_wallet[\"output_address\"].isin(known_wallets)]\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet_tx[\"input_address\"] = df_edges_wallet_tx[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet_tx[\"txId\"] = df_edges_wallet_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"txId\"] = df_edges_tx_wallet[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"output_address\"] = df_edges_tx_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    return df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet\n","\n","def data_to_pyg(df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet):\n","    data = HeteroData()\n","\n","    # Defining PyG objects for transactions\n","    df_class_feature_tx = df_class_feature_tx.fillna(0)\n","    data['tx'].x = torch.tensor(df_class_feature_tx.iloc[:, 3:].values, dtype=torch.float)\n","    data['tx'].y = torch.tensor(df_class_feature_tx[\"class\"].values, dtype=torch.long)\n","    data['tx','is_related_to','tx'].edge_index = torch.tensor([df_edges_tx[\"txId1\"].values,\n","                            df_edges_tx[\"txId2\"].values], dtype=torch.int64)\n","    #data['tx'] = random_node_split(num_val=0.15, num_test=0.2)(data['tx'])\n","    # Defining PyG objects for wallets\n","    data['wallet'].x = torch.tensor(df_class_feature_wallet.iloc[:, 3:].values, dtype=torch.float)\n","    data['wallet'].y = torch.tensor(df_class_feature_wallet[\"class\"].values, dtype=torch.long)\n","    data['wallet','interacts_with','wallet'].edge_index = torch.tensor([df_edges_wallet[\"input_address\"].values,\n","                            df_edges_wallet[\"output_address\"].values], dtype=torch.int64)\n","    #data['wallet'] = random_node_split(num_val=0.15, num_test=0.2)(data['wallet'])\n","    # Defining PyG objects for cross-edges\n","    data['wallet','performs','tx'].edge_index = torch.tensor([df_edges_wallet_tx[\"input_address\"].values,\n","                                         df_edges_wallet_tx[\"txId\"].values], dtype=torch.int64)\n","\n","    data['tx', 'flows_into', 'wallet'].edge_index = torch.tensor([df_edges_tx_wallet[\"txId\"].values,\n","                                         df_edges_tx_wallet[\"output_address\"].values], dtype=torch.int64)\n","\n","    # Impostare il seed per la divisione del dataset\n","    return RandomNodeSplit(num_val=0.10, num_test=0.15)(data)"]},{"cell_type":"markdown","metadata":{"id":"OhFPkCpExzOL"},"source":["##Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQfEkZ1nx44T"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do2dQUF-x7hj"},"outputs":[],"source":["#NEW -> SU RANGE\n","# Utility per conversione a tensor\n","def to_tensor(arr):\n","    return torch.tensor(arr, dtype=torch.float).to(device)\n","\n","def scale_features(data, method=\"standard\"):\n","  if method == 'no':\n","    return data\n","\n","  # Scaling per training\n","  data['tx'].x[data['tx'].train_mask] = to_tensor(\n","      scale_train_data(data['tx'].x[data['tx'].train_mask].cpu().numpy(), method, 'tx')\n","  )\n","  data['wallet'].x[data['wallet'].train_mask] = to_tensor(\n","      scale_train_data(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), method, 'wallet')\n","  )\n","\n","  # Scaling per validation\n","  data['tx'].x[data['tx'].val_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].val_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].val_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), method, 'wallet'))\n","\n","  data['tx'].x[data['tx'].test_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].test_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].test_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), method, 'wallet'))\n","  return data\n","\n","def scale_train_data(train, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = StandardScaler()\n","        scaled_train = scaler.fit_transform(train)  # Scala tutte le colonne\n","        joblib.dump(scaler, f\"scaler_standard_{df}.pkl\")\n","\n","        if 'l2' in scaling_method:\n","            norm = Normalizer(norm='l2')\n","            scaled_train = norm.fit_transform(scaled_train)\n","            joblib.dump(norm, f\"scaler_l2_{df}.pkl\")\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_train\n","\n","def scale_validation_data(val, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = joblib.load(f\"scaler_standard_{df}.pkl\")\n","        scaled_val = scaler.transform(val)  # Scala tutte le colonne\n","\n","        if 'l2' in scaling_method:\n","            norm = joblib.load(f\"scaler_l2_{df}.pkl\")\n","            scaled_val = norm.transform(scaled_val)\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_val"]},{"cell_type":"code","source":["def dimentional_reduction(data, dim_reduction, pca_threshold):\n","    if dim_reduction == 'no':\n","        return data\n","    elif dim_reduction == 'pca':\n","        data1 = copy.deepcopy(data)\n","\n","        transformed_tx_data = apply_pca_train(data['tx'].x[data['tx'].train_mask].cpu().numpy(), 'tx', pca_threshold)\n","        transformed_wallet_data = apply_pca_train(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), 'wallet', pca_threshold)\n","\n","        data1['tx'].x = torch.zeros((data['tx'].x.shape[0], transformed_tx_data.shape[1]), dtype=torch.float, device=device)\n","        data1['wallet'].x = torch.zeros((data['wallet'].x.shape[0], transformed_wallet_data.shape[1]), dtype=torch.float, device=device)\n","\n","        data1['tx'].x[data['tx'].train_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].train_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].val_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].val_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].val_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].test_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].test_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].test_mask] = to_tensor(transformed_wallet_data)\n","\n","        return data1\n","\n","def apply_pca_train(train, df, pca_threshold=0.99):\n","    pca = PCA(random_state=SEED)\n","    pca.fit(train)\n","\n","    # Selezione componenti principali\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","\n","    pca = PCA(n_components=n_components, random_state=SEED)\n","    transformed_data = pca.fit_transform(train).astype(np.float32)\n","\n","    joblib.dump(pca, f\"pca_model_{df}.pkl\")\n","    print(f\"  Numero di componenti principali per {df}: {pca.n_components_}\")\n","\n","    return transformed_data\n","\n","def apply_pca_validation(val, df):\n","    pca = joblib.load(f\"pca_model_{df}.pkl\")\n","    transformed_data = pca.transform(val).astype(np.float32)\n","    return transformed_data"],"metadata":{"id":"PVRMmabn21eP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CaNNtrNLDSdI"},"source":["##Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbfXKtdynFBL"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, Dropout\n","from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, TransformerConv\n","import random\n","from itertools import product\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxfbqge_p64q"},"outputs":[],"source":["def set_seed(seed = 51):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # Per più GPU\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    seed_everything(seed)\n","device = \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiBF-BZYfq7b"},"outputs":[],"source":["class ResidualHeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0.5, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.skips = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1 # Define heads\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","            self.skips.append(Linear(hidden_channels * heads, hidden_channels * heads)) # Fix: Linear layer expects the output of conv\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv, skip in zip(self.convs, self.skips):\n","            x_dict_new = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(F.relu(x + skip(x_dict_new[key]))) for key, x in x_dict_new.items()}  # Residual + Dropout\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdcebmslq5D0"},"outputs":[],"source":["class HeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1  # Definiamo i heads solo se necessario\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv in self.convs:\n","            x_dict = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJEJVt5l04Eb"},"outputs":[],"source":["def is_combination_tested(filepath, new_row):\n","    existing_results = pd.read_csv(filepath)\n","\n","    # Identifica le colonne comuni tra il dataset e new_row\n","    dataset_columns = set(existing_results.columns)\n","    comparison_columns = [col for col in new_row.keys() if col not in ['end', 'num_epoch', 'epoch']]\n","\n","    # Filtra le combinazioni\n","    filtered_results = existing_results.copy()\n","    #filtered_results = filtered_results[filtered_results['end'] == True]\n","    filtered_results = filtered_results[filtered_results['num_epoch'] >= new_row['num_epoch']]\n","\n","    for col in comparison_columns:\n","        if col in dataset_columns:\n","            # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","            filtered_results = filtered_results[\n","                (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","            ]\n","\n","    return not filtered_results.empty\n","\n","def append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, end=False):\n","  def append_and_save_result(filepath, new_row, end=False):\n","    new_row['end'] = end\n","    # Leggi i risultati esistenti\n","    results_df = pd.read_csv(filepath)\n","    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","    results_df.to_csv(filepath, index=False)\n","\n","  append_and_save_result(FILEPATH_TX, params_tx, end)\n","  append_and_save_result(FILEPATH_WALLET, params_wallet, end)\n","  if end:\n","    df_comb = pd.read_csv(path_comb)\n","    filtered_params = {key: params_tx[key] for key in params_tx if \"train\" not in key and \"val\" not in key}\n","    print(filtered_params)\n","    df_comb = pd.concat([df_comb, pd.DataFrame([filtered_params])], ignore_index=True)\n","    df_comb.to_csv(path_comb, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vo1zTF6zxQR"},"outputs":[],"source":["def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def eval(model, data, out_tx, out_wallet, params):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalità di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  loss = params_tx['train_loss'] + params_wallet['train_loss']\n","\n","  with torch.no_grad():\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for wallets\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    print(f\"Epoch {str(params['epoch']).zfill(3)}:      TX: Train Loss: {params_tx['train_loss']:.4f}, Acc: {params_tx['train_acc']:.4f}, F1: {params_tx['train_f1']:.4f} Bal: {params_tx['train_balanced_acc']:.4f} - Val Loss: {params_tx['val_loss']:.4f}, Accuracy: {params_tx['val_acc']:.4f}, F1: {params_tx['val_f1']:.4f} Bal: {params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"           WALLETS: Train Loss: {params_wallet['train_loss']:.8f}, Acc: {params_wallet['train_acc']:.8f}, F1: {params_wallet['train_f1']:.8f} Bal: {params_wallet['train_balanced_acc']:.4f} - Val Loss: {params_wallet['val_loss']:.8f}, Accuracy: {params_wallet['val_acc']:.4f}, F1: {params_wallet['val_f1']:.4f} Bal: {params_wallet['val_balanced_acc']:.4f}\")\n","\n","  return loss, params_tx, params_wallet\n","\n","\n","def eval_total(model, data, out_tx, out_wallet, params, best_epoch):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalitÃ  di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","  tx_mask_test = data['tx'].test_mask\n","  wallet_mask_test = data['wallet'].test_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  with torch.no_grad():\n","    # Calculate metrics for validation\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    report_tx_val = classification_report(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    report_wallet_val = classification_report(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for test\n","    params_tx['test_loss'] = criterion(out_tx[tx_mask_test], data['tx'].y[tx_mask_test].cpu())  # Convert to scalar\n","    params_tx['test_acc'] = accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_precision'] = precision_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_recall'] = recall_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_f1'] = f1_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    report_tx_test = classification_report(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","\n","    params_wallet['test_loss'] = criterion(out_wallet[wallet_mask_test], data['wallet'].y[wallet_mask_test].cpu())  # Convert to scalar\n","    params_wallet['test_acc'] = accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_precision'] = precision_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_recall'] = recall_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_f1'] = f1_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    report_wallet_test = classification_report(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","\n","    # Stampa delle metriche con formattazione migliorata\n","    print('Final_result for '+type_classification)\n","    print(params)\n","    print(f\"Epoch {best_epoch}:\")\n","    print(\"  TX:\")\n","    print(f\"   Train: Loss={params_tx['train_loss']:.4f}, Acc={params_tx['train_acc']:.4f}, F1={params_tx['train_f1']:.4f}, Bal. Acc={params_tx['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_tx['val_loss']:.4f}, Acc={params_tx['val_acc']:.4f}, F1={params_tx['val_f1']:.4f}, Bal. Acc={params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_tx['test_loss']:.4f}, Acc={params_tx['test_acc']:.4f}, F1={params_tx['test_f1']:.4f}, Bal. Acc={params_tx['test_balanced_acc']:.4f}\")\n","    print(report_tx_val)\n","    print(report_tx_test)\n","    print(\"  WALLETS:\")\n","    print(f\"   Train: Loss={params_wallet['train_loss']:.8f}, Acc={params_wallet['train_acc']:.8f}, F1={params_wallet['train_f1']:.8f}, Bal. Acc={params_wallet['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_wallet['val_loss']:.8f}, Acc={params_wallet['val_acc']:.4f}, F1={params_wallet['val_f1']:.4f}, Bal. Acc={params_wallet['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_wallet['test_loss']:.8f}, Acc={params_wallet['test_acc']:.4f}, F1={params_wallet['test_f1']:.4f}, Bal. Acc={params_wallet['test_balanced_acc']:.4f}\")\n","    print(report_wallet_val)\n","    print(report_wallet_test)\n","    print()\n","\n","def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def train(model, data, params):\n","    best_model = None\n","    best_epoch = None\n","\n","    if params['optimizer'] == 'Adam':\n","      optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    elif params['optimizer'] == 'AdamW':\n","      optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    else:\n","      optimizer = None\n","\n","    if params['lr_scheduler'] == 'ReduceLROnPlateau':\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=params['factor'], patience=params['p'])\n","    elif params['lr_scheduler'] == 'CosineAnnealingLR':\n","      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=params['eta_min'])\n","    elif params['lr_scheduler'] == 'StepLR':\n","      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n","    else:\n","      scheduler = None\n","\n","    print(f'Combinazione: {params}')\n","    model.train()\n","    tx_mask = data['tx'].train_mask\n","    wallet_mask = data['wallet'].train_mask\n","    tx_mask_val =  data['tx'].val_mask\n","    wallet_mask_val = data['wallet'].val_mask\n","\n","    best_val_tx_acc = 0\n","    best_val_wallet_acc = 0\n","\n","    best_val_tx_loss = float('inf')\n","    best_val_wallet_loss = float('inf')\n","    patience = params['patience']\n","    epochs_since_best = 0\n","\n","    for epoch in range(params['num_epoch']):\n","        params['epoch'] = epoch+1\n","        optimizer.zero_grad()\n","        out_tx, out_wallet = model(data.x_dict, data.edge_index_dict)\n","        loss, params_tx, params_wallet = eval(model, data, out_tx, out_wallet, params)\n","\n","        val_tx_loss = params_tx['val_loss']\n","        val_wallet_loss = params_wallet['val_loss']\n","        val_tx_acc = params_tx['val_balanced_acc']\n","        val_wallet_acc = params_wallet['val_balanced_acc']\n","\n","        # Check if validation loss has improved\n","        if val_tx_loss < best_val_tx_loss or val_wallet_loss < best_val_wallet_loss:\n","            best_val_tx_loss = val_tx_loss\n","            best_val_wallet_loss = val_wallet_loss\n","            epochs_since_best = 0\n","        else:\n","            epochs_since_best += 1\n","\n","        # Check if early stopping criteria is met\n","        if epochs_since_best >= patience:\n","            print(f'Early stopping at epoch {epoch}')\n","            append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, True)\n","            out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","            eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","            return best_model\n","\n","        if type_classification == 'w':\n","          if best_val_wallet_acc < val_wallet_acc:\n","            best_val_wallet_acc = val_wallet_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        elif type_classification == 'tx':\n","          if best_val_tx_acc < val_tx_acc:\n","            best_val_tx_acc = val_tx_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        else:\n","          print('Definisci modello da considerare')\n","          raise ValueError\n","\n","        loss.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","        scheduler.step(loss)\n","\n","        append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, params['epoch']==params['num_epoch'])\n","\n","    out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","    eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","    return model\n","\n","\n","def train_grid(data_full, param_grid, scalers, dim_reductions, pca_thresholds):\n","    best_model = None\n","    best_f1 = 0\n","\n","    keys, values = zip(*param_grid.items())\n","    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","    combination_counter = 0\n","    total_combinations = len(param_combinations) * len(scalers) * len(dim_reductions) * len(pca_thresholds)\n","\n","    for scaler in scalers:\n","      data = scale_features(data_full.clone(), scaler)\n","      for dim_reduction in dim_reductions:\n","        for pca_threshold in pca_thresholds:\n","          data = dimentional_reduction(data, dim_reduction, pca_threshold)\n","\n","          for params in param_combinations:\n","            combination_counter += 1\n","\n","            set_seed(SEED)\n","            params['scaler'] = scaler\n","            params['dim_reduction'] = dim_reduction # Fixed the typo here: 'dim_reduction' instead of 'dim_reducition'\n","\n","            if params['lr_scheduler'] != 'ReduceLROnPlateau':\n","              params['p'] = None\n","              params['factor'] = None\n","            elif params['lr_scheduler'] != 'CosineAnnealingLR':\n","              params['T_max'] = None\n","              params['eta_min'] = None\n","\n","            if params['conv_type'] != 'Transformer':\n","              params['num_head'] = None\n","\n","            if dim_reduction == 'no':\n","              params['pca_threshold'] = None\n","            else:\n","              params['pca_threshold'] = pca_threshold\n","\n","            if True: #not is_combination_tested(path_comb, params):\n","              print(f\"  Combinazione {combination_counter}/{total_combinations}\")  # Print the counter\n","              model = None\n","              if params[ 'type_model'] == 'HeteroGNN':\n","                model = HeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              elif params[ 'type_model'] == 'ResidualHeteroGNN':\n","                model = ResidualHeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              model = train(model, data, params)\n","            else:\n","              print(f\"  Configurazione {combination_counter}/{total_combinations} già testata, salto...\")\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOYfyiGS4aMO"},"outputs":[],"source":["set_seed(SEED)\n","data = data_to_pyg(*load_data())"]},{"cell_type":"markdown","source":["# Risultati"],"metadata":{"id":"muZp-VdiyQpH"}},{"cell_type":"code","source":["hyperparams = {\n","    \"hidden_channels\": [128],\n","    'num_head': [2],\n","    \"num_layers\": [2],\n","    \"num_epoch\": [300],\n","    \"patience\": [50],\n","    \"lr\": [0.001],\n","    \"weight_decay\": [1e-5],\n","    \"dropout\": [0],\n","    \"conv_type\": ['SAGE'],\n","    \"p\": ['/'],\n","    \"factor\": [\"/\"],\n","    \"eta_min\": [1e-5],\n","    \"T_max\": [15], #10 15\n","    \"aggr\": ['sum'], #,\n","    'lr_scheduler':['CosineAnnealingLR'],\n","    'optimizer': ['Adam'], #Adam\n","    'type_model':['HeteroGNN'],\n","}\n","\n","scaler = ['standard_l2']\n","dim_reduction=['pca']\n","pca_threshold=[0.99]\n","best_model = train_grid(data, hyperparams, scaler, dim_reduction, pca_threshold)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dg8Ev5XeRv8h","executionInfo":{"status":"ok","timestamp":1742675681689,"user_tz":-60,"elapsed":3696419,"user":{"displayName":"Milena Mazza","userId":"07299589247263017405"}},"outputId":"946ffe98-a0ee-4144-d9cb-3173be8b9357"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Numero di componenti principali per tx: 73\n","  Numero di componenti principali per wallet: 22\n","  Combinazione 1/1\n","Combinazione: {'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'SAGE', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 15, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99}\n","Epoch 001:      TX: Train Loss: 0.6957, Acc: 0.3908, F1: 0.5257 Bal: 0.4537 - Val Loss: 0.6958, Accuracy: 0.3780, F1: 0.5113 Bal: 0.4455\n","           WALLETS: Train Loss: 0.70427126, Acc: 0.07749888, F1: 0.00003146 Bal: 0.5000 - Val Loss: 0.70359403, Accuracy: 0.0791, F1: 0.0000 Bal: 0.5000\n","Epoch 002:      TX: Train Loss: 0.6870, Acc: 0.4982, F1: 0.6282 Bal: 0.6067 - Val Loss: 0.6872, Accuracy: 0.4831, F1: 0.6135 Bal: 0.5942\n","           WALLETS: Train Loss: 0.68799949, Acc: 0.39162712, F1: 0.52230769 Bal: 0.5612 - Val Loss: 0.68805581, Accuracy: 0.3886, F1: 0.5177 Bal: 0.5606\n","Epoch 003:      TX: Train Loss: 0.6786, Acc: 0.6310, F1: 0.7492 Bal: 0.7054 - Val Loss: 0.6790, Accuracy: 0.6200, F1: 0.7400 Bal: 0.6987\n","           WALLETS: Train Loss: 0.67625612, Acc: 0.87290097, F1: 0.93006215 Bal: 0.6374 - Val Loss: 0.67683882, Accuracy: 0.8716, F1: 0.9293 Bal: 0.6341\n","Epoch 004:      TX: Train Loss: 0.6700, Acc: 0.7095, F1: 0.8122 Bal: 0.7558 - Val Loss: 0.6705, Accuracy: 0.7025, F1: 0.8074 Bal: 0.7435\n","           WALLETS: Train Loss: 0.66647434, Acc: 0.90597016, F1: 0.94959024 Bal: 0.6112 - Val Loss: 0.66743147, Accuracy: 0.9049, F1: 0.9489 Bal: 0.6124\n","Epoch 005:      TX: Train Loss: 0.6607, Acc: 0.7472, F1: 0.8404 Bal: 0.7798 - Val Loss: 0.6614, Accuracy: 0.7407, F1: 0.8361 Bal: 0.7657\n","           WALLETS: Train Loss: 0.65710205, Acc: 0.91128576, F1: 0.95270994 Bal: 0.5983 - Val Loss: 0.65829802, Accuracy: 0.9103, F1: 0.9521 Bal: 0.6002\n","Epoch 006:      TX: Train Loss: 0.6505, Acc: 0.7654, F1: 0.8536 Bal: 0.7885 - Val Loss: 0.6513, Accuracy: 0.7666, F1: 0.8547 Bal: 0.7840\n","           WALLETS: Train Loss: 0.64704549, Acc: 0.91062539, F1: 0.95229352 Bal: 0.6035 - Val Loss: 0.64832616, Accuracy: 0.9093, F1: 0.9515 Bal: 0.6036\n","Epoch 007:      TX: Train Loss: 0.6390, Acc: 0.7793, F1: 0.8633 Bal: 0.7976 - Val Loss: 0.6401, Accuracy: 0.7797, F1: 0.8638 Bal: 0.7952\n","           WALLETS: Train Loss: 0.63586235, Acc: 0.90593750, F1: 0.94948125 Bal: 0.6211 - Val Loss: 0.63709539, Accuracy: 0.9044, F1: 0.9486 Bal: 0.6198\n","Epoch 008:      TX: Train Loss: 0.6262, Acc: 0.7877, F1: 0.8693 Bal: 0.8021 - Val Loss: 0.6276, Accuracy: 0.7889, F1: 0.8702 Bal: 0.8013\n","           WALLETS: Train Loss: 0.62364542, Acc: 0.89904356, F1: 0.94511317 Bal: 0.6637 - Val Loss: 0.62472862, Accuracy: 0.8987, F1: 0.9448 Bal: 0.6682\n","Epoch 009:      TX: Train Loss: 0.6120, Acc: 0.7939, F1: 0.8736 Bal: 0.8061 - Val Loss: 0.6137, Accuracy: 0.7977, F1: 0.8763 Bal: 0.8062\n","           WALLETS: Train Loss: 0.61070019, Acc: 0.89043338, F1: 0.93950085 Bal: 0.7172 - Val Loss: 0.61155778, Accuracy: 0.8896, F1: 0.9389 Bal: 0.7203\n","Epoch 010:      TX: Train Loss: 0.5963, Acc: 0.8003, F1: 0.8780 Bal: 0.8081 - Val Loss: 0.5983, Accuracy: 0.8034, F1: 0.8802 Bal: 0.8084\n","           WALLETS: Train Loss: 0.59744734, Acc: 0.88184859, F1: 0.93395960 Bal: 0.7521 - Val Loss: 0.59803396, Accuracy: 0.8814, F1: 0.9336 Bal: 0.7542\n","Epoch 011:      TX: Train Loss: 0.5791, Acc: 0.8062, F1: 0.8820 Bal: 0.8109 - Val Loss: 0.5815, Accuracy: 0.8078, F1: 0.8833 Bal: 0.8098\n","           WALLETS: Train Loss: 0.58423811, Acc: 0.87443941, F1: 0.92927637 Bal: 0.7667 - Val Loss: 0.58456206, Accuracy: 0.8738, F1: 0.9288 Bal: 0.7660\n","Epoch 012:      TX: Train Loss: 0.5606, Acc: 0.8131, F1: 0.8867 Bal: 0.8137 - Val Loss: 0.5635, Accuracy: 0.8177, F1: 0.8899 Bal: 0.8153\n","           WALLETS: Train Loss: 0.57114309, Acc: 0.86699395, F1: 0.92453448 Bal: 0.7788 - Val Loss: 0.57124597, Accuracy: 0.8658, F1: 0.9237 Bal: 0.7757\n","Epoch 013:      TX: Train Loss: 0.5409, Acc: 0.8192, F1: 0.8909 Bal: 0.8157 - Val Loss: 0.5442, Accuracy: 0.8247, F1: 0.8946 Bal: 0.8192\n","           WALLETS: Train Loss: 0.55800557, Acc: 0.86016894, F1: 0.92022836 Bal: 0.7833 - Val Loss: 0.55794245, Accuracy: 0.8589, F1: 0.9194 Bal: 0.7800\n","Epoch 014:      TX: Train Loss: 0.5203, Acc: 0.8250, F1: 0.8948 Bal: 0.8183 - Val Loss: 0.5242, Accuracy: 0.8306, F1: 0.8985 Bal: 0.8245\n","           WALLETS: Train Loss: 0.54475778, Acc: 0.85779234, F1: 0.91872668 Bal: 0.7842 - Val Loss: 0.54461294, Accuracy: 0.8568, F1: 0.9180 Bal: 0.7810\n","Epoch 015:      TX: Train Loss: 0.4992, Acc: 0.8303, F1: 0.8983 Bal: 0.8208 - Val Loss: 0.5036, Accuracy: 0.8339, F1: 0.9007 Bal: 0.8253\n","           WALLETS: Train Loss: 0.53160954, Acc: 0.85897157, F1: 0.91945858 Bal: 0.7846 - Val Loss: 0.53148162, Accuracy: 0.8579, F1: 0.9187 Bal: 0.7821\n","Epoch 016:      TX: Train Loss: 0.4778, Acc: 0.8339, F1: 0.9006 Bal: 0.8229 - Val Loss: 0.4827, Accuracy: 0.8383, F1: 0.9036 Bal: 0.8268\n","           WALLETS: Train Loss: 0.51900321, Acc: 0.86184163, F1: 0.92125155 Bal: 0.7846 - Val Loss: 0.51898450, Accuracy: 0.8608, F1: 0.9205 Bal: 0.7822\n","Epoch 017:      TX: Train Loss: 0.4565, Acc: 0.8371, F1: 0.9028 Bal: 0.8249 - Val Loss: 0.4620, Accuracy: 0.8392, F1: 0.9041 Bal: 0.8273\n","           WALLETS: Train Loss: 0.50741822, Acc: 0.86422548, F1: 0.92273608 Bal: 0.7845 - Val Loss: 0.50754267, Accuracy: 0.8633, F1: 0.9221 Bal: 0.7820\n","Epoch 018:      TX: Train Loss: 0.4359, Acc: 0.8402, F1: 0.9047 Bal: 0.8278 - Val Loss: 0.4418, Accuracy: 0.8401, F1: 0.9047 Bal: 0.8277\n","           WALLETS: Train Loss: 0.49713111, Acc: 0.86599614, F1: 0.92382956 Bal: 0.7848 - Val Loss: 0.49736372, Accuracy: 0.8648, F1: 0.9230 Bal: 0.7830\n","Epoch 019:      TX: Train Loss: 0.4162, Acc: 0.8425, F1: 0.9063 Bal: 0.8295 - Val Loss: 0.4225, Accuracy: 0.8436, F1: 0.9070 Bal: 0.8307\n","           WALLETS: Train Loss: 0.48807782, Acc: 0.86634447, F1: 0.92396545 Bal: 0.7903 - Val Loss: 0.48835412, Accuracy: 0.8650, F1: 0.9231 Bal: 0.7880\n","Epoch 020:      TX: Train Loss: 0.3976, Acc: 0.8468, F1: 0.9090 Bal: 0.8334 - Val Loss: 0.4043, Accuracy: 0.8464, F1: 0.9088 Bal: 0.8323\n","           WALLETS: Train Loss: 0.48020560, Acc: 0.86459558, F1: 0.92282303 Bal: 0.7941 - Val Loss: 0.48043641, Accuracy: 0.8636, F1: 0.9221 Bal: 0.7911\n","Epoch 021:      TX: Train Loss: 0.3806, Acc: 0.8503, F1: 0.9112 Bal: 0.8377 - Val Loss: 0.3875, Accuracy: 0.8504, F1: 0.9114 Bal: 0.8354\n","           WALLETS: Train Loss: 0.47364822, Acc: 0.86002380, F1: 0.91995136 Bal: 0.7953 - Val Loss: 0.47376227, Accuracy: 0.8591, F1: 0.9193 Bal: 0.7928\n","Epoch 022:      TX: Train Loss: 0.3651, Acc: 0.8536, F1: 0.9133 Bal: 0.8409 - Val Loss: 0.3722, Accuracy: 0.8532, F1: 0.9132 Bal: 0.8380\n","           WALLETS: Train Loss: 0.46863061, Acc: 0.85492591, F1: 0.91673799 Bal: 0.7960 - Val Loss: 0.46859860, Accuracy: 0.8540, F1: 0.9161 Bal: 0.7935\n","Epoch 023:      TX: Train Loss: 0.3513, Acc: 0.8572, F1: 0.9156 Bal: 0.8444 - Val Loss: 0.3584, Accuracy: 0.8554, F1: 0.9145 Bal: 0.8412\n","           WALLETS: Train Loss: 0.46509063, Acc: 0.85150070, F1: 0.91457970 Bal: 0.7956 - Val Loss: 0.46493775, Accuracy: 0.8506, F1: 0.9140 Bal: 0.7931\n","Epoch 024:      TX: Train Loss: 0.3392, Acc: 0.8606, F1: 0.9177 Bal: 0.8483 - Val Loss: 0.3462, Accuracy: 0.8587, F1: 0.9166 Bal: 0.8440\n","           WALLETS: Train Loss: 0.46263817, Acc: 0.84927650, F1: 0.91317906 Bal: 0.7950 - Val Loss: 0.46245039, Accuracy: 0.8483, F1: 0.9125 Bal: 0.7924\n","Epoch 025:      TX: Train Loss: 0.3286, Acc: 0.8632, F1: 0.9193 Bal: 0.8525 - Val Loss: 0.3355, Accuracy: 0.8629, F1: 0.9192 Bal: 0.8502\n","           WALLETS: Train Loss: 0.46082941, Acc: 0.84657334, F1: 0.91150229 Bal: 0.7925 - Val Loss: 0.46071300, Accuracy: 0.8461, F1: 0.9111 Bal: 0.7904\n","Epoch 026:      TX: Train Loss: 0.3195, Acc: 0.8666, F1: 0.9214 Bal: 0.8576 - Val Loss: 0.3262, Accuracy: 0.8659, F1: 0.9211 Bal: 0.8529\n","           WALLETS: Train Loss: 0.45943269, Acc: 0.84507119, F1: 0.91056996 Bal: 0.7910 - Val Loss: 0.45947087, Accuracy: 0.8438, F1: 0.9097 Bal: 0.7878\n","Epoch 027:      TX: Train Loss: 0.3116, Acc: 0.8695, F1: 0.9232 Bal: 0.8614 - Val Loss: 0.3180, Accuracy: 0.8675, F1: 0.9221 Bal: 0.8528\n","           WALLETS: Train Loss: 0.45824590, Acc: 0.84515827, F1: 0.91063333 Bal: 0.7906 - Val Loss: 0.45845956, Accuracy: 0.8442, F1: 0.9100 Bal: 0.7887\n","Epoch 028:      TX: Train Loss: 0.3049, Acc: 0.8723, F1: 0.9249 Bal: 0.8653 - Val Loss: 0.3110, Accuracy: 0.8699, F1: 0.9236 Bal: 0.8590\n","           WALLETS: Train Loss: 0.45694768, Acc: 0.84535058, F1: 0.91075123 Bal: 0.7908 - Val Loss: 0.45730373, Accuracy: 0.8448, F1: 0.9103 Bal: 0.7891\n","Epoch 029:      TX: Train Loss: 0.2990, Acc: 0.8747, F1: 0.9264 Bal: 0.8682 - Val Loss: 0.3048, Accuracy: 0.8725, F1: 0.9252 Bal: 0.8615\n","           WALLETS: Train Loss: 0.45520470, Acc: 0.84391736, F1: 0.90982271 Bal: 0.7916 - Val Loss: 0.45564061, Accuracy: 0.8432, F1: 0.9093 Bal: 0.7898\n","Epoch 030:      TX: Train Loss: 0.2940, Acc: 0.8767, F1: 0.9277 Bal: 0.8713 - Val Loss: 0.2995, Accuracy: 0.8758, F1: 0.9272 Bal: 0.8663\n","           WALLETS: Train Loss: 0.45290217, Acc: 0.84309734, F1: 0.90929609 Bal: 0.7917 - Val Loss: 0.45335257, Accuracy: 0.8423, F1: 0.9087 Bal: 0.7898\n","Epoch 031:      TX: Train Loss: 0.2896, Acc: 0.8790, F1: 0.9290 Bal: 0.8747 - Val Loss: 0.2948, Accuracy: 0.8774, F1: 0.9282 Bal: 0.8671\n","           WALLETS: Train Loss: 0.45015001, Acc: 0.84298486, F1: 0.90909511 Bal: 0.7990 - Val Loss: 0.45057967, Accuracy: 0.8418, F1: 0.9083 Bal: 0.7963\n","Epoch 032:      TX: Train Loss: 0.2857, Acc: 0.8812, F1: 0.9304 Bal: 0.8778 - Val Loss: 0.2906, Accuracy: 0.8796, F1: 0.9295 Bal: 0.8713\n","           WALLETS: Train Loss: 0.44706929, Acc: 0.84237529, F1: 0.90870384 Bal: 0.7990 - Val Loss: 0.44748843, Accuracy: 0.8413, F1: 0.9080 Bal: 0.7961\n","Epoch 033:      TX: Train Loss: 0.2821, Acc: 0.8830, F1: 0.9315 Bal: 0.8798 - Val Loss: 0.2870, Accuracy: 0.8822, F1: 0.9312 Bal: 0.8717\n","           WALLETS: Train Loss: 0.44373992, Acc: 0.84242246, F1: 0.90873250 Bal: 0.7991 - Val Loss: 0.44418719, Accuracy: 0.8417, F1: 0.9082 Bal: 0.7962\n","Epoch 034:      TX: Train Loss: 0.2787, Acc: 0.8853, F1: 0.9329 Bal: 0.8817 - Val Loss: 0.2837, Accuracy: 0.8831, F1: 0.9317 Bal: 0.8732\n","           WALLETS: Train Loss: 0.44026315, Acc: 0.84316265, F1: 0.90920243 Bal: 0.7993 - Val Loss: 0.44078776, Accuracy: 0.8425, F1: 0.9087 Bal: 0.7964\n","Epoch 035:      TX: Train Loss: 0.2755, Acc: 0.8861, F1: 0.9334 Bal: 0.8822 - Val Loss: 0.2807, Accuracy: 0.8864, F1: 0.9337 Bal: 0.8770\n","           WALLETS: Train Loss: 0.43683583, Acc: 0.84591298, F1: 0.91094358 Bal: 0.8004 - Val Loss: 0.43746856, Accuracy: 0.8453, F1: 0.9105 Bal: 0.7976\n","Epoch 036:      TX: Train Loss: 0.2722, Acc: 0.8878, F1: 0.9345 Bal: 0.8837 - Val Loss: 0.2779, Accuracy: 0.8881, F1: 0.9348 Bal: 0.8790\n","           WALLETS: Train Loss: 0.43365043, Acc: 0.84868870, F1: 0.91270034 Bal: 0.8012 - Val Loss: 0.43439072, Accuracy: 0.8481, F1: 0.9122 Bal: 0.7981\n","Epoch 037:      TX: Train Loss: 0.2688, Acc: 0.8890, F1: 0.9352 Bal: 0.8853 - Val Loss: 0.2751, Accuracy: 0.8885, F1: 0.9351 Bal: 0.8782\n","           WALLETS: Train Loss: 0.43076995, Acc: 0.85223364, F1: 0.91493206 Bal: 0.8024 - Val Loss: 0.43157524, Accuracy: 0.8520, F1: 0.9147 Bal: 0.7996\n","Epoch 038:      TX: Train Loss: 0.2653, Acc: 0.8904, F1: 0.9361 Bal: 0.8870 - Val Loss: 0.2725, Accuracy: 0.8901, F1: 0.9360 Bal: 0.8791\n","           WALLETS: Train Loss: 0.42817727, Acc: 0.85491502, F1: 0.91660653 Bal: 0.8037 - Val Loss: 0.42897248, Accuracy: 0.8545, F1: 0.9163 Bal: 0.8007\n","Epoch 039:      TX: Train Loss: 0.2618, Acc: 0.8924, F1: 0.9373 Bal: 0.8891 - Val Loss: 0.2699, Accuracy: 0.8905, F1: 0.9363 Bal: 0.8813\n","           WALLETS: Train Loss: 0.42588720, Acc: 0.85756375, F1: 0.91825073 Bal: 0.8053 - Val Loss: 0.42660734, Accuracy: 0.8572, F1: 0.9180 Bal: 0.8023\n","Epoch 040:      TX: Train Loss: 0.2581, Acc: 0.8943, F1: 0.9384 Bal: 0.8912 - Val Loss: 0.2673, Accuracy: 0.8912, F1: 0.9367 Bal: 0.8807\n","           WALLETS: Train Loss: 0.42390773, Acc: 0.85837651, F1: 0.91875138 Bal: 0.8060 - Val Loss: 0.42452353, Accuracy: 0.8583, F1: 0.9186 Bal: 0.8032\n","Epoch 041:      TX: Train Loss: 0.2542, Acc: 0.8955, F1: 0.9392 Bal: 0.8929 - Val Loss: 0.2648, Accuracy: 0.8914, F1: 0.9368 Bal: 0.8818\n","           WALLETS: Train Loss: 0.42214718, Acc: 0.85938883, F1: 0.91936957 Bal: 0.8071 - Val Loss: 0.42267019, Accuracy: 0.8596, F1: 0.9194 Bal: 0.8042\n","Epoch 042:      TX: Train Loss: 0.2504, Acc: 0.8972, F1: 0.9402 Bal: 0.8948 - Val Loss: 0.2623, Accuracy: 0.8936, F1: 0.9382 Bal: 0.8830\n","           WALLETS: Train Loss: 0.42046362, Acc: 0.86084019, F1: 0.92026420 Bal: 0.8082 - Val Loss: 0.42093250, Accuracy: 0.8608, F1: 0.9202 Bal: 0.8049\n","Epoch 043:      TX: Train Loss: 0.2465, Acc: 0.8985, F1: 0.9410 Bal: 0.8957 - Val Loss: 0.2599, Accuracy: 0.8938, F1: 0.9383 Bal: 0.8831\n","           WALLETS: Train Loss: 0.41879681, Acc: 0.86364494, F1: 0.92200569 Bal: 0.8091 - Val Loss: 0.41923630, Accuracy: 0.8634, F1: 0.9218 Bal: 0.8059\n","Epoch 044:      TX: Train Loss: 0.2426, Acc: 0.8992, F1: 0.9414 Bal: 0.8968 - Val Loss: 0.2575, Accuracy: 0.8964, F1: 0.9399 Bal: 0.8846\n","           WALLETS: Train Loss: 0.41712937, Acc: 0.86518701, F1: 0.92296368 Bal: 0.8094 - Val Loss: 0.41754067, Accuracy: 0.8645, F1: 0.9225 Bal: 0.8052\n","Epoch 045:      TX: Train Loss: 0.2387, Acc: 0.9009, F1: 0.9424 Bal: 0.8986 - Val Loss: 0.2552, Accuracy: 0.8993, F1: 0.9417 Bal: 0.8871\n","           WALLETS: Train Loss: 0.41540319, Acc: 0.86625013, F1: 0.92361632 Bal: 0.8100 - Val Loss: 0.41574782, Accuracy: 0.8658, F1: 0.9233 Bal: 0.8065\n","Epoch 046:      TX: Train Loss: 0.2349, Acc: 0.9028, F1: 0.9436 Bal: 0.9001 - Val Loss: 0.2530, Accuracy: 0.9008, F1: 0.9426 Bal: 0.8880\n","           WALLETS: Train Loss: 0.41357988, Acc: 0.86595623, F1: 0.92343213 Bal: 0.8101 - Val Loss: 0.41380581, Accuracy: 0.8658, F1: 0.9233 Bal: 0.8066\n","Epoch 047:      TX: Train Loss: 0.2312, Acc: 0.9044, F1: 0.9446 Bal: 0.9022 - Val Loss: 0.2509, Accuracy: 0.9022, F1: 0.9434 Bal: 0.8877\n","           WALLETS: Train Loss: 0.41169307, Acc: 0.86521966, F1: 0.92296941 Bal: 0.8104 - Val Loss: 0.41177312, Accuracy: 0.8646, F1: 0.9226 Bal: 0.8071\n","Epoch 048:      TX: Train Loss: 0.2276, Acc: 0.9065, F1: 0.9458 Bal: 0.9043 - Val Loss: 0.2488, Accuracy: 0.9035, F1: 0.9442 Bal: 0.8885\n","           WALLETS: Train Loss: 0.40978751, Acc: 0.86450850, F1: 0.92251057 Bal: 0.8114 - Val Loss: 0.40972778, Accuracy: 0.8642, F1: 0.9222 Bal: 0.8089\n","Epoch 049:      TX: Train Loss: 0.2241, Acc: 0.9075, F1: 0.9464 Bal: 0.9050 - Val Loss: 0.2469, Accuracy: 0.9046, F1: 0.9449 Bal: 0.8901\n","           WALLETS: Train Loss: 0.40785694, Acc: 0.86436699, F1: 0.92241821 Bal: 0.8116 - Val Loss: 0.40770218, Accuracy: 0.8642, F1: 0.9223 Bal: 0.8089\n","Epoch 050:      TX: Train Loss: 0.2207, Acc: 0.9096, F1: 0.9477 Bal: 0.9073 - Val Loss: 0.2450, Accuracy: 0.9059, F1: 0.9457 Bal: 0.8908\n","           WALLETS: Train Loss: 0.40592256, Acc: 0.86497656, F1: 0.92279541 Bal: 0.8119 - Val Loss: 0.40572235, Accuracy: 0.8649, F1: 0.9227 Bal: 0.8097\n","Epoch 051:      TX: Train Loss: 0.2174, Acc: 0.9113, F1: 0.9487 Bal: 0.9093 - Val Loss: 0.2432, Accuracy: 0.9083, F1: 0.9471 Bal: 0.8921\n","           WALLETS: Train Loss: 0.40403402, Acc: 0.86570224, F1: 0.92324856 Bal: 0.8118 - Val Loss: 0.40380934, Accuracy: 0.8658, F1: 0.9232 Bal: 0.8100\n","Epoch 052:      TX: Train Loss: 0.2141, Acc: 0.9133, F1: 0.9500 Bal: 0.9107 - Val Loss: 0.2413, Accuracy: 0.9105, F1: 0.9484 Bal: 0.8953\n","           WALLETS: Train Loss: 0.40220326, Acc: 0.86627190, F1: 0.92359867 Bal: 0.8122 - Val Loss: 0.40194011, Accuracy: 0.8660, F1: 0.9234 Bal: 0.8093\n","Epoch 053:      TX: Train Loss: 0.2109, Acc: 0.9154, F1: 0.9512 Bal: 0.9124 - Val Loss: 0.2394, Accuracy: 0.9120, F1: 0.9494 Bal: 0.8952\n","           WALLETS: Train Loss: 0.40042105, Acc: 0.86616305, F1: 0.92352602 Bal: 0.8125 - Val Loss: 0.40009442, Accuracy: 0.8659, F1: 0.9233 Bal: 0.8095\n","Epoch 054:      TX: Train Loss: 0.2076, Acc: 0.9177, F1: 0.9526 Bal: 0.9145 - Val Loss: 0.2375, Accuracy: 0.9131, F1: 0.9501 Bal: 0.8948\n","           WALLETS: Train Loss: 0.39870355, Acc: 0.86586189, F1: 0.92333280 Bal: 0.8129 - Val Loss: 0.39831021, Accuracy: 0.8655, F1: 0.9230 Bal: 0.8098\n","Epoch 055:      TX: Train Loss: 0.2044, Acc: 0.9194, F1: 0.9536 Bal: 0.9155 - Val Loss: 0.2355, Accuracy: 0.9149, F1: 0.9511 Bal: 0.8968\n","           WALLETS: Train Loss: 0.39704797, Acc: 0.86588729, F1: 0.92334207 Bal: 0.8133 - Val Loss: 0.39661682, Accuracy: 0.8655, F1: 0.9230 Bal: 0.8107\n","Epoch 056:      TX: Train Loss: 0.2012, Acc: 0.9213, F1: 0.9548 Bal: 0.9178 - Val Loss: 0.2335, Accuracy: 0.9179, F1: 0.9530 Bal: 0.8985\n","           WALLETS: Train Loss: 0.39542764, Acc: 0.86676536, F1: 0.92388785 Bal: 0.8134 - Val Loss: 0.39499450, Accuracy: 0.8665, F1: 0.9236 Bal: 0.8111\n","Epoch 057:      TX: Train Loss: 0.1980, Acc: 0.9234, F1: 0.9560 Bal: 0.9193 - Val Loss: 0.2315, Accuracy: 0.9186, F1: 0.9534 Bal: 0.8988\n","           WALLETS: Train Loss: 0.39383647, Acc: 0.86825663, F1: 0.92481379 Bal: 0.8135 - Val Loss: 0.39342469, Accuracy: 0.8678, F1: 0.9245 Bal: 0.8112\n","Epoch 058:      TX: Train Loss: 0.1948, Acc: 0.9253, F1: 0.9571 Bal: 0.9205 - Val Loss: 0.2295, Accuracy: 0.9195, F1: 0.9539 Bal: 0.8974\n","           WALLETS: Train Loss: 0.39226234, Acc: 0.86968984, F1: 0.92569866 Bal: 0.8139 - Val Loss: 0.39187276, Accuracy: 0.8697, F1: 0.9256 Bal: 0.8114\n","Epoch 059:      TX: Train Loss: 0.1917, Acc: 0.9278, F1: 0.9586 Bal: 0.9226 - Val Loss: 0.2275, Accuracy: 0.9195, F1: 0.9539 Bal: 0.8983\n","           WALLETS: Train Loss: 0.39067754, Acc: 0.87048809, F1: 0.92618832 Bal: 0.8142 - Val Loss: 0.39027315, Accuracy: 0.8705, F1: 0.9262 Bal: 0.8122\n","Epoch 060:      TX: Train Loss: 0.1885, Acc: 0.9299, F1: 0.9598 Bal: 0.9249 - Val Loss: 0.2256, Accuracy: 0.9215, F1: 0.9551 Bal: 0.8994\n","           WALLETS: Train Loss: 0.38907105, Acc: 0.87054977, F1: 0.92622149 Bal: 0.8146 - Val Loss: 0.38862392, Accuracy: 0.8705, F1: 0.9261 Bal: 0.8125\n","Epoch 061:      TX: Train Loss: 0.1854, Acc: 0.9313, F1: 0.9607 Bal: 0.9265 - Val Loss: 0.2238, Accuracy: 0.9239, F1: 0.9565 Bal: 0.8998\n","           WALLETS: Train Loss: 0.38746282, Acc: 0.87119563, F1: 0.92660719 Bal: 0.8156 - Val Loss: 0.38698727, Accuracy: 0.8710, F1: 0.9264 Bal: 0.8129\n","Epoch 062:      TX: Train Loss: 0.1822, Acc: 0.9329, F1: 0.9616 Bal: 0.9285 - Val Loss: 0.2220, Accuracy: 0.9247, F1: 0.9570 Bal: 0.9013\n","           WALLETS: Train Loss: 0.38584489, Acc: 0.87172175, F1: 0.92692344 Bal: 0.8163 - Val Loss: 0.38537893, Accuracy: 0.8714, F1: 0.9267 Bal: 0.8132\n","Epoch 063:      TX: Train Loss: 0.1791, Acc: 0.9344, F1: 0.9625 Bal: 0.9299 - Val Loss: 0.2203, Accuracy: 0.9258, F1: 0.9577 Bal: 0.9019\n","           WALLETS: Train Loss: 0.38423151, Acc: 0.87278124, F1: 0.92757162 Bal: 0.8168 - Val Loss: 0.38380566, Accuracy: 0.8723, F1: 0.9272 Bal: 0.8133\n","Epoch 064:      TX: Train Loss: 0.1761, Acc: 0.9354, F1: 0.9631 Bal: 0.9308 - Val Loss: 0.2186, Accuracy: 0.9272, F1: 0.9585 Bal: 0.9026\n","           WALLETS: Train Loss: 0.38263571, Acc: 0.87365205, F1: 0.92810422 Bal: 0.8172 - Val Loss: 0.38225052, Accuracy: 0.8731, F1: 0.9277 Bal: 0.8131\n","Epoch 065:      TX: Train Loss: 0.1730, Acc: 0.9370, F1: 0.9640 Bal: 0.9327 - Val Loss: 0.2171, Accuracy: 0.9287, F1: 0.9594 Bal: 0.9054\n","           WALLETS: Train Loss: 0.38106287, Acc: 0.87401126, F1: 0.92831825 Bal: 0.8177 - Val Loss: 0.38070089, Accuracy: 0.8735, F1: 0.9280 Bal: 0.8145\n","Epoch 066:      TX: Train Loss: 0.1699, Acc: 0.9380, F1: 0.9646 Bal: 0.9343 - Val Loss: 0.2156, Accuracy: 0.9289, F1: 0.9595 Bal: 0.9055\n","           WALLETS: Train Loss: 0.37951317, Acc: 0.87396047, F1: 0.92827128 Bal: 0.8189 - Val Loss: 0.37916410, Accuracy: 0.8732, F1: 0.9278 Bal: 0.8151\n","Epoch 067:      TX: Train Loss: 0.1669, Acc: 0.9395, F1: 0.9655 Bal: 0.9357 - Val Loss: 0.2141, Accuracy: 0.9298, F1: 0.9600 Bal: 0.9051\n","           WALLETS: Train Loss: 0.37798592, Acc: 0.87415640, F1: 0.92838752 Bal: 0.8192 - Val Loss: 0.37767166, Accuracy: 0.8734, F1: 0.9278 Bal: 0.8158\n","Epoch 068:      TX: Train Loss: 0.1638, Acc: 0.9407, F1: 0.9662 Bal: 0.9374 - Val Loss: 0.2128, Accuracy: 0.9309, F1: 0.9607 Bal: 0.9057\n","           WALLETS: Train Loss: 0.37646809, Acc: 0.87478048, F1: 0.92876472 Bal: 0.8198 - Val Loss: 0.37621766, Accuracy: 0.8739, F1: 0.9282 Bal: 0.8163\n","Epoch 069:      TX: Train Loss: 0.1608, Acc: 0.9420, F1: 0.9670 Bal: 0.9385 - Val Loss: 0.2116, Accuracy: 0.9318, F1: 0.9612 Bal: 0.9061\n","           WALLETS: Train Loss: 0.37496307, Acc: 0.87554607, F1: 0.92922523 Bal: 0.8206 - Val Loss: 0.37479073, Accuracy: 0.8747, F1: 0.9287 Bal: 0.8162\n","Epoch 070:      TX: Train Loss: 0.1579, Acc: 0.9435, F1: 0.9678 Bal: 0.9399 - Val Loss: 0.2105, Accuracy: 0.9329, F1: 0.9618 Bal: 0.9068\n","           WALLETS: Train Loss: 0.37346101, Acc: 0.87581095, F1: 0.92938213 Bal: 0.8211 - Val Loss: 0.37335148, Accuracy: 0.8748, F1: 0.9287 Bal: 0.8167\n","Epoch 071:      TX: Train Loss: 0.1550, Acc: 0.9447, F1: 0.9686 Bal: 0.9415 - Val Loss: 0.2095, Accuracy: 0.9337, F1: 0.9624 Bal: 0.9072\n","           WALLETS: Train Loss: 0.37195840, Acc: 0.87533563, F1: 0.92907394 Bal: 0.8222 - Val Loss: 0.37189111, Accuracy: 0.8739, F1: 0.9282 Bal: 0.8172\n","Epoch 072:      TX: Train Loss: 0.1521, Acc: 0.9459, F1: 0.9692 Bal: 0.9425 - Val Loss: 0.2086, Accuracy: 0.9348, F1: 0.9630 Bal: 0.9079\n","           WALLETS: Train Loss: 0.37045398, Acc: 0.87538280, F1: 0.92908570 Bal: 0.8235 - Val Loss: 0.37044799, Accuracy: 0.8739, F1: 0.9281 Bal: 0.8187\n","Epoch 073:      TX: Train Loss: 0.1493, Acc: 0.9471, F1: 0.9699 Bal: 0.9443 - Val Loss: 0.2077, Accuracy: 0.9357, F1: 0.9635 Bal: 0.9083\n","           WALLETS: Train Loss: 0.36894017, Acc: 0.87571661, F1: 0.92927858 Bal: 0.8244 - Val Loss: 0.36902171, Accuracy: 0.8739, F1: 0.9281 Bal: 0.8194\n","Epoch 074:      TX: Train Loss: 0.1465, Acc: 0.9484, F1: 0.9707 Bal: 0.9462 - Val Loss: 0.2069, Accuracy: 0.9368, F1: 0.9642 Bal: 0.9089\n","           WALLETS: Train Loss: 0.36743271, Acc: 0.87584723, F1: 0.92934620 Bal: 0.8254 - Val Loss: 0.36760253, Accuracy: 0.8741, F1: 0.9283 Bal: 0.8203\n","Epoch 075:      TX: Train Loss: 0.1437, Acc: 0.9495, F1: 0.9713 Bal: 0.9475 - Val Loss: 0.2062, Accuracy: 0.9375, F1: 0.9646 Bal: 0.9083\n","           WALLETS: Train Loss: 0.36591682, Acc: 0.87554970, F1: 0.92915318 Bal: 0.8261 - Val Loss: 0.36616406, Accuracy: 0.8740, F1: 0.9281 Bal: 0.8220\n","Epoch 076:      TX: Train Loss: 0.1410, Acc: 0.9503, F1: 0.9718 Bal: 0.9490 - Val Loss: 0.2055, Accuracy: 0.9381, F1: 0.9649 Bal: 0.9087\n","           WALLETS: Train Loss: 0.36439189, Acc: 0.87507075, F1: 0.92884525 Bal: 0.8270 - Val Loss: 0.36472836, Accuracy: 0.8737, F1: 0.9279 Bal: 0.8240\n","Epoch 077:      TX: Train Loss: 0.1383, Acc: 0.9510, F1: 0.9722 Bal: 0.9500 - Val Loss: 0.2049, Accuracy: 0.9386, F1: 0.9652 Bal: 0.9089\n","           WALLETS: Train Loss: 0.36284715, Acc: 0.87522677, F1: 0.92892897 Bal: 0.8279 - Val Loss: 0.36330590, Accuracy: 0.8738, F1: 0.9280 Bal: 0.8249\n","Epoch 078:      TX: Train Loss: 0.1356, Acc: 0.9521, F1: 0.9728 Bal: 0.9511 - Val Loss: 0.2044, Accuracy: 0.9388, F1: 0.9653 Bal: 0.9091\n","           WALLETS: Train Loss: 0.36129037, Acc: 0.87550979, F1: 0.92909252 Bal: 0.8287 - Val Loss: 0.36190811, Accuracy: 0.8742, F1: 0.9282 Bal: 0.8260\n","Epoch 079:      TX: Train Loss: 0.1330, Acc: 0.9530, F1: 0.9733 Bal: 0.9520 - Val Loss: 0.2040, Accuracy: 0.9399, F1: 0.9660 Bal: 0.9116\n","           WALLETS: Train Loss: 0.35972828, Acc: 0.87572386, F1: 0.92920846 Bal: 0.8299 - Val Loss: 0.36050889, Accuracy: 0.8741, F1: 0.9281 Bal: 0.8269\n","Epoch 080:      TX: Train Loss: 0.1304, Acc: 0.9539, F1: 0.9739 Bal: 0.9532 - Val Loss: 0.2036, Accuracy: 0.9401, F1: 0.9661 Bal: 0.9118\n","           WALLETS: Train Loss: 0.35816249, Acc: 0.87547713, F1: 0.92904533 Bal: 0.8307 - Val Loss: 0.35910621, Accuracy: 0.8737, F1: 0.9279 Bal: 0.8272\n","Epoch 081:      TX: Train Loss: 0.1279, Acc: 0.9548, F1: 0.9744 Bal: 0.9544 - Val Loss: 0.2033, Accuracy: 0.9403, F1: 0.9662 Bal: 0.9119\n","           WALLETS: Train Loss: 0.35659871, Acc: 0.87562953, F1: 0.92912718 Bal: 0.8316 - Val Loss: 0.35772964, Accuracy: 0.8735, F1: 0.9277 Bal: 0.8272\n","Epoch 082:      TX: Train Loss: 0.1254, Acc: 0.9558, F1: 0.9750 Bal: 0.9557 - Val Loss: 0.2032, Accuracy: 0.9410, F1: 0.9666 Bal: 0.9113\n","           WALLETS: Train Loss: 0.35504612, Acc: 0.87597422, F1: 0.92933369 Bal: 0.8320 - Val Loss: 0.35638937, Accuracy: 0.8740, F1: 0.9281 Bal: 0.8281\n","Epoch 083:      TX: Train Loss: 0.1229, Acc: 0.9564, F1: 0.9753 Bal: 0.9563 - Val Loss: 0.2032, Accuracy: 0.9423, F1: 0.9674 Bal: 0.9120\n","           WALLETS: Train Loss: 0.35350433, Acc: 0.87597785, F1: 0.92932451 Bal: 0.8329 - Val Loss: 0.35504720, Accuracy: 0.8743, F1: 0.9282 Bal: 0.8286\n","Epoch 084:      TX: Train Loss: 0.1205, Acc: 0.9574, F1: 0.9759 Bal: 0.9577 - Val Loss: 0.2031, Accuracy: 0.9427, F1: 0.9676 Bal: 0.9113\n","           WALLETS: Train Loss: 0.35197952, Acc: 0.87553882, F1: 0.92903485 Bal: 0.8342 - Val Loss: 0.35368836, Accuracy: 0.8741, F1: 0.9281 Bal: 0.8297\n","Epoch 085:      TX: Train Loss: 0.1181, Acc: 0.9582, F1: 0.9763 Bal: 0.9587 - Val Loss: 0.2031, Accuracy: 0.9436, F1: 0.9681 Bal: 0.9117\n","           WALLETS: Train Loss: 0.35046124, Acc: 0.87490385, F1: 0.92861831 Bal: 0.8360 - Val Loss: 0.35234743, Accuracy: 0.8736, F1: 0.9278 Bal: 0.8318\n","Epoch 086:      TX: Train Loss: 0.1157, Acc: 0.9589, F1: 0.9767 Bal: 0.9596 - Val Loss: 0.2032, Accuracy: 0.9438, F1: 0.9683 Bal: 0.9128\n","           WALLETS: Train Loss: 0.34894627, Acc: 0.87484942, F1: 0.92856669 Bal: 0.8373 - Val Loss: 0.35103473, Accuracy: 0.8740, F1: 0.9280 Bal: 0.8344\n","Epoch 087:      TX: Train Loss: 0.1134, Acc: 0.9597, F1: 0.9772 Bal: 0.9604 - Val Loss: 0.2032, Accuracy: 0.9443, F1: 0.9685 Bal: 0.9131\n","           WALLETS: Train Loss: 0.34744123, Acc: 0.87491836, F1: 0.92858489 Bal: 0.8390 - Val Loss: 0.34972483, Accuracy: 0.8743, F1: 0.9281 Bal: 0.8366\n","Epoch 088:      TX: Train Loss: 0.1112, Acc: 0.9606, F1: 0.9777 Bal: 0.9621 - Val Loss: 0.2033, Accuracy: 0.9452, F1: 0.9690 Bal: 0.9136\n","           WALLETS: Train Loss: 0.34593540, Acc: 0.87419994, F1: 0.92812200 Bal: 0.8403 - Val Loss: 0.34839818, Accuracy: 0.8739, F1: 0.9279 Bal: 0.8383\n","Epoch 089:      TX: Train Loss: 0.1090, Acc: 0.9619, F1: 0.9784 Bal: 0.9648 - Val Loss: 0.2036, Accuracy: 0.9462, F1: 0.9697 Bal: 0.9142\n","           WALLETS: Train Loss: 0.34443024, Acc: 0.87375727, F1: 0.92783467 Bal: 0.8412 - Val Loss: 0.34710002, Accuracy: 0.8735, F1: 0.9276 Bal: 0.8403\n","Epoch 090:      TX: Train Loss: 0.1068, Acc: 0.9628, F1: 0.9790 Bal: 0.9659 - Val Loss: 0.2040, Accuracy: 0.9458, F1: 0.9694 Bal: 0.9110\n","           WALLETS: Train Loss: 0.34293121, Acc: 0.87381533, F1: 0.92786426 Bal: 0.8417 - Val Loss: 0.34584153, Accuracy: 0.8733, F1: 0.9274 Bal: 0.8408\n","Epoch 091:      TX: Train Loss: 0.1047, Acc: 0.9637, F1: 0.9795 Bal: 0.9673 - Val Loss: 0.2043, Accuracy: 0.9458, F1: 0.9694 Bal: 0.9110\n","           WALLETS: Train Loss: 0.34144729, Acc: 0.87399312, F1: 0.92796037 Bal: 0.8427 - Val Loss: 0.34458056, Accuracy: 0.8735, F1: 0.9276 Bal: 0.8415\n","Epoch 092:      TX: Train Loss: 0.1026, Acc: 0.9643, F1: 0.9798 Bal: 0.9683 - Val Loss: 0.2047, Accuracy: 0.9456, F1: 0.9693 Bal: 0.9109\n","           WALLETS: Train Loss: 0.33997086, Acc: 0.87410560, F1: 0.92801915 Bal: 0.8435 - Val Loss: 0.34330916, Accuracy: 0.8734, F1: 0.9275 Bal: 0.8419\n","Epoch 093:      TX: Train Loss: 0.1006, Acc: 0.9649, F1: 0.9802 Bal: 0.9689 - Val Loss: 0.2054, Accuracy: 0.9460, F1: 0.9696 Bal: 0.9101\n","           WALLETS: Train Loss: 0.33850220, Acc: 0.87468614, F1: 0.92836267 Bal: 0.8447 - Val Loss: 0.34207517, Accuracy: 0.8738, F1: 0.9277 Bal: 0.8425\n","Epoch 094:      TX: Train Loss: 0.0986, Acc: 0.9654, F1: 0.9805 Bal: 0.9695 - Val Loss: 0.2061, Accuracy: 0.9462, F1: 0.9697 Bal: 0.9093\n","           WALLETS: Train Loss: 0.33704656, Acc: 0.87509978, F1: 0.92860476 Bal: 0.8457 - Val Loss: 0.34085238, Accuracy: 0.8742, F1: 0.9280 Bal: 0.8420\n","Epoch 095:      TX: Train Loss: 0.0967, Acc: 0.9659, F1: 0.9807 Bal: 0.9700 - Val Loss: 0.2068, Accuracy: 0.9467, F1: 0.9700 Bal: 0.9095\n","           WALLETS: Train Loss: 0.33560786, Acc: 0.87491110, F1: 0.92847466 Bal: 0.8467 - Val Loss: 0.33961132, Accuracy: 0.8738, F1: 0.9277 Bal: 0.8426\n","Epoch 096:      TX: Train Loss: 0.0948, Acc: 0.9663, F1: 0.9810 Bal: 0.9707 - Val Loss: 0.2076, Accuracy: 0.9478, F1: 0.9706 Bal: 0.9111\n","           WALLETS: Train Loss: 0.33418426, Acc: 0.87508164, F1: 0.92857024 Bal: 0.8474 - Val Loss: 0.33839676, Accuracy: 0.8734, F1: 0.9275 Bal: 0.8424\n","Epoch 097:      TX: Train Loss: 0.0929, Acc: 0.9669, F1: 0.9813 Bal: 0.9711 - Val Loss: 0.2087, Accuracy: 0.9484, F1: 0.9710 Bal: 0.9115\n","           WALLETS: Train Loss: 0.33277473, Acc: 0.87597785, F1: 0.92911550 Bal: 0.8482 - Val Loss: 0.33721364, Accuracy: 0.8744, F1: 0.9281 Bal: 0.8431\n","Epoch 098:      TX: Train Loss: 0.0911, Acc: 0.9675, F1: 0.9817 Bal: 0.9721 - Val Loss: 0.2097, Accuracy: 0.9487, F1: 0.9711 Bal: 0.9116\n","           WALLETS: Train Loss: 0.33137295, Acc: 0.87605405, F1: 0.92915097 Bal: 0.8490 - Val Loss: 0.33602270, Accuracy: 0.8740, F1: 0.9278 Bal: 0.8435\n","Epoch 099:      TX: Train Loss: 0.0894, Acc: 0.9679, F1: 0.9819 Bal: 0.9728 - Val Loss: 0.2106, Accuracy: 0.9491, F1: 0.9714 Bal: 0.9109\n","           WALLETS: Train Loss: 0.32999355, Acc: 0.87599237, F1: 0.92909969 Bal: 0.8500 - Val Loss: 0.33485237, Accuracy: 0.8741, F1: 0.9279 Bal: 0.8440\n","Epoch 100:      TX: Train Loss: 0.0876, Acc: 0.9682, F1: 0.9821 Bal: 0.9734 - Val Loss: 0.2118, Accuracy: 0.9504, F1: 0.9721 Bal: 0.9116\n","           WALLETS: Train Loss: 0.32862833, Acc: 0.87684141, F1: 0.92961082 Bal: 0.8511 - Val Loss: 0.33372614, Accuracy: 0.8749, F1: 0.9284 Bal: 0.8454\n","Epoch 101:      TX: Train Loss: 0.0859, Acc: 0.9688, F1: 0.9824 Bal: 0.9737 - Val Loss: 0.2131, Accuracy: 0.9511, F1: 0.9725 Bal: 0.9120\n","           WALLETS: Train Loss: 0.32727408, Acc: 0.87761063, F1: 0.93006873 Bal: 0.8524 - Val Loss: 0.33258158, Accuracy: 0.8760, F1: 0.9290 Bal: 0.8465\n","Epoch 102:      TX: Train Loss: 0.0843, Acc: 0.9692, F1: 0.9826 Bal: 0.9739 - Val Loss: 0.2143, Accuracy: 0.9522, F1: 0.9731 Bal: 0.9126\n","           WALLETS: Train Loss: 0.32593158, Acc: 0.87765417, F1: 0.93008027 Bal: 0.8535 - Val Loss: 0.33143491, Accuracy: 0.8760, F1: 0.9291 Bal: 0.8473\n","Epoch 103:      TX: Train Loss: 0.0827, Acc: 0.9701, F1: 0.9831 Bal: 0.9748 - Val Loss: 0.2156, Accuracy: 0.9528, F1: 0.9735 Bal: 0.9129\n","           WALLETS: Train Loss: 0.32459679, Acc: 0.87813675, F1: 0.93037006 Bal: 0.8542 - Val Loss: 0.33033070, Accuracy: 0.8765, F1: 0.9293 Bal: 0.8482\n","Epoch 104:      TX: Train Loss: 0.0811, Acc: 0.9707, F1: 0.9835 Bal: 0.9755 - Val Loss: 0.2170, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9131\n","           WALLETS: Train Loss: 0.32327411, Acc: 0.87814400, F1: 0.93036237 Bal: 0.8551 - Val Loss: 0.32920590, Accuracy: 0.8767, F1: 0.9294 Bal: 0.8500\n","Epoch 105:      TX: Train Loss: 0.0796, Acc: 0.9713, F1: 0.9838 Bal: 0.9763 - Val Loss: 0.2185, Accuracy: 0.9533, F1: 0.9738 Bal: 0.9112\n","           WALLETS: Train Loss: 0.32196736, Acc: 0.87844153, F1: 0.93053298 Bal: 0.8561 - Val Loss: 0.32811216, Accuracy: 0.8772, F1: 0.9297 Bal: 0.8512\n","Epoch 106:      TX: Train Loss: 0.0781, Acc: 0.9719, F1: 0.9842 Bal: 0.9768 - Val Loss: 0.2198, Accuracy: 0.9537, F1: 0.9740 Bal: 0.9115\n","           WALLETS: Train Loss: 0.32067516, Acc: 0.87901482, F1: 0.93087550 Bal: 0.8570 - Val Loss: 0.32706463, Accuracy: 0.8779, F1: 0.9301 Bal: 0.8525\n","Epoch 107:      TX: Train Loss: 0.0766, Acc: 0.9723, F1: 0.9844 Bal: 0.9772 - Val Loss: 0.2212, Accuracy: 0.9539, F1: 0.9742 Bal: 0.9116\n","           WALLETS: Train Loss: 0.31940064, Acc: 0.87909101, F1: 0.93091202 Bal: 0.8578 - Val Loss: 0.32600409, Accuracy: 0.8778, F1: 0.9301 Bal: 0.8535\n","Epoch 108:      TX: Train Loss: 0.0752, Acc: 0.9731, F1: 0.9849 Bal: 0.9778 - Val Loss: 0.2227, Accuracy: 0.9541, F1: 0.9743 Bal: 0.9127\n","           WALLETS: Train Loss: 0.31815276, Acc: 0.87975138, F1: 0.93131214 Bal: 0.8584 - Val Loss: 0.32501912, Accuracy: 0.8784, F1: 0.9305 Bal: 0.8536\n","Epoch 109:      TX: Train Loss: 0.0738, Acc: 0.9738, F1: 0.9852 Bal: 0.9784 - Val Loss: 0.2242, Accuracy: 0.9548, F1: 0.9747 Bal: 0.9130\n","           WALLETS: Train Loss: 0.31692904, Acc: 0.88001626, F1: 0.93146159 Bal: 0.8594 - Val Loss: 0.32403746, Accuracy: 0.8787, F1: 0.9306 Bal: 0.8544\n","Epoch 110:      TX: Train Loss: 0.0724, Acc: 0.9743, F1: 0.9855 Bal: 0.9789 - Val Loss: 0.2256, Accuracy: 0.9546, F1: 0.9745 Bal: 0.9129\n","           WALLETS: Train Loss: 0.31572625, Acc: 0.88020856, F1: 0.93156705 Bal: 0.8604 - Val Loss: 0.32306808, Accuracy: 0.8792, F1: 0.9309 Bal: 0.8550\n","Epoch 111:      TX: Train Loss: 0.0710, Acc: 0.9747, F1: 0.9858 Bal: 0.9792 - Val Loss: 0.2271, Accuracy: 0.9548, F1: 0.9747 Bal: 0.9130\n","           WALLETS: Train Loss: 0.31454289, Acc: 0.88065848, F1: 0.93184074 Bal: 0.8607 - Val Loss: 0.32214981, Accuracy: 0.8798, F1: 0.9313 Bal: 0.8553\n","Epoch 112:      TX: Train Loss: 0.0697, Acc: 0.9750, F1: 0.9860 Bal: 0.9794 - Val Loss: 0.2287, Accuracy: 0.9559, F1: 0.9753 Bal: 0.9137\n","           WALLETS: Train Loss: 0.31338358, Acc: 0.88089070, F1: 0.93196660 Bal: 0.8621 - Val Loss: 0.32121983, Accuracy: 0.8797, F1: 0.9312 Bal: 0.8563\n","Epoch 113:      TX: Train Loss: 0.0684, Acc: 0.9755, F1: 0.9863 Bal: 0.9796 - Val Loss: 0.2300, Accuracy: 0.9563, F1: 0.9755 Bal: 0.9129\n","           WALLETS: Train Loss: 0.31223825, Acc: 0.88134062, F1: 0.93223763 Bal: 0.8626 - Val Loss: 0.32035646, Accuracy: 0.8801, F1: 0.9314 Bal: 0.8567\n","Epoch 114:      TX: Train Loss: 0.0672, Acc: 0.9758, F1: 0.9864 Bal: 0.9801 - Val Loss: 0.2315, Accuracy: 0.9563, F1: 0.9756 Bal: 0.9119\n","           WALLETS: Train Loss: 0.31110674, Acc: 0.88122088, F1: 0.93214971 Bal: 0.8636 - Val Loss: 0.31946915, Accuracy: 0.8799, F1: 0.9313 Bal: 0.8571\n","Epoch 115:      TX: Train Loss: 0.0659, Acc: 0.9765, F1: 0.9868 Bal: 0.9809 - Val Loss: 0.2332, Accuracy: 0.9563, F1: 0.9756 Bal: 0.9119\n","           WALLETS: Train Loss: 0.30998227, Acc: 0.88182320, F1: 0.93250916 Bal: 0.8646 - Val Loss: 0.31865302, Accuracy: 0.8805, F1: 0.9317 Bal: 0.8581\n","Epoch 116:      TX: Train Loss: 0.0647, Acc: 0.9770, F1: 0.9871 Bal: 0.9815 - Val Loss: 0.2348, Accuracy: 0.9563, F1: 0.9756 Bal: 0.9119\n","           WALLETS: Train Loss: 0.30888209, Acc: 0.88153655, F1: 0.93231586 Bal: 0.8658 - Val Loss: 0.31777817, Accuracy: 0.8802, F1: 0.9315 Bal: 0.8585\n","Epoch 117:      TX: Train Loss: 0.0635, Acc: 0.9774, F1: 0.9873 Bal: 0.9824 - Val Loss: 0.2362, Accuracy: 0.9561, F1: 0.9754 Bal: 0.9118\n","           WALLETS: Train Loss: 0.30781519, Acc: 0.88249082, F1: 0.93288849 Bal: 0.8671 - Val Loss: 0.31704143, Accuracy: 0.8814, F1: 0.9322 Bal: 0.8606\n","Epoch 118:      TX: Train Loss: 0.0624, Acc: 0.9780, F1: 0.9876 Bal: 0.9831 - Val Loss: 0.2379, Accuracy: 0.9570, F1: 0.9759 Bal: 0.9123\n","           WALLETS: Train Loss: 0.30677411, Acc: 0.88189939, F1: 0.93250498 Bal: 0.8685 - Val Loss: 0.31617048, Accuracy: 0.8806, F1: 0.9317 Bal: 0.8614\n","Epoch 119:      TX: Train Loss: 0.0612, Acc: 0.9786, F1: 0.9880 Bal: 0.9838 - Val Loss: 0.2398, Accuracy: 0.9572, F1: 0.9761 Bal: 0.9124\n","           WALLETS: Train Loss: 0.30574816, Acc: 0.88394581, F1: 0.93376763 Bal: 0.8684 - Val Loss: 0.31559432, Accuracy: 0.8830, F1: 0.9331 Bal: 0.8617\n","Epoch 120:      TX: Train Loss: 0.0601, Acc: 0.9790, F1: 0.9882 Bal: 0.9843 - Val Loss: 0.2411, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9118\n","           WALLETS: Train Loss: 0.30475101, Acc: 0.88106486, F1: 0.93197049 Bal: 0.8700 - Val Loss: 0.31455451, Accuracy: 0.8794, F1: 0.9310 Bal: 0.8627\n","Epoch 121:      TX: Train Loss: 0.0591, Acc: 0.9794, F1: 0.9884 Bal: 0.9845 - Val Loss: 0.2427, Accuracy: 0.9590, F1: 0.9771 Bal: 0.9124\n","           WALLETS: Train Loss: 0.30383113, Acc: 0.88640949, F1: 0.93528021 Bal: 0.8685 - Val Loss: 0.31447461, Accuracy: 0.8859, F1: 0.9350 Bal: 0.8623\n","Epoch 122:      TX: Train Loss: 0.0580, Acc: 0.9798, F1: 0.9887 Bal: 0.9849 - Val Loss: 0.2446, Accuracy: 0.9592, F1: 0.9772 Bal: 0.9125\n","           WALLETS: Train Loss: 0.30303368, Acc: 0.87799524, F1: 0.93003726 Bal: 0.8723 - Val Loss: 0.31303808, Accuracy: 0.8759, F1: 0.9287 Bal: 0.8639\n","Epoch 123:      TX: Train Loss: 0.0570, Acc: 0.9801, F1: 0.9888 Bal: 0.9850 - Val Loss: 0.2464, Accuracy: 0.9599, F1: 0.9776 Bal: 0.9129\n","           WALLETS: Train Loss: 0.30221498, Acc: 0.88961336, F1: 0.93723684 Bal: 0.8688 - Val Loss: 0.31373820, Accuracy: 0.8887, F1: 0.9367 Bal: 0.8614\n","Epoch 124:      TX: Train Loss: 0.0560, Acc: 0.9804, F1: 0.9890 Bal: 0.9853 - Val Loss: 0.2479, Accuracy: 0.9605, F1: 0.9779 Bal: 0.9142\n","           WALLETS: Train Loss: 0.30108449, Acc: 0.87868826, F1: 0.93045681 Bal: 0.8731 - Val Loss: 0.31167504, Accuracy: 0.8765, F1: 0.9291 Bal: 0.8645\n","Epoch 125:      TX: Train Loss: 0.0550, Acc: 0.9810, F1: 0.9894 Bal: 0.9862 - Val Loss: 0.2502, Accuracy: 0.9609, F1: 0.9782 Bal: 0.9145\n","           WALLETS: Train Loss: 0.29987526, Acc: 0.88476945, F1: 0.93423047 Bal: 0.8718 - Val Loss: 0.31141403, Accuracy: 0.8837, F1: 0.9336 Bal: 0.8644\n","Epoch 126:      TX: Train Loss: 0.0540, Acc: 0.9812, F1: 0.9895 Bal: 0.9866 - Val Loss: 0.2517, Accuracy: 0.9612, F1: 0.9783 Bal: 0.9146\n","           WALLETS: Train Loss: 0.29905888, Acc: 0.88701180, F1: 0.93560741 Bal: 0.8719 - Val Loss: 0.31113499, Accuracy: 0.8857, F1: 0.9348 Bal: 0.8634\n","Epoch 127:      TX: Train Loss: 0.0531, Acc: 0.9816, F1: 0.9897 Bal: 0.9869 - Val Loss: 0.2534, Accuracy: 0.9612, F1: 0.9783 Bal: 0.9146\n","           WALLETS: Train Loss: 0.29836625, Acc: 0.87906562, F1: 0.93067400 Bal: 0.8744 - Val Loss: 0.30982780, Accuracy: 0.8772, F1: 0.9295 Bal: 0.8666\n","Epoch 128:      TX: Train Loss: 0.0522, Acc: 0.9821, F1: 0.9900 Bal: 0.9872 - Val Loss: 0.2560, Accuracy: 0.9612, F1: 0.9783 Bal: 0.9136\n","           WALLETS: Train Loss: 0.29735312, Acc: 0.88801324, F1: 0.93621796 Bal: 0.8721 - Val Loss: 0.31020758, Accuracy: 0.8866, F1: 0.9353 Bal: 0.8634\n","Epoch 129:      TX: Train Loss: 0.0512, Acc: 0.9822, F1: 0.9900 Bal: 0.9875 - Val Loss: 0.2571, Accuracy: 0.9614, F1: 0.9784 Bal: 0.9138\n","           WALLETS: Train Loss: 0.29628024, Acc: 0.88381156, F1: 0.93361197 Bal: 0.8741 - Val Loss: 0.30894122, Accuracy: 0.8827, F1: 0.9329 Bal: 0.8655\n","Epoch 130:      TX: Train Loss: 0.0504, Acc: 0.9826, F1: 0.9903 Bal: 0.9876 - Val Loss: 0.2591, Accuracy: 0.9618, F1: 0.9787 Bal: 0.9140\n","           WALLETS: Train Loss: 0.29551047, Acc: 0.88172523, F1: 0.93231056 Bal: 0.8751 - Val Loss: 0.30824018, Accuracy: 0.8803, F1: 0.9314 Bal: 0.8663\n","Epoch 131:      TX: Train Loss: 0.0495, Acc: 0.9829, F1: 0.9904 Bal: 0.9878 - Val Loss: 0.2616, Accuracy: 0.9618, F1: 0.9787 Bal: 0.9150\n","           WALLETS: Train Loss: 0.29478288, Acc: 0.88913804, F1: 0.93689042 Bal: 0.8734 - Val Loss: 0.30874056, Accuracy: 0.8877, F1: 0.9360 Bal: 0.8648\n","Epoch 132:      TX: Train Loss: 0.0486, Acc: 0.9830, F1: 0.9905 Bal: 0.9880 - Val Loss: 0.2626, Accuracy: 0.9616, F1: 0.9786 Bal: 0.9149\n","           WALLETS: Train Loss: 0.29382008, Acc: 0.88202276, F1: 0.93248336 Bal: 0.8760 - Val Loss: 0.30719471, Accuracy: 0.8803, F1: 0.9314 Bal: 0.8672\n","Epoch 133:      TX: Train Loss: 0.0478, Acc: 0.9832, F1: 0.9906 Bal: 0.9881 - Val Loss: 0.2653, Accuracy: 0.9625, F1: 0.9791 Bal: 0.9153\n","           WALLETS: Train Loss: 0.29286131, Acc: 0.88543345, F1: 0.93459442 Bal: 0.8753 - Val Loss: 0.30697501, Accuracy: 0.8840, F1: 0.9337 Bal: 0.8653\n","Epoch 134:      TX: Train Loss: 0.0470, Acc: 0.9835, F1: 0.9907 Bal: 0.9883 - Val Loss: 0.2674, Accuracy: 0.9623, F1: 0.9789 Bal: 0.9142\n","           WALLETS: Train Loss: 0.29212135, Acc: 0.88814749, F1: 0.93626505 Bal: 0.8750 - Val Loss: 0.30687791, Accuracy: 0.8868, F1: 0.9354 Bal: 0.8654\n","Epoch 135:      TX: Train Loss: 0.0462, Acc: 0.9838, F1: 0.9909 Bal: 0.9888 - Val Loss: 0.2689, Accuracy: 0.9625, F1: 0.9791 Bal: 0.9153\n","           WALLETS: Train Loss: 0.29138872, Acc: 0.88204453, F1: 0.93247928 Bal: 0.8773 - Val Loss: 0.30564734, Accuracy: 0.8800, F1: 0.9312 Bal: 0.8689\n","Epoch 136:      TX: Train Loss: 0.0454, Acc: 0.9843, F1: 0.9912 Bal: 0.9892 - Val Loss: 0.2718, Accuracy: 0.9634, F1: 0.9796 Bal: 0.9149\n","           WALLETS: Train Loss: 0.29048496, Acc: 0.88866635, F1: 0.93657238 Bal: 0.8759 - Val Loss: 0.30589056, Accuracy: 0.8872, F1: 0.9357 Bal: 0.8656\n","Epoch 137:      TX: Train Loss: 0.0446, Acc: 0.9844, F1: 0.9913 Bal: 0.9894 - Val Loss: 0.2735, Accuracy: 0.9636, F1: 0.9797 Bal: 0.9160\n","           WALLETS: Train Loss: 0.28957960, Acc: 0.88601036, F1: 0.93492931 Bal: 0.8769 - Val Loss: 0.30490318, Accuracy: 0.8844, F1: 0.9340 Bal: 0.8674\n","Epoch 138:      TX: Train Loss: 0.0438, Acc: 0.9848, F1: 0.9915 Bal: 0.9897 - Val Loss: 0.2753, Accuracy: 0.9636, F1: 0.9797 Bal: 0.9160\n","           WALLETS: Train Loss: 0.28881904, Acc: 0.88463157, F1: 0.93406985 Bal: 0.8778 - Val Loss: 0.30422413, Accuracy: 0.8830, F1: 0.9331 Bal: 0.8690\n","Epoch 139:      TX: Train Loss: 0.0431, Acc: 0.9849, F1: 0.9916 Bal: 0.9898 - Val Loss: 0.2781, Accuracy: 0.9640, F1: 0.9800 Bal: 0.9142\n","           WALLETS: Train Loss: 0.28808638, Acc: 0.88986009, F1: 0.93729200 Bal: 0.8768 - Val Loss: 0.30450088, Accuracy: 0.8885, F1: 0.9365 Bal: 0.8669\n","Epoch 140:      TX: Train Loss: 0.0424, Acc: 0.9851, F1: 0.9917 Bal: 0.9902 - Val Loss: 0.2792, Accuracy: 0.9640, F1: 0.9799 Bal: 0.9152\n","           WALLETS: Train Loss: 0.28725585, Acc: 0.88426873, F1: 0.93383781 Bal: 0.8784 - Val Loss: 0.30317742, Accuracy: 0.8827, F1: 0.9329 Bal: 0.8690\n","Epoch 141:      TX: Train Loss: 0.0417, Acc: 0.9855, F1: 0.9919 Bal: 0.9904 - Val Loss: 0.2817, Accuracy: 0.9649, F1: 0.9804 Bal: 0.9157\n","           WALLETS: Train Loss: 0.28637722, Acc: 0.88870989, F1: 0.93657332 Bal: 0.8780 - Val Loss: 0.30317864, Accuracy: 0.8872, F1: 0.9357 Bal: 0.8686\n","Epoch 142:      TX: Train Loss: 0.0409, Acc: 0.9860, F1: 0.9922 Bal: 0.9907 - Val Loss: 0.2836, Accuracy: 0.9649, F1: 0.9804 Bal: 0.9157\n","           WALLETS: Train Loss: 0.28556803, Acc: 0.88843050, F1: 0.93639607 Bal: 0.8784 - Val Loss: 0.30259198, Accuracy: 0.8868, F1: 0.9354 Bal: 0.8685\n","Epoch 143:      TX: Train Loss: 0.0403, Acc: 0.9861, F1: 0.9922 Bal: 0.9909 - Val Loss: 0.2852, Accuracy: 0.9649, F1: 0.9804 Bal: 0.9157\n","           WALLETS: Train Loss: 0.28483367, Acc: 0.88582531, F1: 0.93478842 Bal: 0.8791 - Val Loss: 0.30179507, Accuracy: 0.8839, F1: 0.9336 Bal: 0.8692\n","Epoch 144:      TX: Train Loss: 0.0396, Acc: 0.9865, F1: 0.9924 Bal: 0.9911 - Val Loss: 0.2879, Accuracy: 0.9649, F1: 0.9804 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28409705, Acc: 0.89127516, F1: 0.93813218 Bal: 0.8788 - Val Loss: 0.30210021, Accuracy: 0.8892, F1: 0.9369 Bal: 0.8686\n","Epoch 145:      TX: Train Loss: 0.0389, Acc: 0.9865, F1: 0.9925 Bal: 0.9911 - Val Loss: 0.2893, Accuracy: 0.9649, F1: 0.9804 Bal: 0.9167\n","           WALLETS: Train Loss: 0.28330740, Acc: 0.88588700, F1: 0.93481217 Bal: 0.8802 - Val Loss: 0.30086038, Accuracy: 0.8837, F1: 0.9335 Bal: 0.8695\n","Epoch 146:      TX: Train Loss: 0.0383, Acc: 0.9868, F1: 0.9926 Bal: 0.9915 - Val Loss: 0.2917, Accuracy: 0.9656, F1: 0.9808 Bal: 0.9171\n","           WALLETS: Train Loss: 0.28248379, Acc: 0.89097038, F1: 0.93793491 Bal: 0.8797 - Val Loss: 0.30103654, Accuracy: 0.8892, F1: 0.9369 Bal: 0.8698\n","Epoch 147:      TX: Train Loss: 0.0376, Acc: 0.9871, F1: 0.9928 Bal: 0.9918 - Val Loss: 0.2936, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9173\n","           WALLETS: Train Loss: 0.28167450, Acc: 0.88903282, F1: 0.93673473 Bal: 0.8809 - Val Loss: 0.30021459, Accuracy: 0.8865, F1: 0.9352 Bal: 0.8703\n","Epoch 148:      TX: Train Loss: 0.0370, Acc: 0.9874, F1: 0.9929 Bal: 0.9921 - Val Loss: 0.2955, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9175\n","           WALLETS: Train Loss: 0.28090981, Acc: 0.88933397, F1: 0.93691387 Bal: 0.8814 - Val Loss: 0.29979482, Accuracy: 0.8871, F1: 0.9356 Bal: 0.8712\n","Epoch 149:      TX: Train Loss: 0.0364, Acc: 0.9877, F1: 0.9931 Bal: 0.9924 - Val Loss: 0.2974, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9173\n","           WALLETS: Train Loss: 0.28016871, Acc: 0.89195004, F1: 0.93851494 Bal: 0.8813 - Val Loss: 0.29977563, Accuracy: 0.8901, F1: 0.9374 Bal: 0.8710\n","Epoch 150:      TX: Train Loss: 0.0358, Acc: 0.9878, F1: 0.9932 Bal: 0.9925 - Val Loss: 0.2990, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9173\n","           WALLETS: Train Loss: 0.27942127, Acc: 0.88799872, F1: 0.93607972 Bal: 0.8826 - Val Loss: 0.29875907, Accuracy: 0.8856, F1: 0.9346 Bal: 0.8714\n","Epoch 151:      TX: Train Loss: 0.0352, Acc: 0.9882, F1: 0.9934 Bal: 0.9927 - Val Loss: 0.3015, Accuracy: 0.9658, F1: 0.9809 Bal: 0.9162\n","           WALLETS: Train Loss: 0.27869952, Acc: 0.89404000, F1: 0.93977981 Bal: 0.8821 - Val Loss: 0.29922143, Accuracy: 0.8921, F1: 0.9386 Bal: 0.8708\n","Epoch 152:      TX: Train Loss: 0.0346, Acc: 0.9882, F1: 0.9934 Bal: 0.9927 - Val Loss: 0.3027, Accuracy: 0.9658, F1: 0.9809 Bal: 0.9162\n","           WALLETS: Train Loss: 0.27799752, Acc: 0.88764314, F1: 0.93584569 Bal: 0.8839 - Val Loss: 0.29785299, Accuracy: 0.8848, F1: 0.9341 Bal: 0.8726\n","Epoch 153:      TX: Train Loss: 0.0341, Acc: 0.9885, F1: 0.9936 Bal: 0.9928 - Val Loss: 0.3053, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9163\n","           WALLETS: Train Loss: 0.27730444, Acc: 0.89523737, F1: 0.94050349 Bal: 0.8825 - Val Loss: 0.29864815, Accuracy: 0.8932, F1: 0.9393 Bal: 0.8703\n","Epoch 154:      TX: Train Loss: 0.0335, Acc: 0.9885, F1: 0.9936 Bal: 0.9928 - Val Loss: 0.3064, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9163\n","           WALLETS: Train Loss: 0.27660745, Acc: 0.88767942, F1: 0.93585711 Bal: 0.8848 - Val Loss: 0.29708281, Accuracy: 0.8845, F1: 0.9339 Bal: 0.8731\n","Epoch 155:      TX: Train Loss: 0.0330, Acc: 0.9888, F1: 0.9938 Bal: 0.9930 - Val Loss: 0.3093, Accuracy: 0.9662, F1: 0.9812 Bal: 0.9164\n","           WALLETS: Train Loss: 0.27589869, Acc: 0.89595216, F1: 0.94093244 Bal: 0.8830 - Val Loss: 0.29796273, Accuracy: 0.8935, F1: 0.9395 Bal: 0.8700\n","Epoch 156:      TX: Train Loss: 0.0324, Acc: 0.9889, F1: 0.9938 Bal: 0.9931 - Val Loss: 0.3103, Accuracy: 0.9662, F1: 0.9812 Bal: 0.9164\n","           WALLETS: Train Loss: 0.27518123, Acc: 0.88836882, F1: 0.93627378 Bal: 0.8853 - Val Loss: 0.29632536, Accuracy: 0.8859, F1: 0.9348 Bal: 0.8742\n","Epoch 157:      TX: Train Loss: 0.0319, Acc: 0.9892, F1: 0.9940 Bal: 0.9932 - Val Loss: 0.3132, Accuracy: 0.9662, F1: 0.9812 Bal: 0.9164\n","           WALLETS: Train Loss: 0.27445966, Acc: 0.89627146, F1: 0.94111613 Bal: 0.8839 - Val Loss: 0.29710251, Accuracy: 0.8934, F1: 0.9394 Bal: 0.8710\n","Epoch 158:      TX: Train Loss: 0.0314, Acc: 0.9894, F1: 0.9941 Bal: 0.9935 - Val Loss: 0.3144, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9163\n","           WALLETS: Train Loss: 0.27374369, Acc: 0.88970044, F1: 0.93708465 Bal: 0.8858 - Val Loss: 0.29560387, Accuracy: 0.8872, F1: 0.9356 Bal: 0.8741\n","Epoch 159:      TX: Train Loss: 0.0309, Acc: 0.9896, F1: 0.9942 Bal: 0.9935 - Val Loss: 0.3172, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9156\n","           WALLETS: Train Loss: 0.27303427, Acc: 0.89616261, F1: 0.94103977 Bal: 0.8848 - Val Loss: 0.29615760, Accuracy: 0.8927, F1: 0.9390 Bal: 0.8716\n","Epoch 160:      TX: Train Loss: 0.0304, Acc: 0.9896, F1: 0.9942 Bal: 0.9937 - Val Loss: 0.3188, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9156\n","           WALLETS: Train Loss: 0.27233535, Acc: 0.89133685, F1: 0.93808073 Bal: 0.8863 - Val Loss: 0.29493657, Accuracy: 0.8885, F1: 0.9364 Bal: 0.8739\n","Epoch 161:      TX: Train Loss: 0.0299, Acc: 0.9898, F1: 0.9943 Bal: 0.9939 - Val Loss: 0.3207, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9156\n","           WALLETS: Train Loss: 0.27164862, Acc: 0.89588685, F1: 0.94086174 Bal: 0.8857 - Val Loss: 0.29521564, Accuracy: 0.8923, F1: 0.9387 Bal: 0.8723\n","Epoch 162:      TX: Train Loss: 0.0294, Acc: 0.9898, F1: 0.9943 Bal: 0.9939 - Val Loss: 0.3227, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9156\n","           WALLETS: Train Loss: 0.27097270, Acc: 0.89260316, F1: 0.93884491 Bal: 0.8871 - Val Loss: 0.29425099, Accuracy: 0.8894, F1: 0.9369 Bal: 0.8742\n","Epoch 163:      TX: Train Loss: 0.0290, Acc: 0.9901, F1: 0.9944 Bal: 0.9941 - Val Loss: 0.3248, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9156\n","           WALLETS: Train Loss: 0.27030718, Acc: 0.89624243, F1: 0.94106421 Bal: 0.8869 - Val Loss: 0.29436287, Accuracy: 0.8926, F1: 0.9389 Bal: 0.8731\n","Epoch 164:      TX: Train Loss: 0.0285, Acc: 0.9902, F1: 0.9945 Bal: 0.9942 - Val Loss: 0.3263, Accuracy: 0.9664, F1: 0.9813 Bal: 0.9156\n","           WALLETS: Train Loss: 0.26964611, Acc: 0.89361911, F1: 0.93945120 Bal: 0.8883 - Val Loss: 0.29353142, Accuracy: 0.8900, F1: 0.9373 Bal: 0.8742\n","Epoch 165:      TX: Train Loss: 0.0281, Acc: 0.9905, F1: 0.9947 Bal: 0.9943 - Val Loss: 0.3292, Accuracy: 0.9669, F1: 0.9816 Bal: 0.9148\n","           WALLETS: Train Loss: 0.26899841, Acc: 0.89697900, F1: 0.94150296 Bal: 0.8876 - Val Loss: 0.29369268, Accuracy: 0.8934, F1: 0.9394 Bal: 0.8737\n","Epoch 166:      TX: Train Loss: 0.0276, Acc: 0.9906, F1: 0.9948 Bal: 0.9944 - Val Loss: 0.3304, Accuracy: 0.9660, F1: 0.9811 Bal: 0.9134\n","           WALLETS: Train Loss: 0.26837122, Acc: 0.89333609, F1: 0.93927281 Bal: 0.8888 - Val Loss: 0.29270270, Accuracy: 0.8897, F1: 0.9371 Bal: 0.8753\n","Epoch 167:      TX: Train Loss: 0.0272, Acc: 0.9910, F1: 0.9950 Bal: 0.9946 - Val Loss: 0.3337, Accuracy: 0.9667, F1: 0.9815 Bal: 0.9137\n","           WALLETS: Train Loss: 0.26779196, Acc: 0.89924675, F1: 0.94287760 Bal: 0.8877 - Val Loss: 0.29338810, Accuracy: 0.8954, F1: 0.9406 Bal: 0.8731\n","Epoch 168:      TX: Train Loss: 0.0268, Acc: 0.9911, F1: 0.9950 Bal: 0.9947 - Val Loss: 0.3346, Accuracy: 0.9667, F1: 0.9815 Bal: 0.9137\n","           WALLETS: Train Loss: 0.26734370, Acc: 0.89081436, F1: 0.93772506 Bal: 0.8893 - Val Loss: 0.29174340, Accuracy: 0.8872, F1: 0.9356 Bal: 0.8755\n","Epoch 169:      TX: Train Loss: 0.0264, Acc: 0.9914, F1: 0.9952 Bal: 0.9948 - Val Loss: 0.3381, Accuracy: 0.9671, F1: 0.9817 Bal: 0.9140\n","           WALLETS: Train Loss: 0.26718649, Acc: 0.90481270, F1: 0.94624379 Bal: 0.8867 - Val Loss: 0.29430953, Accuracy: 0.9004, F1: 0.9437 Bal: 0.8693\n","Epoch 170:      TX: Train Loss: 0.0260, Acc: 0.9915, F1: 0.9952 Bal: 0.9950 - Val Loss: 0.3379, Accuracy: 0.9669, F1: 0.9816 Bal: 0.9139\n","           WALLETS: Train Loss: 0.26751107, Acc: 0.88347774, F1: 0.93320132 Bal: 0.8899 - Val Loss: 0.29127678, Accuracy: 0.8796, F1: 0.9309 Bal: 0.8755\n","Epoch 171:      TX: Train Loss: 0.0256, Acc: 0.9919, F1: 0.9955 Bal: 0.9952 - Val Loss: 0.3426, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9141\n","           WALLETS: Train Loss: 0.26733482, Acc: 0.91060725, F1: 0.94971907 Bal: 0.8859 - Val Loss: 0.29607293, Accuracy: 0.9067, F1: 0.9475 Bal: 0.8666\n","Epoch 172:      TX: Train Loss: 0.0252, Acc: 0.9917, F1: 0.9954 Bal: 0.9951 - Val Loss: 0.3403, Accuracy: 0.9671, F1: 0.9817 Bal: 0.9140\n","           WALLETS: Train Loss: 0.26569217, Acc: 0.88713516, F1: 0.93545161 Bal: 0.8905 - Val Loss: 0.29047334, Accuracy: 0.8832, F1: 0.9331 Bal: 0.8759\n","Epoch 173:      TX: Train Loss: 0.0248, Acc: 0.9923, F1: 0.9957 Bal: 0.9954 - Val Loss: 0.3461, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9143\n","           WALLETS: Train Loss: 0.26400396, Acc: 0.89819088, F1: 0.94221371 Bal: 0.8899 - Val Loss: 0.29091379, Accuracy: 0.8936, F1: 0.9395 Bal: 0.8740\n","Epoch 174:      TX: Train Loss: 0.0244, Acc: 0.9923, F1: 0.9957 Bal: 0.9954 - Val Loss: 0.3460, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9143\n","           WALLETS: Train Loss: 0.26430100, Acc: 0.90637291, F1: 0.94716366 Bal: 0.8883 - Val Loss: 0.29302111, Accuracy: 0.9021, F1: 0.9447 Bal: 0.8721\n","Epoch 175:      TX: Train Loss: 0.0241, Acc: 0.9922, F1: 0.9957 Bal: 0.9954 - Val Loss: 0.3457, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9141\n","           WALLETS: Train Loss: 0.26431680, Acc: 0.88616276, F1: 0.93484275 Bal: 0.8912 - Val Loss: 0.28963423, Accuracy: 0.8819, F1: 0.9323 Bal: 0.8764\n","Epoch 176:      TX: Train Loss: 0.0237, Acc: 0.9925, F1: 0.9958 Bal: 0.9956 - Val Loss: 0.3528, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9134\n","           WALLETS: Train Loss: 0.26263112, Acc: 0.90320895, F1: 0.94525125 Bal: 0.8894 - Val Loss: 0.29123035, Accuracy: 0.8986, F1: 0.9426 Bal: 0.8720\n","Epoch 177:      TX: Train Loss: 0.0234, Acc: 0.9924, F1: 0.9957 Bal: 0.9955 - Val Loss: 0.3497, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9141\n","           WALLETS: Train Loss: 0.26202428, Acc: 0.90280257, F1: 0.94500287 Bal: 0.8897 - Val Loss: 0.29080406, Accuracy: 0.8981, F1: 0.9423 Bal: 0.8725\n","Epoch 178:      TX: Train Loss: 0.0230, Acc: 0.9925, F1: 0.9958 Bal: 0.9956 - Val Loss: 0.3519, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9141\n","           WALLETS: Train Loss: 0.26230446, Acc: 0.88900742, F1: 0.93658001 Bal: 0.8923 - Val Loss: 0.28876546, Accuracy: 0.8849, F1: 0.9341 Bal: 0.8769\n","Epoch 179:      TX: Train Loss: 0.0227, Acc: 0.9929, F1: 0.9960 Bal: 0.9958 - Val Loss: 0.3586, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9135\n","           WALLETS: Train Loss: 0.26119396, Acc: 0.90528439, F1: 0.94649394 Bal: 0.8899 - Val Loss: 0.29098606, Accuracy: 0.9005, F1: 0.9437 Bal: 0.8719\n","Epoch 180:      TX: Train Loss: 0.0223, Acc: 0.9927, F1: 0.9959 Bal: 0.9957 - Val Loss: 0.3538, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9141\n","           WALLETS: Train Loss: 0.26024413, Acc: 0.90099563, F1: 0.94389453 Bal: 0.8914 - Val Loss: 0.28946775, Accuracy: 0.8959, F1: 0.9409 Bal: 0.8744\n","Epoch 181:      TX: Train Loss: 0.0220, Acc: 0.9929, F1: 0.9960 Bal: 0.9958 - Val Loss: 0.3575, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9143\n","           WALLETS: Train Loss: 0.26032501, Acc: 0.89170331, F1: 0.93822582 Bal: 0.8929 - Val Loss: 0.28809389, Accuracy: 0.8873, F1: 0.9356 Bal: 0.8768\n","Epoch 182:      TX: Train Loss: 0.0217, Acc: 0.9932, F1: 0.9962 Bal: 0.9959 - Val Loss: 0.3639, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9135\n","           WALLETS: Train Loss: 0.25965545, Acc: 0.90594476, F1: 0.94688245 Bal: 0.8907 - Val Loss: 0.29043701, Accuracy: 0.9007, F1: 0.9438 Bal: 0.8720\n","Epoch 183:      TX: Train Loss: 0.0214, Acc: 0.9930, F1: 0.9961 Bal: 0.9958 - Val Loss: 0.3582, Accuracy: 0.9673, F1: 0.9818 Bal: 0.9141\n","           WALLETS: Train Loss: 0.25865671, Acc: 0.89923949, F1: 0.94282149 Bal: 0.8924 - Val Loss: 0.28835678, Accuracy: 0.8941, F1: 0.9398 Bal: 0.8755\n","Epoch 184:      TX: Train Loss: 0.0211, Acc: 0.9932, F1: 0.9962 Bal: 0.9959 - Val Loss: 0.3628, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9143\n","           WALLETS: Train Loss: 0.25846130, Acc: 0.89466408, F1: 0.94002665 Bal: 0.8936 - Val Loss: 0.28748876, Accuracy: 0.8897, F1: 0.9371 Bal: 0.8761\n","Epoch 185:      TX: Train Loss: 0.0208, Acc: 0.9936, F1: 0.9964 Bal: 0.9962 - Val Loss: 0.3690, Accuracy: 0.9684, F1: 0.9825 Bal: 0.9137\n","           WALLETS: Train Loss: 0.25807843, Acc: 0.90628220, F1: 0.94708051 Bal: 0.8912 - Val Loss: 0.28973165, Accuracy: 0.9009, F1: 0.9440 Bal: 0.8720\n","Epoch 186:      TX: Train Loss: 0.0205, Acc: 0.9932, F1: 0.9962 Bal: 0.9961 - Val Loss: 0.3638, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9143\n","           WALLETS: Train Loss: 0.25717947, Acc: 0.89824531, F1: 0.94220187 Bal: 0.8939 - Val Loss: 0.28737831, Accuracy: 0.8929, F1: 0.9391 Bal: 0.8761\n","Epoch 187:      TX: Train Loss: 0.0202, Acc: 0.9935, F1: 0.9964 Bal: 0.9963 - Val Loss: 0.3680, Accuracy: 0.9675, F1: 0.9820 Bal: 0.9132\n","           WALLETS: Train Loss: 0.25671214, Acc: 0.89772282, F1: 0.94188113 Bal: 0.8942 - Val Loss: 0.28706834, Accuracy: 0.8923, F1: 0.9387 Bal: 0.8760\n","Epoch 188:      TX: Train Loss: 0.0199, Acc: 0.9939, F1: 0.9966 Bal: 0.9963 - Val Loss: 0.3743, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9125\n","           WALLETS: Train Loss: 0.25646371, Acc: 0.90630034, F1: 0.94708197 Bal: 0.8921 - Val Loss: 0.28886381, Accuracy: 0.9006, F1: 0.9438 Bal: 0.8724\n","Epoch 189:      TX: Train Loss: 0.0197, Acc: 0.9937, F1: 0.9965 Bal: 0.9964 - Val Loss: 0.3698, Accuracy: 0.9675, F1: 0.9820 Bal: 0.9132\n","           WALLETS: Train Loss: 0.25578284, Acc: 0.89723299, F1: 0.94156724 Bal: 0.8957 - Val Loss: 0.28649542, Accuracy: 0.8919, F1: 0.9385 Bal: 0.8775\n","Epoch 190:      TX: Train Loss: 0.0194, Acc: 0.9938, F1: 0.9965 Bal: 0.9964 - Val Loss: 0.3732, Accuracy: 0.9675, F1: 0.9820 Bal: 0.9123\n","           WALLETS: Train Loss: 0.25510705, Acc: 0.90097749, F1: 0.94385388 Bal: 0.8942 - Val Loss: 0.28684038, Accuracy: 0.8954, F1: 0.9406 Bal: 0.8753\n","Epoch 191:      TX: Train Loss: 0.0191, Acc: 0.9941, F1: 0.9967 Bal: 0.9966 - Val Loss: 0.3790, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9125\n","           WALLETS: Train Loss: 0.25480077, Acc: 0.90562546, F1: 0.94666213 Bal: 0.8935 - Val Loss: 0.28779066, Accuracy: 0.8997, F1: 0.9432 Bal: 0.8737\n","Epoch 192:      TX: Train Loss: 0.0189, Acc: 0.9939, F1: 0.9966 Bal: 0.9966 - Val Loss: 0.3755, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9124\n","           WALLETS: Train Loss: 0.25436735, Acc: 0.89706971, F1: 0.94146511 Bal: 0.8959 - Val Loss: 0.28578699, Accuracy: 0.8917, F1: 0.9383 Bal: 0.8778\n","Epoch 193:      TX: Train Loss: 0.0186, Acc: 0.9940, F1: 0.9967 Bal: 0.9967 - Val Loss: 0.3775, Accuracy: 0.9682, F1: 0.9823 Bal: 0.9126\n","           WALLETS: Train Loss: 0.25366533, Acc: 0.90421765, F1: 0.94580935 Bal: 0.8941 - Val Loss: 0.28682452, Accuracy: 0.8985, F1: 0.9425 Bal: 0.8746\n","Epoch 194:      TX: Train Loss: 0.0184, Acc: 0.9944, F1: 0.9969 Bal: 0.9969 - Val Loss: 0.3827, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9115\n","           WALLETS: Train Loss: 0.25315458, Acc: 0.90418499, F1: 0.94578408 Bal: 0.8947 - Val Loss: 0.28650588, Accuracy: 0.8985, F1: 0.9425 Bal: 0.8752\n","Epoch 195:      TX: Train Loss: 0.0181, Acc: 0.9944, F1: 0.9969 Bal: 0.9969 - Val Loss: 0.3812, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9115\n","           WALLETS: Train Loss: 0.25281900, Acc: 0.89870974, F1: 0.94245405 Bal: 0.8966 - Val Loss: 0.28520119, Accuracy: 0.8931, F1: 0.9392 Bal: 0.8775\n","Epoch 196:      TX: Train Loss: 0.0179, Acc: 0.9944, F1: 0.9969 Bal: 0.9969 - Val Loss: 0.3824, Accuracy: 0.9677, F1: 0.9821 Bal: 0.9114\n","           WALLETS: Train Loss: 0.25229886, Acc: 0.90639105, F1: 0.94710946 Bal: 0.8948 - Val Loss: 0.28665516, Accuracy: 0.9005, F1: 0.9437 Bal: 0.8746\n","Epoch 197:      TX: Train Loss: 0.0176, Acc: 0.9946, F1: 0.9970 Bal: 0.9970 - Val Loss: 0.3868, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9115\n","           WALLETS: Train Loss: 0.25166747, Acc: 0.90195353, F1: 0.94441336 Bal: 0.8970 - Val Loss: 0.28519109, Accuracy: 0.8959, F1: 0.9409 Bal: 0.8771\n","Epoch 198:      TX: Train Loss: 0.0174, Acc: 0.9946, F1: 0.9970 Bal: 0.9970 - Val Loss: 0.3872, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9115\n","           WALLETS: Train Loss: 0.25118700, Acc: 0.90202972, F1: 0.94445850 Bal: 0.8971 - Val Loss: 0.28495151, Accuracy: 0.8960, F1: 0.9409 Bal: 0.8775\n","Epoch 199:      TX: Train Loss: 0.0172, Acc: 0.9946, F1: 0.9970 Bal: 0.9970 - Val Loss: 0.3874, Accuracy: 0.9680, F1: 0.9822 Bal: 0.9125\n","           WALLETS: Train Loss: 0.25081155, Acc: 0.90693894, F1: 0.94741719 Bal: 0.8969 - Val Loss: 0.28599694, Accuracy: 0.9007, F1: 0.9438 Bal: 0.8756\n","Epoch 200:      TX: Train Loss: 0.0170, Acc: 0.9948, F1: 0.9971 Bal: 0.9971 - Val Loss: 0.3909, Accuracy: 0.9684, F1: 0.9825 Bal: 0.9127\n","           WALLETS: Train Loss: 0.25033462, Acc: 0.90078156, F1: 0.94369321 Bal: 0.8981 - Val Loss: 0.28427613, Accuracy: 0.8946, F1: 0.9401 Bal: 0.8784\n","Epoch 201:      TX: Train Loss: 0.0167, Acc: 0.9950, F1: 0.9972 Bal: 0.9972 - Val Loss: 0.3929, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9129\n","           WALLETS: Train Loss: 0.24976394, Acc: 0.90590485, F1: 0.94679068 Bal: 0.8974 - Val Loss: 0.28518781, Accuracy: 0.8996, F1: 0.9432 Bal: 0.8763\n","Epoch 202:      TX: Train Loss: 0.0165, Acc: 0.9949, F1: 0.9972 Bal: 0.9972 - Val Loss: 0.3924, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9138\n","           WALLETS: Train Loss: 0.24926066, Acc: 0.90482722, F1: 0.94613875 Bal: 0.8978 - Val Loss: 0.28464866, Accuracy: 0.8985, F1: 0.9425 Bal: 0.8773\n","Epoch 203:      TX: Train Loss: 0.0163, Acc: 0.9951, F1: 0.9973 Bal: 0.9973 - Val Loss: 0.3953, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9140\n","           WALLETS: Train Loss: 0.24885957, Acc: 0.90246876, F1: 0.94470865 Bal: 0.8985 - Val Loss: 0.28386253, Accuracy: 0.8959, F1: 0.9409 Bal: 0.8784\n","Epoch 204:      TX: Train Loss: 0.0161, Acc: 0.9953, F1: 0.9974 Bal: 0.9974 - Val Loss: 0.3981, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9140\n","           WALLETS: Train Loss: 0.24845046, Acc: 0.90751586, F1: 0.94775249 Bal: 0.8980 - Val Loss: 0.28496721, Accuracy: 0.9007, F1: 0.9438 Bal: 0.8750\n","Epoch 205:      TX: Train Loss: 0.0159, Acc: 0.9953, F1: 0.9974 Bal: 0.9974 - Val Loss: 0.3972, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9148\n","           WALLETS: Train Loss: 0.24797112, Acc: 0.90254858, F1: 0.94474549 Bal: 0.8996 - Val Loss: 0.28340364, Accuracy: 0.8961, F1: 0.9410 Bal: 0.8789\n","Epoch 206:      TX: Train Loss: 0.0157, Acc: 0.9954, F1: 0.9975 Bal: 0.9975 - Val Loss: 0.4002, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24745724, Acc: 0.90648902, F1: 0.94712980 Bal: 0.8986 - Val Loss: 0.28415272, Accuracy: 0.8998, F1: 0.9433 Bal: 0.8769\n","Epoch 207:      TX: Train Loss: 0.0155, Acc: 0.9956, F1: 0.9975 Bal: 0.9975 - Val Loss: 0.4029, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9141\n","           WALLETS: Train Loss: 0.24698026, Acc: 0.90544767, F1: 0.94649482 Bal: 0.8994 - Val Loss: 0.28362688, Accuracy: 0.8989, F1: 0.9427 Bal: 0.8784\n","Epoch 208:      TX: Train Loss: 0.0153, Acc: 0.9956, F1: 0.9975 Bal: 0.9975 - Val Loss: 0.4027, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9149\n","           WALLETS: Train Loss: 0.24655719, Acc: 0.90413419, F1: 0.94569960 Bal: 0.8998 - Val Loss: 0.28310186, Accuracy: 0.8979, F1: 0.9421 Bal: 0.8793\n","Epoch 209:      TX: Train Loss: 0.0151, Acc: 0.9957, F1: 0.9976 Bal: 0.9976 - Val Loss: 0.4052, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9149\n","           WALLETS: Train Loss: 0.24615198, Acc: 0.90784241, F1: 0.94793270 Bal: 0.8995 - Val Loss: 0.28391114, Accuracy: 0.9013, F1: 0.9441 Bal: 0.8778\n","Epoch 210:      TX: Train Loss: 0.0149, Acc: 0.9957, F1: 0.9976 Bal: 0.9976 - Val Loss: 0.4078, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24572523, Acc: 0.90332869, F1: 0.94520821 Bal: 0.9004 - Val Loss: 0.28253087, Accuracy: 0.8969, F1: 0.9415 Bal: 0.8792\n","Epoch 211:      TX: Train Loss: 0.0148, Acc: 0.9957, F1: 0.9976 Bal: 0.9976 - Val Loss: 0.4085, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9149\n","           WALLETS: Train Loss: 0.24526842, Acc: 0.90832862, F1: 0.94821844 Bal: 0.9001 - Val Loss: 0.28365460, Accuracy: 0.9020, F1: 0.9446 Bal: 0.8785\n","Epoch 212:      TX: Train Loss: 0.0146, Acc: 0.9958, F1: 0.9977 Bal: 0.9977 - Val Loss: 0.4095, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9149\n","           WALLETS: Train Loss: 0.24479821, Acc: 0.90451517, F1: 0.94591951 Bal: 0.9008 - Val Loss: 0.28237948, Accuracy: 0.8979, F1: 0.9421 Bal: 0.8791\n","Epoch 213:      TX: Train Loss: 0.0144, Acc: 0.9961, F1: 0.9978 Bal: 0.9978 - Val Loss: 0.4125, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.24432884, Acc: 0.90733444, F1: 0.94761800 Bal: 0.9005 - Val Loss: 0.28298345, Accuracy: 0.9014, F1: 0.9442 Bal: 0.8790\n","Epoch 214:      TX: Train Loss: 0.0142, Acc: 0.9961, F1: 0.9978 Bal: 0.9978 - Val Loss: 0.4129, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.24387482, Acc: 0.90608264, F1: 0.94686058 Bal: 0.9010 - Val Loss: 0.28244817, Accuracy: 0.8999, F1: 0.9433 Bal: 0.8793\n","Epoch 215:      TX: Train Loss: 0.0141, Acc: 0.9962, F1: 0.9979 Bal: 0.9979 - Val Loss: 0.4138, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.24343383, Acc: 0.90641645, F1: 0.94705358 Bal: 0.9018 - Val Loss: 0.28233698, Accuracy: 0.9003, F1: 0.9435 Bal: 0.8800\n","Epoch 216:      TX: Train Loss: 0.0139, Acc: 0.9963, F1: 0.9979 Bal: 0.9979 - Val Loss: 0.4166, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9141\n","           WALLETS: Train Loss: 0.24300705, Acc: 0.90780250, F1: 0.94789125 Bal: 0.9013 - Val Loss: 0.28258738, Accuracy: 0.9016, F1: 0.9443 Bal: 0.8791\n","Epoch 217:      TX: Train Loss: 0.0137, Acc: 0.9963, F1: 0.9979 Bal: 0.9980 - Val Loss: 0.4178, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9141\n","           WALLETS: Train Loss: 0.24259281, Acc: 0.90563272, F1: 0.94657332 Bal: 0.9027 - Val Loss: 0.28180405, Accuracy: 0.8995, F1: 0.9430 Bal: 0.8805\n","Epoch 218:      TX: Train Loss: 0.0136, Acc: 0.9963, F1: 0.9979 Bal: 0.9980 - Val Loss: 0.4191, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.24219117, Acc: 0.90968926, F1: 0.94901053 Bal: 0.9025 - Val Loss: 0.28272414, Accuracy: 0.9029, F1: 0.9451 Bal: 0.8790\n","Epoch 219:      TX: Train Loss: 0.0134, Acc: 0.9964, F1: 0.9980 Bal: 0.9980 - Val Loss: 0.4210, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.24181326, Acc: 0.90472925, F1: 0.94602201 Bal: 0.9033 - Val Loss: 0.28124303, Accuracy: 0.8984, F1: 0.9423 Bal: 0.8807\n","Epoch 220:      TX: Train Loss: 0.0133, Acc: 0.9965, F1: 0.9980 Bal: 0.9981 - Val Loss: 0.4227, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.24147306, Acc: 0.91168488, F1: 0.95020153 Bal: 0.9027 - Val Loss: 0.28308645, Accuracy: 0.9050, F1: 0.9463 Bal: 0.8797\n","Epoch 221:      TX: Train Loss: 0.0131, Acc: 0.9965, F1: 0.9980 Bal: 0.9981 - Val Loss: 0.4229, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24121374, Acc: 0.90261390, F1: 0.94474183 Bal: 0.9037 - Val Loss: 0.28064299, Accuracy: 0.8962, F1: 0.9410 Bal: 0.8814\n","Epoch 222:      TX: Train Loss: 0.0129, Acc: 0.9966, F1: 0.9981 Bal: 0.9981 - Val Loss: 0.4263, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24106541, Acc: 0.91432635, F1: 0.95178450 Bal: 0.9017 - Val Loss: 0.28418559, Accuracy: 0.9073, F1: 0.9478 Bal: 0.8775\n","Epoch 223:      TX: Train Loss: 0.0128, Acc: 0.9966, F1: 0.9981 Bal: 0.9981 - Val Loss: 0.4267, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24112391, Acc: 0.89917781, F1: 0.94265342 Bal: 0.9042 - Val Loss: 0.28007731, Accuracy: 0.8928, F1: 0.9389 Bal: 0.8825\n","Epoch 224:      TX: Train Loss: 0.0126, Acc: 0.9967, F1: 0.9982 Bal: 0.9982 - Val Loss: 0.4288, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24119076, Acc: 0.91796563, F1: 0.95394819 Bal: 0.9013 - Val Loss: 0.28608298, Accuracy: 0.9109, F1: 0.9499 Bal: 0.8760\n","Epoch 225:      TX: Train Loss: 0.0125, Acc: 0.9967, F1: 0.9982 Bal: 0.9982 - Val Loss: 0.4290, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24111412, Acc: 0.89631137, F1: 0.94090594 Bal: 0.9045 - Val Loss: 0.27985278, Accuracy: 0.8900, F1: 0.9372 Bal: 0.8824\n","Epoch 226:      TX: Train Loss: 0.0124, Acc: 0.9968, F1: 0.9982 Bal: 0.9982 - Val Loss: 0.4329, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.24010697, Acc: 0.91728712, F1: 0.95353975 Bal: 0.9020 - Val Loss: 0.28532887, Accuracy: 0.9104, F1: 0.9496 Bal: 0.8766\n","Epoch 227:      TX: Train Loss: 0.0122, Acc: 0.9968, F1: 0.9982 Bal: 0.9982 - Val Loss: 0.4316, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23882160, Acc: 0.90385481, F1: 0.94547961 Bal: 0.9048 - Val Loss: 0.28005955, Accuracy: 0.8980, F1: 0.9421 Bal: 0.8830\n","Epoch 228:      TX: Train Loss: 0.0121, Acc: 0.9968, F1: 0.9982 Bal: 0.9982 - Val Loss: 0.4339, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23810397, Acc: 0.90834313, F1: 0.94817672 Bal: 0.9052 - Val Loss: 0.28084999, Accuracy: 0.9023, F1: 0.9447 Bal: 0.8825\n","Epoch 229:      TX: Train Loss: 0.0119, Acc: 0.9969, F1: 0.9983 Bal: 0.9983 - Val Loss: 0.4365, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23820288, Acc: 0.91471459, F1: 0.95199152 Bal: 0.9044 - Val Loss: 0.28320128, Accuracy: 0.9078, F1: 0.9480 Bal: 0.8797\n","Epoch 230:      TX: Train Loss: 0.0118, Acc: 0.9970, F1: 0.9983 Bal: 0.9983 - Val Loss: 0.4356, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23841861, Acc: 0.90017199, F1: 0.94324724 Bal: 0.9051 - Val Loss: 0.27929845, Accuracy: 0.8938, F1: 0.9395 Bal: 0.8824\n","Epoch 231:      TX: Train Loss: 0.0117, Acc: 0.9970, F1: 0.9983 Bal: 0.9983 - Val Loss: 0.4386, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23788644, Acc: 0.91686260, F1: 0.95326844 Bal: 0.9043 - Val Loss: 0.28415352, Accuracy: 0.9099, F1: 0.9493 Bal: 0.8789\n","Epoch 232:      TX: Train Loss: 0.0115, Acc: 0.9970, F1: 0.9983 Bal: 0.9983 - Val Loss: 0.4394, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23691599, Acc: 0.90504492, F1: 0.94618947 Bal: 0.9056 - Val Loss: 0.27965179, Accuracy: 0.8990, F1: 0.9427 Bal: 0.8830\n","Epoch 233:      TX: Train Loss: 0.0114, Acc: 0.9971, F1: 0.9984 Bal: 0.9984 - Val Loss: 0.4407, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23626000, Acc: 0.90942076, F1: 0.94881783 Bal: 0.9057 - Val Loss: 0.28054142, Accuracy: 0.9031, F1: 0.9451 Bal: 0.8829\n","Epoch 234:      TX: Train Loss: 0.0113, Acc: 0.9971, F1: 0.9984 Bal: 0.9984 - Val Loss: 0.4424, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9140\n","           WALLETS: Train Loss: 0.23622186, Acc: 0.91437715, F1: 0.95178000 Bal: 0.9055 - Val Loss: 0.28232414, Accuracy: 0.9073, F1: 0.9477 Bal: 0.8803\n","Epoch 235:      TX: Train Loss: 0.0111, Acc: 0.9971, F1: 0.9984 Bal: 0.9984 - Val Loss: 0.4434, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9140\n","           WALLETS: Train Loss: 0.23626016, Acc: 0.90265018, F1: 0.94473807 Bal: 0.9061 - Val Loss: 0.27890140, Accuracy: 0.8962, F1: 0.9410 Bal: 0.8828\n","Epoch 236:      TX: Train Loss: 0.0110, Acc: 0.9973, F1: 0.9985 Bal: 0.9985 - Val Loss: 0.4461, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23577398, Acc: 0.91606798, F1: 0.95278663 Bal: 0.9054 - Val Loss: 0.28289965, Accuracy: 0.9089, F1: 0.9487 Bal: 0.8796\n","Epoch 237:      TX: Train Loss: 0.0109, Acc: 0.9973, F1: 0.9985 Bal: 0.9985 - Val Loss: 0.4458, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23502396, Acc: 0.90667407, F1: 0.94716321 Bal: 0.9063 - Val Loss: 0.27934536, Accuracy: 0.9003, F1: 0.9435 Bal: 0.8830\n","Epoch 238:      TX: Train Loss: 0.0108, Acc: 0.9974, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4480, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23450361, Acc: 0.90995051, F1: 0.94912718 Bal: 0.9065 - Val Loss: 0.28007481, Accuracy: 0.9033, F1: 0.9453 Bal: 0.8829\n","Epoch 239:      TX: Train Loss: 0.0107, Acc: 0.9974, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4497, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9138\n","           WALLETS: Train Loss: 0.23436891, Acc: 0.91424653, F1: 0.95169361 Bal: 0.9065 - Val Loss: 0.28156713, Accuracy: 0.9070, F1: 0.9475 Bal: 0.8808\n","Epoch 240:      TX: Train Loss: 0.0105, Acc: 0.9974, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4493, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9138\n","           WALLETS: Train Loss: 0.23428535, Acc: 0.90473651, F1: 0.94599104 Bal: 0.9068 - Val Loss: 0.27863744, Accuracy: 0.8984, F1: 0.9423 Bal: 0.8835\n","Epoch 241:      TX: Train Loss: 0.0104, Acc: 0.9975, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4524, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9140\n","           WALLETS: Train Loss: 0.23388924, Acc: 0.91597727, F1: 0.95272046 Bal: 0.9068 - Val Loss: 0.28207263, Accuracy: 0.9089, F1: 0.9487 Bal: 0.8806\n","Epoch 242:      TX: Train Loss: 0.0103, Acc: 0.9975, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4530, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9138\n","           WALLETS: Train Loss: 0.23330005, Acc: 0.90733081, F1: 0.94754914 Bal: 0.9072 - Val Loss: 0.27889541, Accuracy: 0.9005, F1: 0.9436 Bal: 0.8834\n","Epoch 243:      TX: Train Loss: 0.0102, Acc: 0.9975, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4544, Accuracy: 0.9686, F1: 0.9826 Bal: 0.9138\n","           WALLETS: Train Loss: 0.23279928, Acc: 0.91148532, F1: 0.95003922 Bal: 0.9072 - Val Loss: 0.27993995, Accuracy: 0.9044, F1: 0.9459 Bal: 0.8827\n","Epoch 244:      TX: Train Loss: 0.0101, Acc: 0.9975, F1: 0.9986 Bal: 0.9986 - Val Loss: 0.4561, Accuracy: 0.9688, F1: 0.9827 Bal: 0.9140\n","           WALLETS: Train Loss: 0.23253758, Acc: 0.91350634, F1: 0.95124336 Bal: 0.9075 - Val Loss: 0.28056151, Accuracy: 0.9062, F1: 0.9470 Bal: 0.8821\n","Epoch 245:      TX: Train Loss: 0.0100, Acc: 0.9976, F1: 0.9986 Bal: 0.9987 - Val Loss: 0.4569, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23238266, Acc: 0.90706594, F1: 0.94738594 Bal: 0.9076 - Val Loss: 0.27849445, Accuracy: 0.9002, F1: 0.9434 Bal: 0.8834\n","Epoch 246:      TX: Train Loss: 0.0099, Acc: 0.9976, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4590, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23211984, Acc: 0.91602807, F1: 0.95274151 Bal: 0.9078 - Val Loss: 0.28144714, Accuracy: 0.9088, F1: 0.9486 Bal: 0.8811\n","Epoch 247:      TX: Train Loss: 0.0097, Acc: 0.9976, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4595, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.23170842, Acc: 0.90728727, F1: 0.94751674 Bal: 0.9078 - Val Loss: 0.27834636, Accuracy: 0.9003, F1: 0.9435 Bal: 0.8833\n","Epoch 248:      TX: Train Loss: 0.0096, Acc: 0.9977, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4620, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23122202, Acc: 0.91386192, F1: 0.95145155 Bal: 0.9079 - Val Loss: 0.28035438, Accuracy: 0.9067, F1: 0.9473 Bal: 0.8815\n","Epoch 249:      TX: Train Loss: 0.0095, Acc: 0.9977, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4628, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23081070, Acc: 0.91111159, F1: 0.94981070 Bal: 0.9077 - Val Loss: 0.27928579, Accuracy: 0.9040, F1: 0.9457 Bal: 0.8828\n","Epoch 250:      TX: Train Loss: 0.0094, Acc: 0.9977, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4636, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23051549, Acc: 0.91007387, F1: 0.94918771 Bal: 0.9079 - Val Loss: 0.27887034, Accuracy: 0.9026, F1: 0.9449 Bal: 0.8830\n","Epoch 251:      TX: Train Loss: 0.0093, Acc: 0.9977, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4656, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23028812, Acc: 0.91470371, F1: 0.95194918 Bal: 0.9083 - Val Loss: 0.28041327, Accuracy: 0.9073, F1: 0.9477 Bal: 0.8821\n","Epoch 252:      TX: Train Loss: 0.0092, Acc: 0.9977, F1: 0.9987 Bal: 0.9987 - Val Loss: 0.4662, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.23005535, Acc: 0.90794038, F1: 0.94790636 Bal: 0.9081 - Val Loss: 0.27811509, Accuracy: 0.9005, F1: 0.9436 Bal: 0.8831\n","Epoch 253:      TX: Train Loss: 0.0091, Acc: 0.9978, F1: 0.9988 Bal: 0.9988 - Val Loss: 0.4681, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22975467, Acc: 0.91608612, F1: 0.95277091 Bal: 0.9083 - Val Loss: 0.28080758, Accuracy: 0.9085, F1: 0.9484 Bal: 0.8813\n","Epoch 254:      TX: Train Loss: 0.0090, Acc: 0.9978, F1: 0.9988 Bal: 0.9988 - Val Loss: 0.4692, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22939923, Acc: 0.90851366, F1: 0.94824734 Bal: 0.9084 - Val Loss: 0.27805844, Accuracy: 0.9010, F1: 0.9439 Bal: 0.8830\n","Epoch 255:      TX: Train Loss: 0.0089, Acc: 0.9979, F1: 0.9988 Bal: 0.9988 - Val Loss: 0.4713, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9153\n","           WALLETS: Train Loss: 0.22900033, Acc: 0.91511734, F1: 0.95219119 Bal: 0.9087 - Val Loss: 0.28018075, Accuracy: 0.9074, F1: 0.9478 Bal: 0.8815\n","Epoch 256:      TX: Train Loss: 0.0088, Acc: 0.9979, F1: 0.9988 Bal: 0.9988 - Val Loss: 0.4719, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.22861342, Acc: 0.91057459, F1: 0.94947954 Bal: 0.9087 - Val Loss: 0.27843180, Accuracy: 0.9032, F1: 0.9452 Bal: 0.8833\n","Epoch 257:      TX: Train Loss: 0.0087, Acc: 0.9980, F1: 0.9989 Bal: 0.9989 - Val Loss: 0.4736, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9153\n","           WALLETS: Train Loss: 0.22825789, Acc: 0.91317978, F1: 0.95103204 Bal: 0.9092 - Val Loss: 0.27919468, Accuracy: 0.9057, F1: 0.9468 Bal: 0.8829\n","Epoch 258:      TX: Train Loss: 0.0086, Acc: 0.9981, F1: 0.9989 Bal: 0.9989 - Val Loss: 0.4749, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9142\n","           WALLETS: Train Loss: 0.22793867, Acc: 0.91297659, F1: 0.95091040 Bal: 0.9093 - Val Loss: 0.27905098, Accuracy: 0.9055, F1: 0.9466 Bal: 0.8829\n","Epoch 259:      TX: Train Loss: 0.0086, Acc: 0.9981, F1: 0.9990 Bal: 0.9990 - Val Loss: 0.4756, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9143\n","           WALLETS: Train Loss: 0.22764969, Acc: 0.91146355, F1: 0.95000584 Bal: 0.9093 - Val Loss: 0.27846605, Accuracy: 0.9040, F1: 0.9457 Bal: 0.8837\n","Epoch 260:      TX: Train Loss: 0.0085, Acc: 0.9982, F1: 0.9990 Bal: 0.9990 - Val Loss: 0.4775, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9143\n","           WALLETS: Train Loss: 0.22738123, Acc: 0.91503026, F1: 0.95213236 Bal: 0.9095 - Val Loss: 0.27972150, Accuracy: 0.9070, F1: 0.9475 Bal: 0.8821\n","Epoch 261:      TX: Train Loss: 0.0084, Acc: 0.9982, F1: 0.9990 Bal: 0.9990 - Val Loss: 0.4784, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9142\n","           WALLETS: Train Loss: 0.22713254, Acc: 0.91002670, F1: 0.94914343 Bal: 0.9096 - Val Loss: 0.27792579, Accuracy: 0.9024, F1: 0.9447 Bal: 0.8833\n","Epoch 262:      TX: Train Loss: 0.0083, Acc: 0.9982, F1: 0.9990 Bal: 0.9990 - Val Loss: 0.4800, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9142\n","           WALLETS: Train Loss: 0.22690278, Acc: 0.91673561, F1: 0.95314612 Bal: 0.9095 - Val Loss: 0.28036326, Accuracy: 0.9085, F1: 0.9484 Bal: 0.8814\n","Epoch 263:      TX: Train Loss: 0.0082, Acc: 0.9982, F1: 0.9990 Bal: 0.9990 - Val Loss: 0.4807, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22671631, Acc: 0.90847738, F1: 0.94820895 Bal: 0.9101 - Val Loss: 0.27739930, Accuracy: 0.9007, F1: 0.9437 Bal: 0.8830\n","Epoch 264:      TX: Train Loss: 0.0081, Acc: 0.9983, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4833, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9142\n","           WALLETS: Train Loss: 0.22657049, Acc: 0.91894893, F1: 0.95445797 Bal: 0.9096 - Val Loss: 0.28132823, Accuracy: 0.9106, F1: 0.9497 Bal: 0.8811\n","Epoch 265:      TX: Train Loss: 0.0080, Acc: 0.9982, F1: 0.9990 Bal: 0.9990 - Val Loss: 0.4830, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22652309, Acc: 0.90631123, F1: 0.94690806 Bal: 0.9099 - Val Loss: 0.27688232, Accuracy: 0.8984, F1: 0.9423 Bal: 0.8832\n","Epoch 266:      TX: Train Loss: 0.0079, Acc: 0.9983, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4859, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9153\n","           WALLETS: Train Loss: 0.22648524, Acc: 0.92134730, F1: 0.95587981 Bal: 0.9092 - Val Loss: 0.28275844, Accuracy: 0.9127, F1: 0.9509 Bal: 0.8796\n","Epoch 267:      TX: Train Loss: 0.0079, Acc: 0.9983, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4857, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22650249, Acc: 0.90420313, F1: 0.94563418 Bal: 0.9101 - Val Loss: 0.27651772, Accuracy: 0.8965, F1: 0.9412 Bal: 0.8832\n","Epoch 268:      TX: Train Loss: 0.0078, Acc: 0.9984, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4885, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22622377, Acc: 0.92272971, F1: 0.95669696 Bal: 0.9089 - Val Loss: 0.28352901, Accuracy: 0.9142, F1: 0.9518 Bal: 0.8787\n","Epoch 269:      TX: Train Loss: 0.0077, Acc: 0.9984, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4882, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22573429, Acc: 0.90518280, F1: 0.94622182 Bal: 0.9105 - Val Loss: 0.27646974, Accuracy: 0.8974, F1: 0.9417 Bal: 0.8831\n","Epoch 270:      TX: Train Loss: 0.0076, Acc: 0.9984, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4915, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22487932, Acc: 0.92018258, F1: 0.95518186 Bal: 0.9103 - Val Loss: 0.28151605, Accuracy: 0.9116, F1: 0.9502 Bal: 0.8817\n","Epoch 271:      TX: Train Loss: 0.0075, Acc: 0.9984, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4915, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22413658, Acc: 0.91134381, F1: 0.94991904 Bal: 0.9109 - Val Loss: 0.27759558, Accuracy: 0.9033, F1: 0.9453 Bal: 0.8838\n","Epoch 272:      TX: Train Loss: 0.0075, Acc: 0.9984, F1: 0.9991 Bal: 0.9991 - Val Loss: 0.4931, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22373326, Acc: 0.91356439, F1: 0.95124297 Bal: 0.9112 - Val Loss: 0.27825096, Accuracy: 0.9053, F1: 0.9465 Bal: 0.8836\n","Epoch 273:      TX: Train Loss: 0.0074, Acc: 0.9985, F1: 0.9992 Bal: 0.9992 - Val Loss: 0.4951, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22366869, Acc: 0.91837564, F1: 0.95410366 Bal: 0.9113 - Val Loss: 0.28024560, Accuracy: 0.9100, F1: 0.9493 Bal: 0.8834\n","Epoch 274:      TX: Train Loss: 0.0073, Acc: 0.9985, F1: 0.9992 Bal: 0.9992 - Val Loss: 0.4952, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9164\n","           WALLETS: Train Loss: 0.22373471, Acc: 0.90812905, F1: 0.94798792 Bal: 0.9113 - Val Loss: 0.27669528, Accuracy: 0.9005, F1: 0.9436 Bal: 0.8837\n","Epoch 275:      TX: Train Loss: 0.0072, Acc: 0.9986, F1: 0.9992 Bal: 0.9992 - Val Loss: 0.4978, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9153\n","           WALLETS: Train Loss: 0.22364402, Acc: 0.92157588, F1: 0.95600186 Bal: 0.9107 - Val Loss: 0.28209141, Accuracy: 0.9126, F1: 0.9509 Bal: 0.8812\n","Epoch 276:      TX: Train Loss: 0.0072, Acc: 0.9986, F1: 0.9992 Bal: 0.9992 - Val Loss: 0.4979, Accuracy: 0.9691, F1: 0.9828 Bal: 0.9151\n","           WALLETS: Train Loss: 0.22333345, Acc: 0.90767913, F1: 0.94771752 Bal: 0.9113 - Val Loss: 0.27655026, Accuracy: 0.8998, F1: 0.9432 Bal: 0.8838\n","Epoch 277:      TX: Train Loss: 0.0071, Acc: 0.9986, F1: 0.9992 Bal: 0.9992 - Val Loss: 0.5004, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22273320, Acc: 0.92022975, F1: 0.95520022 Bal: 0.9114 - Val Loss: 0.28107148, Accuracy: 0.9111, F1: 0.9500 Bal: 0.8822\n","Epoch 278:      TX: Train Loss: 0.0070, Acc: 0.9986, F1: 0.9992 Bal: 0.9992 - Val Loss: 0.5009, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22214775, Acc: 0.91214206, F1: 0.95038725 Bal: 0.9119 - Val Loss: 0.27747160, Accuracy: 0.9038, F1: 0.9456 Bal: 0.8838\n","Epoch 279:      TX: Train Loss: 0.0069, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5029, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22173713, Acc: 0.91541850, F1: 0.95233811 Bal: 0.9123 - Val Loss: 0.27853957, Accuracy: 0.9067, F1: 0.9474 Bal: 0.8833\n","Epoch 280:      TX: Train Loss: 0.0069, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5043, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22153699, Acc: 0.91766448, F1: 0.95367335 Bal: 0.9123 - Val Loss: 0.27946243, Accuracy: 0.9087, F1: 0.9485 Bal: 0.8831\n","Epoch 281:      TX: Train Loss: 0.0068, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5048, Accuracy: 0.9693, F1: 0.9829 Bal: 0.9152\n","           WALLETS: Train Loss: 0.22145954, Acc: 0.91111885, F1: 0.94977261 Bal: 0.9121 - Val Loss: 0.27700970, Accuracy: 0.9030, F1: 0.9451 Bal: 0.8839\n","Epoch 282:      TX: Train Loss: 0.0067, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5072, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9153\n","           WALLETS: Train Loss: 0.22135976, Acc: 0.92088286, F1: 0.95558077 Bal: 0.9121 - Val Loss: 0.28113902, Accuracy: 0.9117, F1: 0.9503 Bal: 0.8829\n","Epoch 283:      TX: Train Loss: 0.0067, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5069, Accuracy: 0.9695, F1: 0.9831 Bal: 0.9153\n","           WALLETS: Train Loss: 0.22117591, Acc: 0.90958767, F1: 0.94885405 Bal: 0.9122 - Val Loss: 0.27655122, Accuracy: 0.9014, F1: 0.9441 Bal: 0.8837\n","Epoch 284:      TX: Train Loss: 0.0066, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5095, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.22081321, Acc: 0.92101711, F1: 0.95565985 Bal: 0.9121 - Val Loss: 0.28116649, Accuracy: 0.9119, F1: 0.9505 Bal: 0.8828\n","Epoch 285:      TX: Train Loss: 0.0065, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5100, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.22037978, Acc: 0.91136195, F1: 0.94991379 Bal: 0.9126 - Val Loss: 0.27687982, Accuracy: 0.9028, F1: 0.9450 Bal: 0.8835\n","Epoch 286:      TX: Train Loss: 0.0065, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5121, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21992233, Acc: 0.91894893, F1: 0.95442731 Bal: 0.9132 - Val Loss: 0.27979207, Accuracy: 0.9094, F1: 0.9490 Bal: 0.8830\n","Epoch 287:      TX: Train Loss: 0.0064, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5131, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21953221, Acc: 0.91482344, F1: 0.95197845 Bal: 0.9129 - Val Loss: 0.27791709, Accuracy: 0.9059, F1: 0.9468 Bal: 0.8833\n","Epoch 288:      TX: Train Loss: 0.0063, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5145, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21922870, Acc: 0.91582851, F1: 0.95257468 Bal: 0.9131 - Val Loss: 0.27826411, Accuracy: 0.9066, F1: 0.9472 Bal: 0.8829\n","Epoch 289:      TX: Train Loss: 0.0063, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5160, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21899863, Acc: 0.91822325, F1: 0.95399507 Bal: 0.9134 - Val Loss: 0.27926567, Accuracy: 0.9088, F1: 0.9486 Bal: 0.8832\n","Epoch 290:      TX: Train Loss: 0.0062, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5164, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21882121, Acc: 0.91331766, F1: 0.95107736 Bal: 0.9132 - Val Loss: 0.27728477, Accuracy: 0.9044, F1: 0.9459 Bal: 0.8831\n","Epoch 291:      TX: Train Loss: 0.0061, Acc: 0.9987, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5184, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21866390, Acc: 0.92048011, F1: 0.95533218 Bal: 0.9133 - Val Loss: 0.28050527, Accuracy: 0.9111, F1: 0.9500 Bal: 0.8828\n","Epoch 292:      TX: Train Loss: 0.0061, Acc: 0.9988, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5191, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21853040, Acc: 0.91175019, F1: 0.95013511 Bal: 0.9138 - Val Loss: 0.27672610, Accuracy: 0.9029, F1: 0.9450 Bal: 0.8834\n","Epoch 293:      TX: Train Loss: 0.0060, Acc: 0.9988, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5213, Accuracy: 0.9699, F1: 0.9833 Bal: 0.9156\n","           WALLETS: Train Loss: 0.21837570, Acc: 0.92215643, F1: 0.95632293 Bal: 0.9133 - Val Loss: 0.28147230, Accuracy: 0.9127, F1: 0.9509 Bal: 0.8822\n","Epoch 294:      TX: Train Loss: 0.0060, Acc: 0.9988, F1: 0.9993 Bal: 0.9993 - Val Loss: 0.5216, Accuracy: 0.9697, F1: 0.9832 Bal: 0.9154\n","           WALLETS: Train Loss: 0.21823870, Acc: 0.91037140, F1: 0.94930823 Bal: 0.9139 - Val Loss: 0.27634275, Accuracy: 0.9018, F1: 0.9444 Bal: 0.8833\n","Epoch 295:      TX: Train Loss: 0.0059, Acc: 0.9989, F1: 0.9994 Bal: 0.9994 - Val Loss: 0.5243, Accuracy: 0.9699, F1: 0.9833 Bal: 0.9156\n","           WALLETS: Train Loss: 0.21802519, Acc: 0.92321229, F1: 0.95694596 Bal: 0.9132 - Val Loss: 0.28210801, Accuracy: 0.9140, F1: 0.9517 Bal: 0.8819\n","Epoch 296:      TX: Train Loss: 0.0058, Acc: 0.9989, F1: 0.9994 Bal: 0.9994 - Val Loss: 0.5240, Accuracy: 0.9699, F1: 0.9833 Bal: 0.9156\n","           WALLETS: Train Loss: 0.21780546, Acc: 0.91004122, F1: 0.94910926 Bal: 0.9140 - Val Loss: 0.27625117, Accuracy: 0.9012, F1: 0.9440 Bal: 0.8836\n","Epoch 297:      TX: Train Loss: 0.0058, Acc: 0.9989, F1: 0.9994 Bal: 0.9994 - Val Loss: 0.5266, Accuracy: 0.9699, F1: 0.9833 Bal: 0.9156\n","           WALLETS: Train Loss: 0.21744744, Acc: 0.92329211, F1: 0.95699054 Bal: 0.9135 - Val Loss: 0.28207988, Accuracy: 0.9142, F1: 0.9518 Bal: 0.8823\n","Epoch 298:      TX: Train Loss: 0.0057, Acc: 0.9989, F1: 0.9994 Bal: 0.9994 - Val Loss: 0.5268, Accuracy: 0.9699, F1: 0.9833 Bal: 0.9156\n","           WALLETS: Train Loss: 0.21705566, Acc: 0.91112974, F1: 0.94975991 Bal: 0.9142 - Val Loss: 0.27655798, Accuracy: 0.9024, F1: 0.9447 Bal: 0.8833\n","Epoch 299:      TX: Train Loss: 0.0057, Acc: 0.9989, F1: 0.9994 Bal: 0.9994 - Val Loss: 0.5290, Accuracy: 0.9702, F1: 0.9834 Bal: 0.9157\n","           WALLETS: Train Loss: 0.21658771, Acc: 0.92202943, F1: 0.95624161 Bal: 0.9141 - Val Loss: 0.28102505, Accuracy: 0.9128, F1: 0.9510 Bal: 0.8825\n","Epoch 300:      TX: Train Loss: 0.0056, Acc: 0.9989, F1: 0.9994 Bal: 0.9994 - Val Loss: 0.5296, Accuracy: 0.9702, F1: 0.9834 Bal: 0.9157\n","           WALLETS: Train Loss: 0.21615520, Acc: 0.91435538, F1: 0.95168224 Bal: 0.9147 - Val Loss: 0.27729335, Accuracy: 0.9051, F1: 0.9463 Bal: 0.8835\n","{'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'SAGE', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 15, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99, 'epoch': 300, 'end': True}\n","Final_result for w\n","{'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'SAGE', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 15, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99, 'epoch': 300}\n","Epoch 281:\n","  TX:\n","   Train: Loss=0.0068, Acc=0.9987, F1=0.9993, Bal. Acc=0.9993\n","   Val:   Loss=0.5048, Acc=0.9693, F1=0.9829, Bal. Acc=0.9152\n","   Test:  Loss=0.5345, Acc=0.9650, F1=0.9806, Bal. Acc=0.9021\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.85      0.85       453\n","           1       0.98      0.98      0.98      4105\n","\n","    accuracy                           0.97      4558\n","   macro avg       0.91      0.92      0.91      4558\n","weighted avg       0.97      0.97      0.97      4558\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.82      0.82       665\n","           1       0.98      0.98      0.98      6172\n","\n","    accuracy                           0.97      6837\n","   macro avg       0.90      0.90      0.90      6837\n","weighted avg       0.97      0.97      0.97      6837\n","\n","  WALLETS:\n","   Train: Loss=0.22145954, Acc=0.91111885, F1=0.94977261, Bal. Acc=0.9121\n","   Val:   Loss=0.27700970, Acc=0.9030, F1=0.9451, Bal. Acc=0.8839\n","   Test:  Loss=0.28013599, Acc=0.9011, F1=0.9440, Bal. Acc=0.8792\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.86      0.58      2906\n","           1       0.99      0.91      0.95     33841\n","\n","    accuracy                           0.90     36747\n","   macro avg       0.71      0.88      0.76     36747\n","weighted avg       0.94      0.90      0.92     36747\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.85      0.58      4340\n","           1       0.99      0.91      0.94     50781\n","\n","    accuracy                           0.90     55121\n","   macro avg       0.71      0.88      0.76     55121\n","weighted avg       0.94      0.90      0.92     55121\n","\n","\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIoomnaeh74t1AKH1zYUzUr42JRiCeq-","timestamp":1739893838211}],"collapsed_sections":["_QWrR51eyGrX"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}