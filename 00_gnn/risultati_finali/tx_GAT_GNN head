{"cells":[{"cell_type":"markdown","source":["# Addestramento\n","Addestramento del migliore modello SAGE per calcolare le performance sul test set per la classificazione delle transazioni"],"metadata":{"id":"llE922-B6T-U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iidXjXM7pXO"},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","!pip install torch_geometric\n","#!pip install torch-sparse\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","import os.path as osp\n","import torch\n","import warnings\n","from torch_geometric.data import Data, HeteroData\n","from torch_geometric.transforms import RandomNodeSplit\n","from torch_geometric.nn import GCNConv, GATConv, SAGEConv, ChebConv\n","import torch_geometric.nn as pyg_nn\n","import torch.nn as nn\n","import torch_geometric.utils as pyg_utils\n","from torch.nn import Module, Linear\n","import torch.nn.functional as F\n","from sklearn.metrics import precision_recall_fscore_support, f1_score, classification_report\n","from torch_geometric.seed import seed_everything\n","import joblib\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","warnings.simplefilter(action='ignore')\n","SEED = 51\n","FILEPATH_TX = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/tx_result_3.csv\"\n","FILEPATH_WALLET = \"/content/drive/MyDrive/blockchain/00_gnn/risultati_finali/final_res/w_result_3.csv\"\n","\n","base_path = \"/content/drive/MyDrive/blockchain/E++/\"\n","path_comb = '/content/drive/MyDrive/blockchain/00_gnn/combination_do.csv'\n","\n","type_classification = 'tx'"]},{"cell_type":"markdown","metadata":{"id":"XOk9zPoYDC5I"},"source":["##Crea db vuoto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCrI1kO2CYxi"},"outputs":[],"source":["def create_df():\n","  # Crea DataFrame vuoti\n","  df_tx = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","  df_wallet = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","\n","  # Salva i DataFrame come file CSV\n","  df_tx.to_csv(FILEPATH_TX, index=False)\n","  df_wallet.to_csv(FILEPATH_WALLET, index=False)\n","\n","#create_df()"]},{"cell_type":"markdown","metadata":{"id":"W1Syx8hzDPvM"},"source":["##Carica dati"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qobTcbrVqpwG"},"outputs":[],"source":["def load_data():\n","    # Loading transactions\n","\n","    #Reading edges, features and classes from transaction files (as done with the original dataset)\n","    df_edges_tx = pd.read_csv(osp.join(base_path, \"txs_edgelist.csv\"))\n","    df_features_tx = pd.read_csv(osp.join(base_path, \"txs_features.csv\"), header=None)\n","    df_classes_tx = pd.read_csv(osp.join(base_path, \"txs_classes.csv\"))\n","\n","    #Columns naming based on index\n","    colNames1_tx = {'0': 'txId', 1: \"Time step\"}\n","    colNames2_tx = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(94)}\n","    colNames3_tx = {str(ii+96): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n","\n","    colNames_tx = dict(colNames1_tx, **colNames2_tx, **colNames3_tx)\n","    colNames_tx = {int(jj): item_kk for jj, item_kk in colNames_tx.items()}\n","\n","    # Rename feature columns\n","    df_features_tx = df_features_tx.rename(columns=colNames_tx)\n","\n","    # Map unknown class to '3'\n","    df_classes_tx.loc[df_classes_tx['class'] == 'unknown', 'class'] = '3'\n","\n","    # Merge classes and features in one Dataframe\n","    df_class_feature_tx = pd.merge(df_classes_tx, df_features_tx)\n","\n","    # Exclude records with unknown class transaction\n","    df_class_feature_tx = df_class_feature_tx[df_class_feature_tx['class'] != 3]\n","\n","    # Build Dataframe with head and tail of transactions (edges)\n","    known_txs = df_class_feature_tx[\"txId\"].values\n","    df_edges_tx = df_edges_tx[(df_edges_tx[\"txId1\"].isin(known_txs)) & (df_edges_tx[\"txId2\"].isin(known_txs))]\n","\n","    # Build indices for features and edge types\n","    features_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"txId\"].unique()))}\n","    class_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_tx[\"txId\"] = df_class_feature_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_class_feature_tx[\"class\"] = df_class_feature_tx[\"class\"].apply(lambda name: class_idx_tx[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_tx[\"txId1\"] = df_edges_tx[\"txId1\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx[\"txId2\"] = df_edges_tx[\"txId2\"].apply(lambda name: features_idx_tx[name])\n","\n","    # Loading wallets\n","\n","    # From file\n","    df_edges_wallet = pd.read_csv(osp.join(base_path, \"AddrAddr_edgelist.csv\"))\n","    df_class_feature_wallet = pd.read_csv(osp.join(base_path, \"wallets_features_classes_combined.csv\"))\n","\n","    # Exclude records with unknown class transaction\n","    #print(df_class_feature_wallet.shape)\n","    df_class_feature_wallet = df_class_feature_wallet[df_class_feature_wallet[\"class\"] != 3]\n","    #print(df_class_feature_wallet.shape)\n","\n","    # Build Dataframe with head and tail of AddrToAddr (edges)\n","    known_wallets = df_class_feature_wallet[\"address\"].values\n","    df_edges_wallet = df_edges_wallet[(df_edges_wallet[\"input_address\"].isin(known_wallets)) & (df_edges_wallet[\"output_address\"].isin(known_wallets))]\n","\n","    # Building indices for features and edge types\n","    features_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"address\"].unique()))}\n","    class_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_wallet[\"address\"] = df_class_feature_wallet[\"address\"].apply(lambda name: features_idx_wallet[name])\n","    df_class_feature_wallet[\"class\"] = df_class_feature_wallet[\"class\"].apply(lambda name: class_idx_wallet[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet[\"input_address\"] = df_edges_wallet[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet[\"output_address\"] = df_edges_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    # Loading AddrTx and TxAddr\n","\n","    # From file\n","    df_edges_wallet_tx = pd.read_csv(osp.join(base_path, \"AddrTx_edgelist.csv\"))\n","    df_edges_tx_wallet = pd.read_csv(osp.join(base_path, \"TxAddr_edgelist.csv\"))\n","\n","    # Build Dataframe with head and tail of AddrTx (edges)\n","    df_edges_wallet_tx = df_edges_wallet_tx[(df_edges_wallet_tx[\"input_address\"].isin(known_wallets)) & df_edges_wallet_tx[\"txId\"].isin(known_txs)]\n","    df_edges_tx_wallet = df_edges_tx_wallet[(df_edges_tx_wallet[\"txId\"].isin(known_txs)) & df_edges_tx_wallet[\"output_address\"].isin(known_wallets)]\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet_tx[\"input_address\"] = df_edges_wallet_tx[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet_tx[\"txId\"] = df_edges_wallet_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"txId\"] = df_edges_tx_wallet[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"output_address\"] = df_edges_tx_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    return df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet\n","\n","def data_to_pyg(df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet):\n","    data = HeteroData()\n","\n","    # Defining PyG objects for transactions\n","    df_class_feature_tx = df_class_feature_tx.fillna(0)\n","    data['tx'].x = torch.tensor(df_class_feature_tx.iloc[:, 3:].values, dtype=torch.float)\n","    data['tx'].y = torch.tensor(df_class_feature_tx[\"class\"].values, dtype=torch.long)\n","    data['tx','is_related_to','tx'].edge_index = torch.tensor([df_edges_tx[\"txId1\"].values,\n","                            df_edges_tx[\"txId2\"].values], dtype=torch.int64)\n","    #data['tx'] = random_node_split(num_val=0.15, num_test=0.2)(data['tx'])\n","    # Defining PyG objects for wallets\n","    data['wallet'].x = torch.tensor(df_class_feature_wallet.iloc[:, 3:].values, dtype=torch.float)\n","    data['wallet'].y = torch.tensor(df_class_feature_wallet[\"class\"].values, dtype=torch.long)\n","    data['wallet','interacts_with','wallet'].edge_index = torch.tensor([df_edges_wallet[\"input_address\"].values,\n","                            df_edges_wallet[\"output_address\"].values], dtype=torch.int64)\n","    #data['wallet'] = random_node_split(num_val=0.15, num_test=0.2)(data['wallet'])\n","    # Defining PyG objects for cross-edges\n","    data['wallet','performs','tx'].edge_index = torch.tensor([df_edges_wallet_tx[\"input_address\"].values,\n","                                         df_edges_wallet_tx[\"txId\"].values], dtype=torch.int64)\n","\n","    data['tx', 'flows_into', 'wallet'].edge_index = torch.tensor([df_edges_tx_wallet[\"txId\"].values,\n","                                         df_edges_tx_wallet[\"output_address\"].values], dtype=torch.int64)\n","\n","    # Impostare il seed per la divisione del dataset\n","    return RandomNodeSplit(num_val=0.10, num_test=0.15)(data)"]},{"cell_type":"markdown","metadata":{"id":"OhFPkCpExzOL"},"source":["##Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQfEkZ1nx44T"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do2dQUF-x7hj"},"outputs":[],"source":["#NEW -> SU RANGE\n","# Utility per conversione a tensor\n","def to_tensor(arr):\n","    return torch.tensor(arr, dtype=torch.float).to(device)\n","\n","def scale_features(data, method=\"standard\"):\n","  if method == 'no':\n","    return data\n","\n","  # Scaling per training\n","  data['tx'].x[data['tx'].train_mask] = to_tensor(\n","      scale_train_data(data['tx'].x[data['tx'].train_mask].cpu().numpy(), method, 'tx')\n","  )\n","  data['wallet'].x[data['wallet'].train_mask] = to_tensor(\n","      scale_train_data(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), method, 'wallet')\n","  )\n","\n","  # Scaling per validation\n","  data['tx'].x[data['tx'].val_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].val_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].val_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), method, 'wallet'))\n","\n","  data['tx'].x[data['tx'].test_mask] = to_tensor(scale_validation_data(data['tx'].x[data['tx'].test_mask].cpu().numpy(), method, 'tx'))\n","  data['wallet'].x[data['wallet'].test_mask] = to_tensor(scale_validation_data(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), method, 'wallet'))\n","  return data\n","\n","def scale_train_data(train, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = StandardScaler()\n","        scaled_train = scaler.fit_transform(train)  # Scala tutte le colonne\n","        joblib.dump(scaler, f\"scaler_standard_{df}.pkl\")\n","\n","        if 'l2' in scaling_method:\n","            norm = Normalizer(norm='l2')\n","            scaled_train = norm.fit_transform(scaled_train)\n","            joblib.dump(norm, f\"scaler_l2_{df}.pkl\")\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_train\n","\n","def scale_validation_data(val, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = joblib.load(f\"scaler_standard_{df}.pkl\")\n","        scaled_val = scaler.transform(val)  # Scala tutte le colonne\n","\n","        if 'l2' in scaling_method:\n","            norm = joblib.load(f\"scaler_l2_{df}.pkl\")\n","            scaled_val = norm.transform(scaled_val)\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_val"]},{"cell_type":"code","source":["def dimentional_reduction(data, dim_reduction, pca_threshold):\n","    if dim_reduction == 'no':\n","        return data\n","    elif dim_reduction == 'pca':\n","        data1 = copy.deepcopy(data)\n","\n","        transformed_tx_data = apply_pca_train(data['tx'].x[data['tx'].train_mask].cpu().numpy(), 'tx', pca_threshold)\n","        transformed_wallet_data = apply_pca_train(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), 'wallet', pca_threshold)\n","\n","        data1['tx'].x = torch.zeros((data['tx'].x.shape[0], transformed_tx_data.shape[1]), dtype=torch.float, device=device)\n","        data1['wallet'].x = torch.zeros((data['wallet'].x.shape[0], transformed_wallet_data.shape[1]), dtype=torch.float, device=device)\n","\n","        data1['tx'].x[data['tx'].train_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].train_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].val_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].val_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].val_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].test_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].test_mask].cpu().numpy(), 'wallet')\n","        data1['tx'].x[data['tx'].test_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].test_mask] = to_tensor(transformed_wallet_data)\n","\n","        return data1\n","\n","def apply_pca_train(train, df, pca_threshold=0.99):\n","    pca = PCA(random_state=SEED)\n","    pca.fit(train)\n","\n","    # Selezione componenti principali\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","\n","    pca = PCA(n_components=n_components, random_state=SEED)\n","    transformed_data = pca.fit_transform(train).astype(np.float32)\n","\n","    joblib.dump(pca, f\"pca_model_{df}.pkl\")\n","    print(f\"  Numero di componenti principali per {df}: {pca.n_components_}\")\n","\n","    return transformed_data\n","\n","def apply_pca_validation(val, df):\n","    pca = joblib.load(f\"pca_model_{df}.pkl\")\n","    transformed_data = pca.transform(val).astype(np.float32)\n","    return transformed_data"],"metadata":{"id":"PVRMmabn21eP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CaNNtrNLDSdI"},"source":["##Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbfXKtdynFBL"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, Dropout\n","from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, TransformerConv\n","import random\n","from itertools import product\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxfbqge_p64q"},"outputs":[],"source":["def set_seed(seed = 51):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # Per più GPU\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    seed_everything(seed)\n","device = \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiBF-BZYfq7b"},"outputs":[],"source":["class ResidualHeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0.5, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.skips = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1 # Define heads\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","            self.skips.append(Linear(hidden_channels * heads, hidden_channels * heads)) # Fix: Linear layer expects the output of conv\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv, skip in zip(self.convs, self.skips):\n","            x_dict_new = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(F.relu(x + skip(x_dict_new[key]))) for key, x in x_dict_new.items()}  # Residual + Dropout\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdcebmslq5D0"},"outputs":[],"source":["class HeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1  # Definiamo i heads solo se necessario\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv in self.convs:\n","            x_dict = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJEJVt5l04Eb"},"outputs":[],"source":["def is_combination_tested(filepath, new_row):\n","    existing_results = pd.read_csv(filepath)\n","\n","    # Identifica le colonne comuni tra il dataset e new_row\n","    dataset_columns = set(existing_results.columns)\n","    comparison_columns = [col for col in new_row.keys() if col not in ['end', 'num_epoch', 'epoch']]\n","\n","    # Filtra le combinazioni\n","    filtered_results = existing_results.copy()\n","    #filtered_results = filtered_results[filtered_results['end'] == True]\n","    filtered_results = filtered_results[filtered_results['num_epoch'] >= new_row['num_epoch']]\n","\n","    for col in comparison_columns:\n","        if col in dataset_columns:\n","            # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","            filtered_results = filtered_results[\n","                (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","            ]\n","\n","    return not filtered_results.empty\n","\n","def append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, end=False):\n","  def append_and_save_result(filepath, new_row, end=False):\n","    new_row['end'] = end\n","    # Leggi i risultati esistenti\n","    results_df = pd.read_csv(filepath)\n","    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","    results_df.to_csv(filepath, index=False)\n","\n","  append_and_save_result(FILEPATH_TX, params_tx, end)\n","  append_and_save_result(FILEPATH_WALLET, params_wallet, end)\n","  if end:\n","    df_comb = pd.read_csv(path_comb)\n","    filtered_params = {key: params_tx[key] for key in params_tx if \"train\" not in key and \"val\" not in key}\n","    print(filtered_params)\n","    df_comb = pd.concat([df_comb, pd.DataFrame([filtered_params])], ignore_index=True)\n","    df_comb.to_csv(path_comb, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vo1zTF6zxQR"},"outputs":[],"source":["def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def eval(model, data, out_tx, out_wallet, params):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalità di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  loss = params_tx['train_loss'] + params_wallet['train_loss']\n","\n","  with torch.no_grad():\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for wallets\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    print(f\"Epoch {str(params['epoch']).zfill(3)}:      TX: Train Loss: {params_tx['train_loss']:.4f}, Acc: {params_tx['train_acc']:.4f}, F1: {params_tx['train_f1']:.4f} Bal: {params_tx['train_balanced_acc']:.4f} - Val Loss: {params_tx['val_loss']:.4f}, Accuracy: {params_tx['val_acc']:.4f}, F1: {params_tx['val_f1']:.4f} Bal: {params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"           WALLETS: Train Loss: {params_wallet['train_loss']:.8f}, Acc: {params_wallet['train_acc']:.8f}, F1: {params_wallet['train_f1']:.8f} Bal: {params_wallet['train_balanced_acc']:.4f} - Val Loss: {params_wallet['val_loss']:.8f}, Accuracy: {params_wallet['val_acc']:.4f}, F1: {params_wallet['val_f1']:.4f} Bal: {params_wallet['val_balanced_acc']:.4f}\")\n","\n","  return loss, params_tx, params_wallet\n","\n","\n","def eval_total(model, data, out_tx, out_wallet, params, best_epoch):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalitÃ  di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","  tx_mask_test = data['tx'].test_mask\n","  wallet_mask_test = data['wallet'].test_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask].cpu())  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask].cpu())  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  with torch.no_grad():\n","    # Calculate metrics for validation\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val].cpu())  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    report_tx_val = classification_report(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val].cpu())  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    report_wallet_val = classification_report(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for test\n","    params_tx['test_loss'] = criterion(out_tx[tx_mask_test], data['tx'].y[tx_mask_test].cpu())  # Convert to scalar\n","    params_tx['test_acc'] = accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_precision'] = precision_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_recall'] = recall_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_f1'] = f1_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    params_tx['test_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","    report_tx_test = classification_report(data['tx'].y[tx_mask_test].cpu(), out_tx[tx_mask_test].argmax(dim=1).cpu())\n","\n","    params_wallet['test_loss'] = criterion(out_wallet[wallet_mask_test], data['wallet'].y[wallet_mask_test].cpu())  # Convert to scalar\n","    params_wallet['test_acc'] = accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_precision'] = precision_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_recall'] = recall_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_f1'] = f1_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    params_wallet['test_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","    report_wallet_test = classification_report(data['wallet'].y[wallet_mask_test].cpu(), out_wallet[wallet_mask_test].argmax(dim=1).cpu())\n","\n","    # Stampa delle metriche con formattazione migliorata\n","    print('Final_result for '+type_classification)\n","    print(params)\n","    print(f\"Epoch {best_epoch}:\")\n","    print(\"  TX:\")\n","    print(f\"   Train: Loss={params_tx['train_loss']:.4f}, Acc={params_tx['train_acc']:.4f}, F1={params_tx['train_f1']:.4f}, Bal. Acc={params_tx['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_tx['val_loss']:.4f}, Acc={params_tx['val_acc']:.4f}, F1={params_tx['val_f1']:.4f}, Bal. Acc={params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_tx['test_loss']:.4f}, Acc={params_tx['test_acc']:.4f}, F1={params_tx['test_f1']:.4f}, Bal. Acc={params_tx['test_balanced_acc']:.4f}\")\n","    print(report_tx_val)\n","    print(report_tx_test)\n","    print(\"  WALLETS:\")\n","    print(f\"   Train: Loss={params_wallet['train_loss']:.8f}, Acc={params_wallet['train_acc']:.8f}, F1={params_wallet['train_f1']:.8f}, Bal. Acc={params_wallet['train_balanced_acc']:.4f}\")\n","    print(f\"   Val:   Loss={params_wallet['val_loss']:.8f}, Acc={params_wallet['val_acc']:.4f}, F1={params_wallet['val_f1']:.4f}, Bal. Acc={params_wallet['val_balanced_acc']:.4f}\")\n","    print(f\"   Test:  Loss={params_wallet['test_loss']:.8f}, Acc={params_wallet['test_acc']:.4f}, F1={params_wallet['test_f1']:.4f}, Bal. Acc={params_wallet['test_balanced_acc']:.4f}\")\n","    print(report_wallet_val)\n","    print(report_wallet_test)\n","    print()\n","\n","def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def train(model, data, params):\n","    best_model = None\n","    best_epoch = None\n","\n","    if params['optimizer'] == 'Adam':\n","      optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    elif params['optimizer'] == 'AdamW':\n","      optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    else:\n","      optimizer = None\n","\n","    if params['lr_scheduler'] == 'ReduceLROnPlateau':\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=params['factor'], patience=params['p'])\n","    elif params['lr_scheduler'] == 'CosineAnnealingLR':\n","      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=params['eta_min'])\n","    elif params['lr_scheduler'] == 'StepLR':\n","      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n","    else:\n","      scheduler = None\n","\n","    print(f'Combinazione: {params}')\n","    model.train()\n","    tx_mask = data['tx'].train_mask\n","    wallet_mask = data['wallet'].train_mask\n","    tx_mask_val =  data['tx'].val_mask\n","    wallet_mask_val = data['wallet'].val_mask\n","\n","    best_val_tx_acc = 0\n","    best_val_wallet_acc = 0\n","\n","    best_val_tx_loss = float('inf')\n","    best_val_wallet_loss = float('inf')\n","    patience = params['patience']\n","    epochs_since_best = 0\n","\n","    for epoch in range(params['num_epoch']):\n","        params['epoch'] = epoch+1\n","        optimizer.zero_grad()\n","        out_tx, out_wallet = model(data.x_dict, data.edge_index_dict)\n","        loss, params_tx, params_wallet = eval(model, data, out_tx, out_wallet, params)\n","\n","        val_tx_loss = params_tx['val_loss']\n","        val_wallet_loss = params_wallet['val_loss']\n","        val_tx_acc = params_tx['val_balanced_acc']\n","        val_wallet_acc = params_wallet['val_balanced_acc']\n","\n","        # Check if validation loss has improved\n","        if val_tx_loss < best_val_tx_loss or val_wallet_loss < best_val_wallet_loss:\n","            best_val_tx_loss = val_tx_loss\n","            best_val_wallet_loss = val_wallet_loss\n","            epochs_since_best = 0\n","        else:\n","            epochs_since_best += 1\n","\n","        # Check if early stopping criteria is met\n","        if epochs_since_best >= patience:\n","            print(f'Early stopping at epoch {epoch}')\n","            append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, True)\n","            out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","            eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","            return best_model\n","\n","        if type_classification == 'w':\n","          if best_val_wallet_acc < val_wallet_acc:\n","            best_val_wallet_acc = val_wallet_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        elif type_classification == 'tx':\n","          if best_val_tx_acc < val_tx_acc:\n","            best_val_tx_acc = val_tx_acc\n","            best_model = copy.deepcopy(model)\n","            best_epoch = epoch+1\n","\n","        else:\n","          print('Definisci modello da considerare')\n","          raise ValueError\n","\n","        loss.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","        scheduler.step(loss)\n","\n","        append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, params['epoch']==params['num_epoch'])\n","\n","    out_tx, out_wallet = best_model(data.x_dict, data.edge_index_dict)\n","    eval_total(best_model, data, out_tx, out_wallet, params, best_epoch)\n","    return model\n","\n","\n","def train_grid(data_full, param_grid, scalers, dim_reductions, pca_thresholds):\n","    best_model = None\n","    best_f1 = 0\n","\n","    keys, values = zip(*param_grid.items())\n","    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","    combination_counter = 0\n","    total_combinations = len(param_combinations) * len(scalers) * len(dim_reductions) * len(pca_thresholds)\n","\n","    for scaler in scalers:\n","      data = scale_features(data_full.clone(), scaler)\n","      for dim_reduction in dim_reductions:\n","        for pca_threshold in pca_thresholds:\n","          data = dimentional_reduction(data, dim_reduction, pca_threshold)\n","\n","          for params in param_combinations:\n","            combination_counter += 1\n","\n","            set_seed(SEED)\n","            params['scaler'] = scaler\n","            params['dim_reduction'] = dim_reduction # Fixed the typo here: 'dim_reduction' instead of 'dim_reducition'\n","\n","            if params['lr_scheduler'] != 'ReduceLROnPlateau':\n","              params['p'] = None\n","              params['factor'] = None\n","            elif params['lr_scheduler'] != 'CosineAnnealingLR':\n","              params['T_max'] = None\n","              params['eta_min'] = None\n","\n","            if params['conv_type'] != 'Transformer':\n","              params['num_head'] = None\n","\n","            if dim_reduction == 'no':\n","              params['pca_threshold'] = None\n","            else:\n","              params['pca_threshold'] = pca_threshold\n","\n","            if True: #not is_combination_tested(path_comb, params):\n","              print(f\"  Combinazione {combination_counter}/{total_combinations}\")  # Print the counter\n","              model = None\n","              if params[ 'type_model'] == 'HeteroGNN':\n","                model = HeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              elif params[ 'type_model'] == 'ResidualHeteroGNN':\n","                model = ResidualHeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              model = train(model, data, params)\n","            else:\n","              print(f\"  Configurazione {combination_counter}/{total_combinations} già testata, salto...\")\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOYfyiGS4aMO"},"outputs":[],"source":["set_seed(SEED)\n","data = data_to_pyg(*load_data())"]},{"cell_type":"markdown","source":["# Risultati"],"metadata":{"id":"NUQfJRR56Els"}},{"cell_type":"code","source":["hyperparams = {\n","    \"hidden_channels\": [128],\n","    'num_head': ['/'],\n","    \"num_layers\": [2],\n","    \"num_epoch\": [300],\n","    \"patience\": [50],\n","    \"lr\": [0.001],\n","    \"weight_decay\": [1e-5],\n","    \"dropout\": [0],\n","    \"conv_type\": ['GAT'],\n","    \"p\": ['/'],\n","    \"factor\": ['/'],\n","    \"eta_min\": [1e-5],\n","    \"T_max\": [10], #10 15\n","    \"aggr\": ['sum'], #,\n","    'lr_scheduler':['CosineAnnealingLR'],\n","    'optimizer': ['Adam'], #Adam\n","    'type_model':['HeteroGNN'],\n","}\n","\n","scaler = ['standard_l2']\n","dim_reduction=['pca']\n","pca_threshold=[0.99]\n","best_model = train_grid(data, hyperparams, scaler, dim_reduction, pca_threshold)"],"metadata":{"id":"S-ezqxPia_cD","executionInfo":{"status":"ok","timestamp":1742733344719,"user_tz":-60,"elapsed":2240651,"user":{"displayName":"Claudia Brunetti","userId":"17627296144817418720"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eea1bbef-ad9b-4b43-9abd-ce3fbe75077e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Numero di componenti principali per tx: 73\n","  Numero di componenti principali per wallet: 22\n","  Combinazione 1/1\n","Combinazione: {'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'GAT', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 10, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99}\n","Epoch 001:      TX: Train Loss: 0.6938, Acc: 0.1055, F1: 0.0142 Bal: 0.4991 - Val Loss: 0.6938, Accuracy: 0.1075, F1: 0.0188 Bal: 0.5025\n","           WALLETS: Train Loss: 0.68794990, Acc: 0.92245396, F1: 0.95966299 Bal: 0.5000 - Val Loss: 0.68845993, Accuracy: 0.9208, F1: 0.9588 Bal: 0.4999\n","Epoch 002:      TX: Train Loss: 0.6931, Acc: 0.1612, F1: 0.1405 Bal: 0.5019 - Val Loss: 0.6932, Accuracy: 0.1630, F1: 0.1452 Bal: 0.5019\n","           WALLETS: Train Loss: 0.68754309, Acc: 0.92251201, F1: 0.95969441 Bal: 0.5000 - Val Loss: 0.68816900, Accuracy: 0.9209, F1: 0.9588 Bal: 0.5000\n","Epoch 003:      TX: Train Loss: 0.6926, Acc: 0.3036, F1: 0.3938 Bal: 0.5130 - Val Loss: 0.6929, Accuracy: 0.2986, F1: 0.3902 Bal: 0.4977\n","           WALLETS: Train Loss: 0.68720269, Acc: 0.92251564, F1: 0.95969637 Bal: 0.5000 - Val Loss: 0.68792027, Accuracy: 0.9209, F1: 0.9588 Bal: 0.5000\n","Epoch 004:      TX: Train Loss: 0.6921, Acc: 0.6509, F1: 0.7784 Bal: 0.5302 - Val Loss: 0.6926, Accuracy: 0.6474, F1: 0.7758 Bal: 0.5274\n","           WALLETS: Train Loss: 0.68685180, Acc: 0.92251564, F1: 0.95969637 Bal: 0.5000 - Val Loss: 0.68765759, Accuracy: 0.9209, F1: 0.9588 Bal: 0.5000\n","Epoch 005:      TX: Train Loss: 0.6917, Acc: 0.6790, F1: 0.8002 Bal: 0.5370 - Val Loss: 0.6924, Accuracy: 0.6781, F1: 0.7997 Bal: 0.5356\n","           WALLETS: Train Loss: 0.68649101, Acc: 0.92250112, F1: 0.95968821 Bal: 0.5000 - Val Loss: 0.68738359, Accuracy: 0.9209, F1: 0.9588 Bal: 0.5000\n","Epoch 006:      TX: Train Loss: 0.6912, Acc: 0.6663, F1: 0.7901 Bal: 0.5401 - Val Loss: 0.6921, Accuracy: 0.6670, F1: 0.7908 Bal: 0.5382\n","           WALLETS: Train Loss: 0.68612921, Acc: 0.92246484, F1: 0.95966842 Bal: 0.5000 - Val Loss: 0.68711048, Accuracy: 0.9209, F1: 0.9588 Bal: 0.5000\n","Epoch 007:      TX: Train Loss: 0.6907, Acc: 0.6407, F1: 0.7691 Bal: 0.5439 - Val Loss: 0.6919, Accuracy: 0.6380, F1: 0.7672 Bal: 0.5398\n","           WALLETS: Train Loss: 0.68576568, Acc: 0.92235962, F1: 0.95961056 Bal: 0.5001 - Val Loss: 0.68683571, Accuracy: 0.9208, F1: 0.9588 Bal: 0.5000\n","Epoch 008:      TX: Train Loss: 0.6902, Acc: 0.6131, F1: 0.7458 Bal: 0.5436 - Val Loss: 0.6917, Accuracy: 0.6082, F1: 0.7424 Bal: 0.5321\n","           WALLETS: Train Loss: 0.68539214, Acc: 0.92204032, F1: 0.95943543 Bal: 0.5003 - Val Loss: 0.68655372, Accuracy: 0.9204, F1: 0.9586 Bal: 0.4997\n","Epoch 009:      TX: Train Loss: 0.6897, Acc: 0.5895, F1: 0.7250 Bal: 0.5423 - Val Loss: 0.6915, Accuracy: 0.5895, F1: 0.7255 Bal: 0.5384\n","           WALLETS: Train Loss: 0.68502289, Acc: 0.92187341, F1: 0.95934321 Bal: 0.5004 - Val Loss: 0.68628007, Accuracy: 0.9201, F1: 0.9584 Bal: 0.4997\n","Epoch 010:      TX: Train Loss: 0.6892, Acc: 0.5761, F1: 0.7127 Bal: 0.5444 - Val Loss: 0.6913, Accuracy: 0.5768, F1: 0.7138 Bal: 0.5402\n","           WALLETS: Train Loss: 0.68466091, Acc: 0.92168836, F1: 0.95924191 Bal: 0.5005 - Val Loss: 0.68601602, Accuracy: 0.9199, F1: 0.9583 Bal: 0.4996\n","Epoch 011:      TX: Train Loss: 0.6887, Acc: 0.5819, F1: 0.7179 Bal: 0.5455 - Val Loss: 0.6911, Accuracy: 0.5827, F1: 0.7191 Bal: 0.5405\n","           WALLETS: Train Loss: 0.68429661, Acc: 0.92155411, F1: 0.95916795 Bal: 0.5006 - Val Loss: 0.68575257, Accuracy: 0.9198, F1: 0.9582 Bal: 0.5002\n","Epoch 012:      TX: Train Loss: 0.6881, Acc: 0.5984, F1: 0.7327 Bal: 0.5458 - Val Loss: 0.6908, Accuracy: 0.5981, F1: 0.7328 Bal: 0.5422\n","           WALLETS: Train Loss: 0.68394256, Acc: 0.92115862, F1: 0.95895085 Bal: 0.5007 - Val Loss: 0.68550038, Accuracy: 0.9194, F1: 0.9580 Bal: 0.5003\n","Epoch 013:      TX: Train Loss: 0.6875, Acc: 0.6124, F1: 0.7448 Bal: 0.5481 - Val Loss: 0.6905, Accuracy: 0.6130, F1: 0.7457 Bal: 0.5436\n","           WALLETS: Train Loss: 0.68359929, Acc: 0.92060347, F1: 0.95864642 Bal: 0.5009 - Val Loss: 0.68525851, Accuracy: 0.9188, F1: 0.9577 Bal: 0.5004\n","Epoch 014:      TX: Train Loss: 0.6868, Acc: 0.6239, F1: 0.7547 Bal: 0.5483 - Val Loss: 0.6903, Accuracy: 0.6237, F1: 0.7551 Bal: 0.5417\n","           WALLETS: Train Loss: 0.68326771, Acc: 0.91985603, F1: 0.95823264 Bal: 0.5016 - Val Loss: 0.68502706, Accuracy: 0.9178, F1: 0.9571 Bal: 0.5002\n","Epoch 015:      TX: Train Loss: 0.6862, Acc: 0.6357, F1: 0.7647 Bal: 0.5482 - Val Loss: 0.6900, Accuracy: 0.6349, F1: 0.7647 Bal: 0.5391\n","           WALLETS: Train Loss: 0.68294555, Acc: 0.91889087, F1: 0.95770293 Bal: 0.5018 - Val Loss: 0.68480718, Accuracy: 0.9169, F1: 0.9566 Bal: 0.5003\n","Epoch 016:      TX: Train Loss: 0.6855, Acc: 0.6407, F1: 0.7688 Bal: 0.5482 - Val Loss: 0.6898, Accuracy: 0.6395, F1: 0.7686 Bal: 0.5377\n","           WALLETS: Train Loss: 0.68263996, Acc: 0.91770802, F1: 0.95705261 Bal: 0.5021 - Val Loss: 0.68460643, Accuracy: 0.9159, F1: 0.9561 Bal: 0.5015\n","Epoch 017:      TX: Train Loss: 0.6847, Acc: 0.6413, F1: 0.7694 Bal: 0.5479 - Val Loss: 0.6895, Accuracy: 0.6395, F1: 0.7688 Bal: 0.5357\n","           WALLETS: Train Loss: 0.68235117, Acc: 0.91623126, F1: 0.95623790 Bal: 0.5026 - Val Loss: 0.68442398, Accuracy: 0.9139, F1: 0.9550 Bal: 0.5009\n","Epoch 018:      TX: Train Loss: 0.6840, Acc: 0.6327, F1: 0.7620 Bal: 0.5510 - Val Loss: 0.6892, Accuracy: 0.6321, F1: 0.7624 Bal: 0.5385\n","           WALLETS: Train Loss: 0.68207610, Acc: 0.91491415, F1: 0.95550909 Bal: 0.5032 - Val Loss: 0.68425745, Accuracy: 0.9120, F1: 0.9540 Bal: 0.5002\n","Epoch 019:      TX: Train Loss: 0.6832, Acc: 0.6289, F1: 0.7587 Bal: 0.5509 - Val Loss: 0.6890, Accuracy: 0.6297, F1: 0.7600 Bal: 0.5430\n","           WALLETS: Train Loss: 0.68182391, Acc: 0.91383289, F1: 0.95491034 Bal: 0.5036 - Val Loss: 0.68411535, Accuracy: 0.9107, F1: 0.9532 Bal: 0.5003\n","Epoch 020:      TX: Train Loss: 0.6824, Acc: 0.6254, F1: 0.7557 Bal: 0.5517 - Val Loss: 0.6888, Accuracy: 0.6288, F1: 0.7590 Bal: 0.5465\n","           WALLETS: Train Loss: 0.68159372, Acc: 0.91299110, F1: 0.95444217 Bal: 0.5041 - Val Loss: 0.68399501, Accuracy: 0.9096, F1: 0.9526 Bal: 0.5000\n","Epoch 021:      TX: Train Loss: 0.6815, Acc: 0.6248, F1: 0.7552 Bal: 0.5521 - Val Loss: 0.6886, Accuracy: 0.6279, F1: 0.7585 Bal: 0.5440\n","           WALLETS: Train Loss: 0.68138725, Acc: 0.91220737, F1: 0.95400665 Bal: 0.5045 - Val Loss: 0.68389285, Accuracy: 0.9087, F1: 0.9521 Bal: 0.5004\n","Epoch 022:      TX: Train Loss: 0.6807, Acc: 0.6277, F1: 0.7576 Bal: 0.5536 - Val Loss: 0.6885, Accuracy: 0.6290, F1: 0.7594 Bal: 0.5436\n","           WALLETS: Train Loss: 0.68120569, Acc: 0.91142727, F1: 0.95357245 Bal: 0.5048 - Val Loss: 0.68381166, Accuracy: 0.9079, F1: 0.9517 Bal: 0.5006\n","Epoch 023:      TX: Train Loss: 0.6798, Acc: 0.6315, F1: 0.7607 Bal: 0.5547 - Val Loss: 0.6883, Accuracy: 0.6327, F1: 0.7625 Bal: 0.5447\n","           WALLETS: Train Loss: 0.68104768, Acc: 0.91055645, F1: 0.95308727 Bal: 0.5052 - Val Loss: 0.68375099, Accuracy: 0.9071, F1: 0.9512 Bal: 0.5002\n","Epoch 024:      TX: Train Loss: 0.6790, Acc: 0.6359, F1: 0.7643 Bal: 0.5557 - Val Loss: 0.6882, Accuracy: 0.6384, F1: 0.7674 Bal: 0.5430\n","           WALLETS: Train Loss: 0.68091470, Acc: 0.91027344, F1: 0.95292987 Bal: 0.5053 - Val Loss: 0.68371397, Accuracy: 0.9069, F1: 0.9511 Bal: 0.5001\n","Epoch 025:      TX: Train Loss: 0.6781, Acc: 0.6378, F1: 0.7659 Bal: 0.5564 - Val Loss: 0.6882, Accuracy: 0.6387, F1: 0.7675 Bal: 0.5431\n","           WALLETS: Train Loss: 0.68080658, Acc: 0.90875314, F1: 0.95208262 Bal: 0.5058 - Val Loss: 0.68370378, Accuracy: 0.9060, F1: 0.9506 Bal: 0.5007\n","Epoch 026:      TX: Train Loss: 0.6773, Acc: 0.6370, F1: 0.7651 Bal: 0.5577 - Val Loss: 0.6881, Accuracy: 0.6367, F1: 0.7659 Bal: 0.5430\n","           WALLETS: Train Loss: 0.68072087, Acc: 0.90881482, F1: 0.95211601 Bal: 0.5059 - Val Loss: 0.68371582, Accuracy: 0.9061, F1: 0.9506 Bal: 0.5009\n","Epoch 027:      TX: Train Loss: 0.6764, Acc: 0.6350, F1: 0.7633 Bal: 0.5591 - Val Loss: 0.6881, Accuracy: 0.6354, F1: 0.7648 Bal: 0.5432\n","           WALLETS: Train Loss: 0.68065441, Acc: 0.90865517, F1: 0.95202395 Bal: 0.5063 - Val Loss: 0.68374181, Accuracy: 0.9058, F1: 0.9505 Bal: 0.5006\n","Epoch 028:      TX: Train Loss: 0.6755, Acc: 0.6353, F1: 0.7634 Bal: 0.5610 - Val Loss: 0.6881, Accuracy: 0.6345, F1: 0.7640 Bal: 0.5447\n","           WALLETS: Train Loss: 0.68060213, Acc: 0.90837579, F1: 0.95186648 Bal: 0.5066 - Val Loss: 0.68377531, Accuracy: 0.9055, F1: 0.9503 Bal: 0.5003\n","Epoch 029:      TX: Train Loss: 0.6746, Acc: 0.6381, F1: 0.7657 Bal: 0.5624 - Val Loss: 0.6881, Accuracy: 0.6356, F1: 0.7648 Bal: 0.5453\n","           WALLETS: Train Loss: 0.68055809, Acc: 0.90753763, F1: 0.95140032 Bal: 0.5067 - Val Loss: 0.68381494, Accuracy: 0.9046, F1: 0.9499 Bal: 0.5003\n","Epoch 030:      TX: Train Loss: 0.6738, Acc: 0.6415, F1: 0.7685 Bal: 0.5631 - Val Loss: 0.6881, Accuracy: 0.6411, F1: 0.7693 Bal: 0.5464\n","           WALLETS: Train Loss: 0.68051594, Acc: 0.90716753, F1: 0.95119294 Bal: 0.5069 - Val Loss: 0.68385553, Accuracy: 0.9043, F1: 0.9497 Bal: 0.5006\n","Epoch 031:      TX: Train Loss: 0.6729, Acc: 0.6424, F1: 0.7692 Bal: 0.5640 - Val Loss: 0.6882, Accuracy: 0.6406, F1: 0.7690 Bal: 0.5462\n","           WALLETS: Train Loss: 0.68047231, Acc: 0.90529891, F1: 0.95015069 Bal: 0.5071 - Val Loss: 0.68389392, Accuracy: 0.9026, F1: 0.9487 Bal: 0.5003\n","Epoch 032:      TX: Train Loss: 0.6720, Acc: 0.6417, F1: 0.7685 Bal: 0.5652 - Val Loss: 0.6882, Accuracy: 0.6398, F1: 0.7683 Bal: 0.5457\n","           WALLETS: Train Loss: 0.68042332, Acc: 0.90481270, F1: 0.94987849 Bal: 0.5072 - Val Loss: 0.68392473, Accuracy: 0.9020, F1: 0.9484 Bal: 0.5003\n","Epoch 033:      TX: Train Loss: 0.6711, Acc: 0.6437, F1: 0.7701 Bal: 0.5659 - Val Loss: 0.6883, Accuracy: 0.6413, F1: 0.7696 Bal: 0.5446\n","           WALLETS: Train Loss: 0.68036836, Acc: 0.90433738, F1: 0.94961020 Bal: 0.5075 - Val Loss: 0.68394107, Accuracy: 0.9015, F1: 0.9481 Bal: 0.5005\n","Epoch 034:      TX: Train Loss: 0.6701, Acc: 0.6461, F1: 0.7720 Bal: 0.5671 - Val Loss: 0.6883, Accuracy: 0.6435, F1: 0.7713 Bal: 0.5468\n","           WALLETS: Train Loss: 0.68030632, Acc: 0.90347745, F1: 0.94912700 Bal: 0.5079 - Val Loss: 0.68394399, Accuracy: 0.9006, F1: 0.9476 Bal: 0.5008\n","Epoch 035:      TX: Train Loss: 0.6692, Acc: 0.6489, F1: 0.7743 Bal: 0.5686 - Val Loss: 0.6884, Accuracy: 0.6455, F1: 0.7729 Bal: 0.5469\n","           WALLETS: Train Loss: 0.68023807, Acc: 0.90216397, F1: 0.94838892 Bal: 0.5083 - Val Loss: 0.68393970, Accuracy: 0.8996, F1: 0.9470 Bal: 0.5013\n","Epoch 036:      TX: Train Loss: 0.6682, Acc: 0.6512, F1: 0.7761 Bal: 0.5705 - Val Loss: 0.6885, Accuracy: 0.6455, F1: 0.7731 Bal: 0.5439\n","           WALLETS: Train Loss: 0.68016613, Acc: 0.90174671, F1: 0.94815521 Bal: 0.5083 - Val Loss: 0.68392986, Accuracy: 0.8990, F1: 0.9467 Bal: 0.5010\n","Epoch 037:      TX: Train Loss: 0.6673, Acc: 0.6510, F1: 0.7758 Bal: 0.5721 - Val Loss: 0.6885, Accuracy: 0.6422, F1: 0.7706 Bal: 0.5411\n","           WALLETS: Train Loss: 0.68009305, Acc: 0.90113351, F1: 0.94780857 Bal: 0.5086 - Val Loss: 0.68391514, Accuracy: 0.8985, F1: 0.9464 Bal: 0.5012\n","Epoch 038:      TX: Train Loss: 0.6663, Acc: 0.6518, F1: 0.7763 Bal: 0.5730 - Val Loss: 0.6886, Accuracy: 0.6422, F1: 0.7704 Bal: 0.5431\n","           WALLETS: Train Loss: 0.68002045, Acc: 0.90048766, F1: 0.94744104 Bal: 0.5092 - Val Loss: 0.68389922, Accuracy: 0.8980, F1: 0.9461 Bal: 0.5022\n","Epoch 039:      TX: Train Loss: 0.6653, Acc: 0.6536, F1: 0.7778 Bal: 0.5740 - Val Loss: 0.6887, Accuracy: 0.6439, F1: 0.7720 Bal: 0.5411\n","           WALLETS: Train Loss: 0.67995006, Acc: 0.89973295, F1: 0.94701380 Bal: 0.5095 - Val Loss: 0.68388706, Accuracy: 0.8973, F1: 0.9457 Bal: 0.5021\n","Epoch 040:      TX: Train Loss: 0.6643, Acc: 0.6540, F1: 0.7781 Bal: 0.5742 - Val Loss: 0.6888, Accuracy: 0.6446, F1: 0.7725 Bal: 0.5415\n","           WALLETS: Train Loss: 0.67988396, Acc: 0.89930117, F1: 0.94676878 Bal: 0.5098 - Val Loss: 0.68388462, Accuracy: 0.8968, F1: 0.9454 Bal: 0.5023\n","Epoch 041:      TX: Train Loss: 0.6632, Acc: 0.6524, F1: 0.7767 Bal: 0.5756 - Val Loss: 0.6889, Accuracy: 0.6419, F1: 0.7705 Bal: 0.5390\n","           WALLETS: Train Loss: 0.67982346, Acc: 0.89833965, F1: 0.94622389 Bal: 0.5103 - Val Loss: 0.68388617, Accuracy: 0.8956, F1: 0.9448 Bal: 0.5021\n","Epoch 042:      TX: Train Loss: 0.6622, Acc: 0.6531, F1: 0.7771 Bal: 0.5774 - Val Loss: 0.6890, Accuracy: 0.6417, F1: 0.7704 Bal: 0.5379\n","           WALLETS: Train Loss: 0.67976844, Acc: 0.89765025, F1: 0.94582948 Bal: 0.5109 - Val Loss: 0.68389410, Accuracy: 0.8947, F1: 0.9443 Bal: 0.5018\n","Epoch 043:      TX: Train Loss: 0.6611, Acc: 0.6554, F1: 0.7789 Bal: 0.5789 - Val Loss: 0.6892, Accuracy: 0.6441, F1: 0.7723 Bal: 0.5403\n","           WALLETS: Train Loss: 0.67971796, Acc: 0.89710236, F1: 0.94551951 Bal: 0.5110 - Val Loss: 0.68390912, Accuracy: 0.8942, F1: 0.9440 Bal: 0.5022\n","Epoch 044:      TX: Train Loss: 0.6600, Acc: 0.6547, F1: 0.7783 Bal: 0.5808 - Val Loss: 0.6894, Accuracy: 0.6455, F1: 0.7732 Bal: 0.5420\n","           WALLETS: Train Loss: 0.67966968, Acc: 0.89671413, F1: 0.94529725 Bal: 0.5114 - Val Loss: 0.68392891, Accuracy: 0.8939, F1: 0.9438 Bal: 0.5025\n","Epoch 045:      TX: Train Loss: 0.6589, Acc: 0.6546, F1: 0.7780 Bal: 0.5825 - Val Loss: 0.6896, Accuracy: 0.6461, F1: 0.7737 Bal: 0.5433\n","           WALLETS: Train Loss: 0.67962050, Acc: 0.89628598, F1: 0.94505296 Bal: 0.5117 - Val Loss: 0.68395007, Accuracy: 0.8933, F1: 0.9435 Bal: 0.5025\n","Epoch 046:      TX: Train Loss: 0.6577, Acc: 0.6560, F1: 0.7790 Bal: 0.5852 - Val Loss: 0.6900, Accuracy: 0.6474, F1: 0.7748 Bal: 0.5421\n","           WALLETS: Train Loss: 0.67956936, Acc: 0.89573446, F1: 0.94473634 Bal: 0.5122 - Val Loss: 0.68396968, Accuracy: 0.8926, F1: 0.9431 Bal: 0.5029\n","Epoch 047:      TX: Train Loss: 0.6565, Acc: 0.6559, F1: 0.7788 Bal: 0.5864 - Val Loss: 0.6902, Accuracy: 0.6472, F1: 0.7745 Bal: 0.5439\n","           WALLETS: Train Loss: 0.67951453, Acc: 0.89501967, F1: 0.94432739 Bal: 0.5127 - Val Loss: 0.68398672, Accuracy: 0.8918, F1: 0.9426 Bal: 0.5027\n","Epoch 048:      TX: Train Loss: 0.6553, Acc: 0.6556, F1: 0.7785 Bal: 0.5881 - Val Loss: 0.6905, Accuracy: 0.6470, F1: 0.7745 Bal: 0.5418\n","           WALLETS: Train Loss: 0.67945558, Acc: 0.89404363, F1: 0.94377246 Bal: 0.5130 - Val Loss: 0.68400663, Accuracy: 0.8908, F1: 0.9420 Bal: 0.5027\n","Epoch 049:      TX: Train Loss: 0.6541, Acc: 0.6587, F1: 0.7810 Bal: 0.5892 - Val Loss: 0.6910, Accuracy: 0.6503, F1: 0.7771 Bal: 0.5427\n","           WALLETS: Train Loss: 0.67939317, Acc: 0.89377150, F1: 0.94361686 Bal: 0.5132 - Val Loss: 0.68402648, Accuracy: 0.8901, F1: 0.9416 Bal: 0.5024\n","Epoch 050:      TX: Train Loss: 0.6528, Acc: 0.6571, F1: 0.7796 Bal: 0.5909 - Val Loss: 0.6913, Accuracy: 0.6468, F1: 0.7743 Bal: 0.5427\n","           WALLETS: Train Loss: 0.67932820, Acc: 0.89351388, F1: 0.94346891 Bal: 0.5134 - Val Loss: 0.68404371, Accuracy: 0.8900, F1: 0.9416 Bal: 0.5027\n","Epoch 051:      TX: Train Loss: 0.6515, Acc: 0.6595, F1: 0.7815 Bal: 0.5923 - Val Loss: 0.6918, Accuracy: 0.6485, F1: 0.7758 Bal: 0.5407\n","           WALLETS: Train Loss: 0.67926204, Acc: 0.89342680, F1: 0.94341702 Bal: 0.5136 - Val Loss: 0.68406266, Accuracy: 0.8899, F1: 0.9415 Bal: 0.5030\n","Epoch 052:      TX: Train Loss: 0.6502, Acc: 0.6614, F1: 0.7828 Bal: 0.5945 - Val Loss: 0.6922, Accuracy: 0.6505, F1: 0.7774 Bal: 0.5408\n","           WALLETS: Train Loss: 0.67919636, Acc: 0.89400734, F1: 0.94374584 Bal: 0.5136 - Val Loss: 0.68408257, Accuracy: 0.8901, F1: 0.9416 Bal: 0.5024\n","Epoch 053:      TX: Train Loss: 0.6488, Acc: 0.6608, F1: 0.7823 Bal: 0.5952 - Val Loss: 0.6927, Accuracy: 0.6483, F1: 0.7756 Bal: 0.5416\n","           WALLETS: Train Loss: 0.67913216, Acc: 0.89342317, F1: 0.94341040 Bal: 0.5141 - Val Loss: 0.68410254, Accuracy: 0.8896, F1: 0.9413 Bal: 0.5031\n","Epoch 054:      TX: Train Loss: 0.6474, Acc: 0.6631, F1: 0.7841 Bal: 0.5965 - Val Loss: 0.6933, Accuracy: 0.6501, F1: 0.7771 Bal: 0.5396\n","           WALLETS: Train Loss: 0.67906880, Acc: 0.89347034, F1: 0.94343708 Bal: 0.5141 - Val Loss: 0.68412882, Accuracy: 0.8895, F1: 0.9413 Bal: 0.5033\n","Epoch 055:      TX: Train Loss: 0.6460, Acc: 0.6620, F1: 0.7831 Bal: 0.5983 - Val Loss: 0.6937, Accuracy: 0.6474, F1: 0.7749 Bal: 0.5411\n","           WALLETS: Train Loss: 0.67900658, Acc: 0.89328166, F1: 0.94332904 Bal: 0.5142 - Val Loss: 0.68415147, Accuracy: 0.8895, F1: 0.9413 Bal: 0.5037\n","Epoch 056:      TX: Train Loss: 0.6445, Acc: 0.6670, F1: 0.7871 Bal: 0.5994 - Val Loss: 0.6946, Accuracy: 0.6516, F1: 0.7784 Bal: 0.5395\n","           WALLETS: Train Loss: 0.67894489, Acc: 0.89258501, F1: 0.94292985 Bal: 0.5146 - Val Loss: 0.68417007, Accuracy: 0.8886, F1: 0.9408 Bal: 0.5034\n","Epoch 057:      TX: Train Loss: 0.6431, Acc: 0.6587, F1: 0.7802 Bal: 0.6018 - Val Loss: 0.6945, Accuracy: 0.6409, F1: 0.7695 Bal: 0.5414\n","           WALLETS: Train Loss: 0.67888325, Acc: 0.89195367, F1: 0.94256873 Bal: 0.5149 - Val Loss: 0.68418890, Accuracy: 0.8878, F1: 0.9403 Bal: 0.5033\n","Epoch 058:      TX: Train Loss: 0.6417, Acc: 0.6759, F1: 0.7942 Bal: 0.6013 - Val Loss: 0.6966, Accuracy: 0.6599, F1: 0.7853 Bal: 0.5372\n","           WALLETS: Train Loss: 0.67881912, Acc: 0.89198270, F1: 0.94258394 Bal: 0.5150 - Val Loss: 0.68421251, Accuracy: 0.8876, F1: 0.9402 Bal: 0.5031\n","Epoch 059:      TX: Train Loss: 0.6401, Acc: 0.6561, F1: 0.7779 Bal: 0.6040 - Val Loss: 0.6955, Accuracy: 0.6373, F1: 0.7667 Bal: 0.5394\n","           WALLETS: Train Loss: 0.67875475, Acc: 0.88999797, F1: 0.94144958 Bal: 0.5156 - Val Loss: 0.68421930, Accuracy: 0.8853, F1: 0.9389 Bal: 0.5025\n","Epoch 060:      TX: Train Loss: 0.6385, Acc: 0.6686, F1: 0.7881 Bal: 0.6037 - Val Loss: 0.6969, Accuracy: 0.6509, F1: 0.7778 Bal: 0.5411\n","           WALLETS: Train Loss: 0.67868686, Acc: 0.89182668, F1: 0.94249158 Bal: 0.5154 - Val Loss: 0.68424833, Accuracy: 0.8871, F1: 0.9400 Bal: 0.5031\n","Epoch 061:      TX: Train Loss: 0.6370, Acc: 0.6766, F1: 0.7944 Bal: 0.6059 - Val Loss: 0.6983, Accuracy: 0.6588, F1: 0.7842 Bal: 0.5396\n","           WALLETS: Train Loss: 0.67861736, Acc: 0.88975487, F1: 0.94130987 Bal: 0.5157 - Val Loss: 0.68426073, Accuracy: 0.8849, F1: 0.9387 Bal: 0.5028\n","Epoch 062:      TX: Train Loss: 0.6354, Acc: 0.6600, F1: 0.7809 Bal: 0.6080 - Val Loss: 0.6974, Accuracy: 0.6402, F1: 0.7691 Bal: 0.5390\n","           WALLETS: Train Loss: 0.67854738, Acc: 0.88949362, F1: 0.94116011 Bal: 0.5158 - Val Loss: 0.68427247, Accuracy: 0.8845, F1: 0.9384 Bal: 0.5030\n","Epoch 063:      TX: Train Loss: 0.6337, Acc: 0.6710, F1: 0.7898 Bal: 0.6079 - Val Loss: 0.6990, Accuracy: 0.6527, F1: 0.7792 Bal: 0.5401\n","           WALLETS: Train Loss: 0.67847592, Acc: 0.88962787, F1: 0.94123533 Bal: 0.5159 - Val Loss: 0.68429434, Accuracy: 0.8845, F1: 0.9385 Bal: 0.5034\n","Epoch 064:      TX: Train Loss: 0.6321, Acc: 0.6793, F1: 0.7964 Bal: 0.6087 - Val Loss: 0.7005, Accuracy: 0.6610, F1: 0.7860 Bal: 0.5388\n","           WALLETS: Train Loss: 0.67840439, Acc: 0.88774474, F1: 0.94015490 Bal: 0.5166 - Val Loss: 0.68430001, Accuracy: 0.8825, F1: 0.9373 Bal: 0.5034\n","Epoch 065:      TX: Train Loss: 0.6305, Acc: 0.6642, F1: 0.7839 Bal: 0.6135 - Val Loss: 0.6996, Accuracy: 0.6428, F1: 0.7712 Bal: 0.5405\n","           WALLETS: Train Loss: 0.67833352, Acc: 0.88935211, F1: 0.94107303 Bal: 0.5164 - Val Loss: 0.68432301, Accuracy: 0.8843, F1: 0.9383 Bal: 0.5039\n","Epoch 066:      TX: Train Loss: 0.6287, Acc: 0.6749, F1: 0.7927 Bal: 0.6132 - Val Loss: 0.7014, Accuracy: 0.6553, F1: 0.7813 Bal: 0.5416\n","           WALLETS: Train Loss: 0.67826194, Acc: 0.88909087, F1: 0.94092272 Bal: 0.5166 - Val Loss: 0.68434465, Accuracy: 0.8839, F1: 0.9381 Bal: 0.5038\n","Epoch 067:      TX: Train Loss: 0.6270, Acc: 0.6783, F1: 0.7953 Bal: 0.6139 - Val Loss: 0.7026, Accuracy: 0.6597, F1: 0.7847 Bal: 0.5430\n","           WALLETS: Train Loss: 0.67818958, Acc: 0.88739278, F1: 0.93995066 Bal: 0.5169 - Val Loss: 0.68436652, Accuracy: 0.8820, F1: 0.9370 Bal: 0.5034\n","Epoch 068:      TX: Train Loss: 0.6253, Acc: 0.6657, F1: 0.7848 Bal: 0.6188 - Val Loss: 0.7018, Accuracy: 0.6424, F1: 0.7705 Bal: 0.5452\n","           WALLETS: Train Loss: 0.67811662, Acc: 0.88669613, F1: 0.93954524 Bal: 0.5175 - Val Loss: 0.68439180, Accuracy: 0.8815, F1: 0.9367 Bal: 0.5035\n","Epoch 069:      TX: Train Loss: 0.6234, Acc: 0.6807, F1: 0.7970 Bal: 0.6178 - Val Loss: 0.7042, Accuracy: 0.6615, F1: 0.7860 Bal: 0.5450\n","           WALLETS: Train Loss: 0.67804146, Acc: 0.88580717, F1: 0.93903209 Bal: 0.5180 - Val Loss: 0.68440610, Accuracy: 0.8805, F1: 0.9361 Bal: 0.5034\n","Epoch 070:      TX: Train Loss: 0.6216, Acc: 0.6792, F1: 0.7956 Bal: 0.6213 - Val Loss: 0.7045, Accuracy: 0.6571, F1: 0.7825 Bal: 0.5445\n","           WALLETS: Train Loss: 0.67796463, Acc: 0.88832891, F1: 0.94047589 Bal: 0.5177 - Val Loss: 0.68444198, Accuracy: 0.8830, F1: 0.9376 Bal: 0.5036\n","Epoch 071:      TX: Train Loss: 0.6198, Acc: 0.6712, F1: 0.7891 Bal: 0.6231 - Val Loss: 0.7044, Accuracy: 0.6459, F1: 0.7733 Bal: 0.5471\n","           WALLETS: Train Loss: 0.67788708, Acc: 0.88527380, F1: 0.93872322 Bal: 0.5183 - Val Loss: 0.68443692, Accuracy: 0.8796, F1: 0.9356 Bal: 0.5026\n","Epoch 072:      TX: Train Loss: 0.6179, Acc: 0.6877, F1: 0.8022 Bal: 0.6238 - Val Loss: 0.7074, Accuracy: 0.6674, F1: 0.7905 Bal: 0.5492\n","           WALLETS: Train Loss: 0.67780870, Acc: 0.88889131, F1: 0.94079754 Bal: 0.5176 - Val Loss: 0.68450350, Accuracy: 0.8834, F1: 0.9378 Bal: 0.5027\n","Epoch 073:      TX: Train Loss: 0.6160, Acc: 0.6733, F1: 0.7906 Bal: 0.6256 - Val Loss: 0.7062, Accuracy: 0.6461, F1: 0.7735 Bal: 0.5463\n","           WALLETS: Train Loss: 0.67773229, Acc: 0.88407643, F1: 0.93802965 Bal: 0.5189 - Val Loss: 0.68447310, Accuracy: 0.8786, F1: 0.9351 Bal: 0.5025\n","Epoch 074:      TX: Train Loss: 0.6140, Acc: 0.6847, F1: 0.7996 Bal: 0.6281 - Val Loss: 0.7084, Accuracy: 0.6588, F1: 0.7838 Bal: 0.5465\n","           WALLETS: Train Loss: 0.67765635, Acc: 0.88999071, F1: 0.94142399 Bal: 0.5176 - Val Loss: 0.68458325, Accuracy: 0.8844, F1: 0.9384 Bal: 0.5022\n","Epoch 075:      TX: Train Loss: 0.6121, Acc: 0.6861, F1: 0.8006 Bal: 0.6297 - Val Loss: 0.7095, Accuracy: 0.6595, F1: 0.7843 Bal: 0.5468\n","           WALLETS: Train Loss: 0.67758179, Acc: 0.88162001, F1: 0.93660792 Bal: 0.5198 - Val Loss: 0.68451273, Accuracy: 0.8760, F1: 0.9336 Bal: 0.5030\n","Epoch 076:      TX: Train Loss: 0.6101, Acc: 0.6779, F1: 0.7940 Bal: 0.6315 - Val Loss: 0.7094, Accuracy: 0.6487, F1: 0.7756 Bal: 0.5467\n","           WALLETS: Train Loss: 0.67750466, Acc: 0.89014310, F1: 0.94150672 Bal: 0.5180 - Val Loss: 0.68464839, Accuracy: 0.8846, F1: 0.9385 Bal: 0.5026\n","Epoch 077:      TX: Train Loss: 0.6082, Acc: 0.6958, F1: 0.8081 Bal: 0.6322 - Val Loss: 0.7133, Accuracy: 0.6720, F1: 0.7940 Bal: 0.5508\n","           WALLETS: Train Loss: 0.67742366, Acc: 0.88214612, F1: 0.93691086 Bal: 0.5198 - Val Loss: 0.68457961, Accuracy: 0.8765, F1: 0.9338 Bal: 0.5031\n","Epoch 078:      TX: Train Loss: 0.6063, Acc: 0.6753, F1: 0.7917 Bal: 0.6354 - Val Loss: 0.7109, Accuracy: 0.6428, F1: 0.7707 Bal: 0.5474\n","           WALLETS: Train Loss: 0.67734289, Acc: 0.88676870, F1: 0.93956898 Bal: 0.5192 - Val Loss: 0.68465453, Accuracy: 0.8811, F1: 0.9365 Bal: 0.5024\n","Early stopping at epoch 77\n","{'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'GAT', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 10, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99, 'epoch': 78, 'end': True}\n","Final_result for tx\n","{'hidden_channels': 128, 'num_head': None, 'num_layers': 2, 'num_epoch': 300, 'patience': 50, 'lr': 0.001, 'weight_decay': 1e-05, 'dropout': 0, 'conv_type': 'GAT', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 10, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99, 'epoch': 78}\n","Epoch 77:\n","  TX:\n","   Train: Loss=0.6082, Acc=0.6958, F1=0.8081, Bal. Acc=0.6322\n","   Val:   Loss=0.7133, Acc=0.6720, F1=0.7940, Bal. Acc=0.5508\n","   Test:  Loss=0.7017, Acc=0.6725, F1=0.7936, Bal. Acc=0.5691\n","              precision    recall  f1-score   support\n","\n","           0       0.13      0.40      0.19       453\n","           1       0.91      0.70      0.79      4105\n","\n","    accuracy                           0.67      4558\n","   macro avg       0.52      0.55      0.49      4558\n","weighted avg       0.84      0.67      0.73      4558\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.14      0.44      0.21       665\n","           1       0.92      0.70      0.79      6172\n","\n","    accuracy                           0.67      6837\n","   macro avg       0.53      0.57      0.50      6837\n","weighted avg       0.84      0.67      0.74      6837\n","\n","  WALLETS:\n","   Train: Loss=0.67742366, Acc=0.88214612, F1=0.93691086, Bal. Acc=0.5198\n","   Val:   Loss=0.68457961, Acc=0.8765, F1=0.9338, Bal. Acc=0.5031\n","   Test:  Loss=0.68403685, Acc=0.8775, F1=0.9344, Bal. Acc=0.5052\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.06      0.07      2906\n","           1       0.92      0.95      0.93     33841\n","\n","    accuracy                           0.88     36747\n","   macro avg       0.50      0.50      0.50     36747\n","weighted avg       0.86      0.88      0.87     36747\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.09      0.06      0.08      4340\n","           1       0.92      0.95      0.93     50781\n","\n","    accuracy                           0.88     55121\n","   macro avg       0.51      0.51      0.50     55121\n","weighted avg       0.86      0.88      0.87     55121\n","\n","\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIoomnaeh74t1AKH1zYUzUr42JRiCeq-","timestamp":1739893838211}],"collapsed_sections":["llE922-B6T-U"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}