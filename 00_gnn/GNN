{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0iidXjXM7pXO"},"outputs":[],"source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","!pip install torch_geometric\n","#!pip install torch-sparse\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","import os.path as osp\n","import torch\n","import warnings\n","from torch_geometric.data import Data, HeteroData\n","from torch_geometric.transforms import RandomNodeSplit\n","from torch_geometric.nn import GCNConv, GATConv, SAGEConv, ChebConv\n","import torch_geometric.nn as pyg_nn\n","import torch.nn as nn\n","import torch_geometric.utils as pyg_utils\n","from torch.nn import Module, Linear\n","import torch.nn.functional as F\n","from sklearn.metrics import precision_recall_fscore_support, f1_score\n","from torch_geometric.seed import seed_everything\n","import joblib\n","drive.mount('/content/drive')  # Autenticazione con Google Drive\n","\n","warnings.simplefilter(action='ignore')\n","SEED = 51\n","FILEPATH_TX = '/content/drive/MyDrive/blockchain/00_gnn/all_tx.csv'\n","FILEPATH_WALLET = '/content/drive/MyDrive/blockchain/00_gnn/all_wallet.csv'\n","\n","base_path = \"/content/drive/MyDrive/blockchain/E++/\"\n","path_comb = '/content/drive/MyDrive/blockchain/00_gnn/combination_do.csv'"]},{"cell_type":"markdown","metadata":{"id":"XOk9zPoYDC5I"},"source":["#Crea db vuoto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCrI1kO2CYxi"},"outputs":[],"source":["def create_df():\n","  # Crea DataFrame vuoti\n","  df_tx = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","  df_wallet = pd.DataFrame(columns=['epoch', 'hidden_channels', 'out_channels', 'num_layers', 'num_epoch', 'patience', 'lr', 'weight_decay', 'conv_type', 'eps', 'gamma','step_size', 'aggr', 'end'])\n","\n","  # Salva i DataFrame come file CSV\n","  df_tx.to_csv(FILEPATH_TX, index=False)\n","  df_wallet.to_csv(FILEPATH_WALLET, index=False)\n","\n","#create_df()"]},{"cell_type":"markdown","metadata":{"id":"W1Syx8hzDPvM"},"source":["#Carica dati"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qobTcbrVqpwG"},"outputs":[],"source":["def load_data():\n","    # Loading transactions\n","\n","    #Reading edges, features and classes from transaction files (as done with the original dataset)\n","    df_edges_tx = pd.read_csv(osp.join(base_path, \"txs_edgelist.csv\"))\n","    df_features_tx = pd.read_csv(osp.join(base_path, \"txs_features.csv\"), header=None)\n","    df_classes_tx = pd.read_csv(osp.join(base_path, \"txs_classes.csv\"))\n","\n","    #Columns naming based on index\n","    colNames1_tx = {'0': 'txId', 1: \"Time step\"}\n","    colNames2_tx = {str(ii+2): \"Local_feature_\" + str(ii+1) for ii in range(94)}\n","    colNames3_tx = {str(ii+96): \"Aggregate_feature_\" + str(ii+1) for ii in range(72)}\n","\n","    colNames_tx = dict(colNames1_tx, **colNames2_tx, **colNames3_tx)\n","    colNames_tx = {int(jj): item_kk for jj, item_kk in colNames_tx.items()}\n","\n","    # Rename feature columns\n","    df_features_tx = df_features_tx.rename(columns=colNames_tx)\n","\n","    # Map unknown class to '3'\n","    df_classes_tx.loc[df_classes_tx['class'] == 'unknown', 'class'] = '3'\n","\n","    # Merge classes and features in one Dataframe\n","    df_class_feature_tx = pd.merge(df_classes_tx, df_features_tx)\n","\n","    # Exclude records with unknown class transaction\n","    df_class_feature_tx = df_class_feature_tx[df_class_feature_tx['class'] != 3]\n","\n","    # Build Dataframe with head and tail of transactions (edges)\n","    known_txs = df_class_feature_tx[\"txId\"].values\n","    df_edges_tx = df_edges_tx[(df_edges_tx[\"txId1\"].isin(known_txs)) & (df_edges_tx[\"txId2\"].isin(known_txs))]\n","\n","    # Build indices for features and edge types\n","    features_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"txId\"].unique()))}\n","    class_idx_tx = {name: idx for idx, name in enumerate(sorted(df_class_feature_tx[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_tx[\"txId\"] = df_class_feature_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_class_feature_tx[\"class\"] = df_class_feature_tx[\"class\"].apply(lambda name: class_idx_tx[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_tx[\"txId1\"] = df_edges_tx[\"txId1\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx[\"txId2\"] = df_edges_tx[\"txId2\"].apply(lambda name: features_idx_tx[name])\n","\n","    # Loading wallets\n","\n","    # From file\n","    df_edges_wallet = pd.read_csv(osp.join(base_path, \"AddrAddr_edgelist.csv\"))\n","    df_class_feature_wallet = pd.read_csv(osp.join(base_path, \"wallets_features_classes_combined.csv\"))\n","\n","    # Exclude records with unknown class transaction\n","    #print(df_class_feature_wallet.shape)\n","    df_class_feature_wallet = df_class_feature_wallet[df_class_feature_wallet[\"class\"] != 3]\n","    #print(df_class_feature_wallet.shape)\n","\n","    # Build Dataframe with head and tail of AddrToAddr (edges)\n","    known_wallets = df_class_feature_wallet[\"address\"].values\n","    df_edges_wallet = df_edges_wallet[(df_edges_wallet[\"input_address\"].isin(known_wallets)) & (df_edges_wallet[\"output_address\"].isin(known_wallets))]\n","\n","    # Building indices for features and edge types\n","    features_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"address\"].unique()))}\n","    class_idx_wallet = {name: idx for idx, name in enumerate(sorted(df_class_feature_wallet[\"class\"].unique()))}\n","\n","    # Apply index encoding to features\n","    df_class_feature_wallet[\"address\"] = df_class_feature_wallet[\"address\"].apply(lambda name: features_idx_wallet[name])\n","    df_class_feature_wallet[\"class\"] = df_class_feature_wallet[\"class\"].apply(lambda name: class_idx_wallet[name])\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet[\"input_address\"] = df_edges_wallet[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet[\"output_address\"] = df_edges_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    # Loading AddrTx and TxAddr\n","\n","    # From file\n","    df_edges_wallet_tx = pd.read_csv(osp.join(base_path, \"AddrTx_edgelist.csv\"))\n","    df_edges_tx_wallet = pd.read_csv(osp.join(base_path, \"TxAddr_edgelist.csv\"))\n","\n","    # Build Dataframe with head and tail of AddrTx (edges)\n","    df_edges_wallet_tx = df_edges_wallet_tx[(df_edges_wallet_tx[\"input_address\"].isin(known_wallets)) & df_edges_wallet_tx[\"txId\"].isin(known_txs)]\n","    df_edges_tx_wallet = df_edges_tx_wallet[(df_edges_tx_wallet[\"txId\"].isin(known_txs)) & df_edges_tx_wallet[\"output_address\"].isin(known_wallets)]\n","\n","    # Apply index encoding to edges\n","    df_edges_wallet_tx[\"input_address\"] = df_edges_wallet_tx[\"input_address\"].apply(lambda name: features_idx_wallet[name])\n","    df_edges_wallet_tx[\"txId\"] = df_edges_wallet_tx[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"txId\"] = df_edges_tx_wallet[\"txId\"].apply(lambda name: features_idx_tx[name])\n","    df_edges_tx_wallet[\"output_address\"] = df_edges_tx_wallet[\"output_address\"].apply(lambda name: features_idx_wallet[name])\n","\n","    return df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet\n","\n","def data_to_pyg(df_class_feature_tx, df_edges_tx, df_class_feature_wallet, df_edges_wallet, df_edges_wallet_tx, df_edges_tx_wallet, features_idx_tx, features_idx_wallet):\n","    data = HeteroData()\n","\n","    # Defining PyG objects for transactions\n","    df_class_feature_tx = df_class_feature_tx.fillna(0)\n","    data['tx'].x = torch.tensor(df_class_feature_tx.iloc[:, 3:].values, dtype=torch.float)\n","    data['tx'].y = torch.tensor(df_class_feature_tx[\"class\"].values, dtype=torch.long)\n","    data['tx','is_related_to','tx'].edge_index = torch.tensor([df_edges_tx[\"txId1\"].values,\n","                            df_edges_tx[\"txId2\"].values], dtype=torch.int64)\n","    #data['tx'] = random_node_split(num_val=0.15, num_test=0.2)(data['tx'])\n","    # Defining PyG objects for wallets\n","    data['wallet'].x = torch.tensor(df_class_feature_wallet.iloc[:, 3:].values, dtype=torch.float)\n","    data['wallet'].y = torch.tensor(df_class_feature_wallet[\"class\"].values, dtype=torch.long)\n","    data['wallet','interacts_with','wallet'].edge_index = torch.tensor([df_edges_wallet[\"input_address\"].values,\n","                            df_edges_wallet[\"output_address\"].values], dtype=torch.int64)\n","    #data['wallet'] = random_node_split(num_val=0.15, num_test=0.2)(data['wallet'])\n","    # Defining PyG objects for cross-edges\n","    data['wallet','performs','tx'].edge_index = torch.tensor([df_edges_wallet_tx[\"input_address\"].values,\n","                                         df_edges_wallet_tx[\"txId\"].values], dtype=torch.int64)\n","\n","    data['tx', 'flows_into', 'wallet'].edge_index = torch.tensor([df_edges_tx_wallet[\"txId\"].values,\n","                                         df_edges_tx_wallet[\"output_address\"].values], dtype=torch.int64)\n","\n","    # Impostare il seed per la divisione del dataset\n","    return RandomNodeSplit(num_val=0.10, num_test=0.15)(data)"]},{"cell_type":"markdown","metadata":{"id":"OhFPkCpExzOL"},"source":["#Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQfEkZ1nx44T"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do2dQUF-x7hj"},"outputs":[],"source":["#NEW -> SU RANGE\n","# Utility per conversione a tensor\n","def to_tensor(arr):\n","    return torch.tensor(arr, dtype=torch.float).to(device)\n","\n","def scale_features(data, method=\"standard\"):\n","  if method == 'no':\n","    return data\n","\n","  # Scaling per training\n","  data['tx'].x[data['tx'].train_mask] = to_tensor(\n","      scale_train_data(data['tx'].x[data['tx'].train_mask].cpu().numpy(), method, 'tx')\n","  )\n","  data['wallet'].x[data['wallet'].train_mask] = to_tensor(\n","      scale_train_data(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), method, 'wallet')\n","  )\n","\n","  # Scaling per validation\n","  data['tx'].x[data['tx'].val_mask] = to_tensor(\n","      scale_validation_data(data['tx'].x[data['tx'].val_mask].cpu().numpy(), method, 'tx')\n","  )\n","  data['wallet'].x[data['wallet'].val_mask] = to_tensor(\n","      scale_validation_data(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), method, 'wallet')\n","  )\n","\n","  return data\n","\n","def scale_train_data(train, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = StandardScaler()\n","        scaled_train = scaler.fit_transform(train)  # Scala tutte le colonne\n","        joblib.dump(scaler, f\"scaler_standard_{df}.pkl\")\n","\n","        if 'l2' in scaling_method:\n","            norm = Normalizer(norm='l2')\n","            scaled_train = norm.fit_transform(scaled_train)\n","            joblib.dump(norm, f\"scaler_l2_{df}.pkl\")\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_train\n","\n","def scale_validation_data(val, scaling_method, df):\n","\n","    if 'standard' in scaling_method:\n","        scaler = joblib.load(f\"scaler_standard_{df}.pkl\")\n","        scaled_val = scaler.transform(val)  # Scala tutte le colonne\n","\n","        if 'l2' in scaling_method:\n","            norm = joblib.load(f\"scaler_l2_{df}.pkl\")\n","            scaled_val = norm.transform(scaled_val)\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    return scaled_val"]},{"cell_type":"code","source":["def dimentional_reduction(data, dim_reduction, pca_threshold):\n","    if dim_reduction == 'no':\n","        return data\n","    elif dim_reduction == 'pca':\n","        data1 = copy.deepcopy(data)\n","\n","        transformed_tx_data = apply_pca_train(data['tx'].x[data['tx'].train_mask].cpu().numpy(), 'tx', pca_threshold)\n","        transformed_wallet_data = apply_pca_train(data['wallet'].x[data['wallet'].train_mask].cpu().numpy(), 'wallet', pca_threshold)\n","\n","        data1['tx'].x = torch.zeros((data['tx'].x.shape[0], transformed_tx_data.shape[1]), dtype=torch.float, device=device)\n","        data1['wallet'].x = torch.zeros((data['wallet'].x.shape[0], transformed_wallet_data.shape[1]), dtype=torch.float, device=device)\n","\n","        data1['tx'].x[data['tx'].train_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].train_mask] = to_tensor(transformed_wallet_data)\n","\n","        transformed_tx_data = apply_pca_validation(data['tx'].x[data['tx'].val_mask].cpu().numpy(), 'tx')\n","        transformed_wallet_data = apply_pca_validation(data['wallet'].x[data['wallet'].val_mask].cpu().numpy(), 'wallet')\n","\n","        data1['tx'].x[data['tx'].val_mask] = to_tensor(transformed_tx_data)\n","        data1['wallet'].x[data['wallet'].val_mask] = to_tensor(transformed_wallet_data)\n","\n","        return data1\n","\n","def apply_pca_train(train, df, pca_threshold=0.99):\n","    pca = PCA(random_state=SEED)\n","    pca.fit(train)\n","\n","    # Selezione componenti principali\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","\n","    pca = PCA(n_components=n_components, random_state=SEED)\n","    transformed_data = pca.fit_transform(train).astype(np.float32)\n","\n","    joblib.dump(pca, f\"pca_model_{df}.pkl\")\n","    print(f\"  Numero di componenti principali per {df}: {pca.n_components_}\")\n","\n","    return transformed_data\n","\n","def apply_pca_validation(val, df):\n","    pca = joblib.load(f\"pca_model_{df}.pkl\")\n","    transformed_data = pca.transform(val).astype(np.float32)\n","    return transformed_data"],"metadata":{"id":"PVRMmabn21eP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CaNNtrNLDSdI"},"source":["#Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbfXKtdynFBL"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.nn import Linear, Dropout\n","from torch_geometric.nn import HeteroConv, GATConv, SAGEConv, TransformerConv\n","import random\n","from itertools import product\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxfbqge_p64q"},"outputs":[],"source":["def set_seed(seed = 51):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # Per più GPU\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.use_deterministic_algorithms(True, warn_only=True)\n","\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    seed_everything(seed)\n","device = \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiBF-BZYfq7b"},"outputs":[],"source":["class ResidualHeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0.5, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.skips = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1 # Define heads\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False, heads=heads)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","            self.skips.append(Linear(hidden_channels * heads, hidden_channels * heads)) # Fix: Linear layer expects the output of conv\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv, skip in zip(self.convs, self.skips):\n","            x_dict_new = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(F.relu(x + skip(x_dict_new[key]))) for key, x in x_dict_new.items()}  # Residual + Dropout\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdcebmslq5D0"},"outputs":[],"source":["class HeteroGNN(torch.nn.Module):\n","    def __init__(self, conv, hidden_channels=64, num_layers=2, aggr='sum', dropout_prob=0, out_channels=2, num_head=1):\n","        super().__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.dropout = Dropout(p=dropout_prob)\n","        heads = num_head if conv == 'Transformer' else 1  # Definiamo i heads solo se necessario\n","\n","        for _ in range(num_layers):\n","            if conv == 'GAT':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'interacts_with', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('wallet', 'performs', 'tx'): GATConv((-1, -1), hidden_channels, add_self_loops=False),\n","                    ('tx', 'flows_into', 'wallet'): GATConv((-1, -1), hidden_channels, add_self_loops=False)\n","                }, aggr=aggr)\n","            elif conv == 'SAGE':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'interacts_with', 'wallet'): SAGEConv(-1, hidden_channels),\n","                    ('wallet', 'performs', 'tx'): SAGEConv(-1, hidden_channels),\n","                    ('tx', 'flows_into', 'wallet'): SAGEConv(-1, hidden_channels)\n","                }, aggr=aggr)\n","            elif conv == 'Transformer':\n","                conv_layer = HeteroConv({\n","                    ('tx', 'is_related_to', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'interacts_with', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('wallet', 'performs', 'tx'): TransformerConv((-1, -1), hidden_channels, heads=heads),\n","                    ('tx', 'flows_into', 'wallet'): TransformerConv((-1, -1), hidden_channels, heads=heads)\n","                }, aggr=aggr)\n","            else:\n","                raise ValueError(\"Invalid convolution type. Choose from ['GAT', 'SAGE', 'Transformer']\")\n","\n","            self.convs.append(conv_layer)\n","\n","        # FIX: Modifica della dimensione di input dei layer lineari\n","        self.lin_tx = Linear(hidden_channels * heads, out_channels)\n","        self.lin_wallet = Linear(hidden_channels * heads, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict):\n","        for conv in self.convs:\n","            x_dict = conv(x_dict, edge_index_dict)\n","            x_dict = {key: self.dropout(x.relu()) for key, x in x_dict.items()}\n","        return self.lin_tx(x_dict['tx']), self.lin_wallet(x_dict['wallet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJEJVt5l04Eb"},"outputs":[],"source":["def is_combination_tested(filepath, new_row):\n","    existing_results = pd.read_csv(filepath)\n","\n","    # Identifica le colonne comuni tra il dataset e new_row\n","    dataset_columns = set(existing_results.columns)\n","    comparison_columns = [col for col in new_row.keys() if col not in ['end', 'num_epoch', 'epoch']]\n","\n","    # Filtra le combinazioni\n","    filtered_results = existing_results.copy()\n","    #filtered_results = filtered_results[filtered_results['end'] == True]\n","    filtered_results = filtered_results[filtered_results['num_epoch'] >= new_row['num_epoch']]\n","\n","    for col in comparison_columns:\n","        if col in dataset_columns:\n","            # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","            filtered_results = filtered_results[\n","                (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","            ]\n","\n","    return not filtered_results.empty\n","\n","def append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, end=False):\n","  def append_and_save_result(filepath, new_row, end=False):\n","    new_row['end'] = end\n","    # Leggi i risultati esistenti\n","    results_df = pd.read_csv(filepath)\n","    results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","    results_df.to_csv(filepath, index=False)\n","\n","  append_and_save_result(FILEPATH_TX, params_tx, end)\n","  append_and_save_result(FILEPATH_WALLET, params_wallet, end)\n","  if end:\n","    df_comb = pd.read_csv(path_comb)\n","    filtered_params = {key: params_tx[key] for key in params_tx if \"train\" not in key and \"val\" not in key}\n","    print(filtered_params)\n","    df_comb = pd.concat([df_comb, pd.DataFrame([filtered_params])], ignore_index=True)\n","    df_comb.to_csv(path_comb, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vo1zTF6zxQR"},"outputs":[],"source":["def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def eval(model, data, out_tx, out_wallet, params):\n","\n","  class_weights = compute_class_weights(data)\n","  criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n","\n","  model.eval()  # Imposta il modello in modalità di valutazione\n","\n","  tx_mask = data['tx'].train_mask\n","  wallet_mask = data['wallet'].train_mask\n","  tx_mask_val =  data['tx'].val_mask\n","  wallet_mask_val = data['wallet'].val_mask\n","\n","  params_tx = copy.copy(params)\n","  params_wallet = copy.copy(params)\n","\n","  # Calculate metrics for transactions\n","  params_tx['train_loss'] = criterion(out_tx[tx_mask], data['tx'].y[tx_mask])  # Convert to scalar\n","  params_tx['train_acc'] = accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_f1'] = f1_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  params_tx['train_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask].cpu(), out_tx[tx_mask].argmax(dim=1).cpu())\n","  # Calculate metrics for wallets\n","  params_wallet['train_loss'] = criterion(out_wallet[wallet_mask], data['wallet'].y[wallet_mask])  # Convert to scalar\n","  params_wallet['train_acc'] = accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_f1'] = f1_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","  params_wallet['train_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask].cpu(), out_wallet[wallet_mask].argmax(dim=1).cpu())\n","\n","  loss = params_tx['train_loss'] + params_wallet['train_loss']\n","\n","  with torch.no_grad():\n","    params_tx['val_loss'] = criterion(out_tx[tx_mask_val], data['tx'].y[tx_mask_val])  # Convert to scalar\n","    params_tx['val_acc'] = accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_precision'] = precision_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_recall'] = recall_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_f1'] = f1_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","    params_tx['val_balanced_acc'] = balanced_accuracy_score(data['tx'].y[tx_mask_val].cpu(), out_tx[tx_mask_val].argmax(dim=1).cpu())\n","\n","    # Calculate metrics for wallets\n","    params_wallet['val_loss'] = criterion(out_wallet[wallet_mask_val], data['wallet'].y[wallet_mask_val])  # Convert to scalar\n","    params_wallet['val_acc'] = accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_precision'] = precision_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_recall'] = recall_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_f1'] = f1_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","    params_wallet['val_balanced_acc'] = balanced_accuracy_score(data['wallet'].y[wallet_mask_val].cpu(), out_wallet[wallet_mask_val].argmax(dim=1).cpu())\n","\n","    print(f\"Epoch {str(params['epoch']).zfill(3)}:      TX: Train Loss: {params_tx['train_loss']:.4f}, Acc: {params_tx['train_acc']:.4f}, F1: {params_tx['train_f1']:.4f} Bal: {params_tx['train_balanced_acc']:.4f} - Val Loss: {params_tx['val_loss']:.4f}, Accuracy: {params_tx['val_acc']:.4f}, F1: {params_tx['val_f1']:.4f} Bal: {params_tx['val_balanced_acc']:.4f}\")\n","    print(f\"           WALLETS: Train Loss: {params_wallet['train_loss']:.8f}, Acc: {params_wallet['train_acc']:.8f}, F1: {params_wallet['train_f1']:.8f} Bal: {params_wallet['train_balanced_acc']:.4f} - Val Loss: {params_wallet['val_loss']:.8f}, Accuracy: {params_wallet['val_acc']:.4f}, F1: {params_wallet['val_f1']:.4f} Bal: {params_wallet['val_balanced_acc']:.4f}\")\n","\n","  return loss, params_tx, params_wallet\n","\n","def compute_class_weights(data):\n","    class_counts = torch.bincount(data['tx'].y)\n","    weights = 1.0 / class_counts.float()\n","    weights /= weights.sum()\n","    return weights\n","\n","def train(model, data, params):\n","    if params['optimizer'] == 'Adam':\n","      optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    elif params['optimizer'] == 'AdamW':\n","      optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n","    else:\n","      optimizer = None\n","\n","    if params['lr_scheduler'] == 'ReduceLROnPlateau':\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=params['factor'], patience=params['p'])\n","    elif params['lr_scheduler'] == 'CosineAnnealingLR':\n","      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=params['eta_min'])\n","    elif params['lr_scheduler'] == 'StepLR':\n","      scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n","    else:\n","      scheduler = None\n","\n","    print(f'Combinazione: {params}')\n","    model.train()\n","    tx_mask = data['tx'].train_mask\n","    wallet_mask = data['wallet'].train_mask\n","    tx_mask_val =  data['tx'].val_mask\n","    wallet_mask_val = data['wallet'].val_mask\n","\n","    best_val_tx_loss = float('inf')\n","    best_val_wallet_loss = float('inf')\n","    patience = params['patience']\n","    epochs_since_best = 0\n","\n","    for epoch in range(params['num_epoch']):\n","        params['epoch'] = epoch+1\n","        optimizer.zero_grad()\n","        out_tx, out_wallet = model(data.x_dict, data.edge_index_dict)\n","        loss, params_tx, params_wallet = eval(model, data, out_tx, out_wallet, params)\n","\n","        loss.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","        scheduler.step(loss)\n","\n","        val_tx_loss = params_tx['val_loss']\n","        val_wallet_loss = params_wallet['val_loss']\n","\n","        # Check if validation loss has improved\n","        if val_tx_loss < best_val_tx_loss or val_wallet_loss < best_val_wallet_loss:\n","            best_val_tx_loss = val_tx_loss\n","            best_val_wallet_loss = val_wallet_loss\n","            epochs_since_best = 0\n","        else:\n","            epochs_since_best += 1\n","\n","        # Check if early stopping criteria is met\n","        if epochs_since_best >= patience:\n","            print(f'Early stopping at epoch {epoch}')\n","            append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, True)\n","            break\n","\n","        append_and_save_results(FILEPATH_TX, FILEPATH_WALLET, params_tx, params_wallet, params['epoch']==params['num_epoch'])\n","\n","    return model\n","\n","\n","def train_grid(data_full, param_grid, scalers, dim_reductions, pca_thresholds):\n","    best_model = None\n","    best_f1 = 0\n","\n","    keys, values = zip(*param_grid.items())\n","    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","    combination_counter = 0\n","    total_combinations = len(param_combinations) * len(scalers) * len(dim_reductions) * len(pca_thresholds)\n","\n","    for scaler in scalers:\n","      data = scale_features(data_full.clone(), scaler)\n","      for dim_reduction in dim_reductions:\n","        for pca_threshold in pca_thresholds:\n","          data = dimentional_reduction(data, dim_reduction, pca_threshold)\n","\n","          for params in param_combinations:\n","            combination_counter += 1\n","\n","            set_seed(SEED)\n","            params['scaler'] = scaler\n","            params['dim_reduction'] = dim_reduction # Fixed the typo here: 'dim_reduction' instead of 'dim_reducition'\n","\n","            if params['lr_scheduler'] != 'ReduceLROnPlateau':\n","              params['p'] = None\n","              params['factor'] = None\n","            elif params['lr_scheduler'] != 'CosineAnnealingLR':\n","              params['T_max'] = None\n","              params['eta_min'] = None\n","\n","            if params['conv_type'] != 'Transformer':\n","              params['num_head'] = None\n","\n","            if dim_reduction == 'no':\n","              params['pca_threshold'] = None\n","            else:\n","              params['pca_threshold'] = pca_threshold\n","\n","            if not is_combination_tested(path_comb, params):\n","              print(f\"  Combinazione {combination_counter}/{total_combinations}\")  # Print the counter\n","              model = None\n","              if params[ 'type_model'] == 'HeteroGNN':\n","                model = HeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              elif params[ 'type_model'] == 'ResidualHeteroGNN':\n","                model = ResidualHeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n","                                  aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n","              model = train(model, data, params)\n","            else:\n","              print(f\"  Configurazione {combination_counter}/{total_combinations} già testata, salto...\")\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOYfyiGS4aMO"},"outputs":[],"source":["set_seed(SEED)\n","data = data_to_pyg(*load_data())"]},{"cell_type":"code","source":["hyperparams = {\n","    \"hidden_channels\": [32],\n","    'num_head': [2],\n","    \"num_layers\": [2],\n","    \"num_epoch\": [301],\n","    \"patience\": [50],\n","    \"lr\": [0.001],\n","    \"weight_decay\": [0],\n","    \"dropout\": [0],\n","    \"conv_type\": ['SAGE'],\n","    \"p\": [5],\n","    \"factor\": [0.5],\n","    \"eta_min\": [1e-5],\n","    \"T_max\": [10], #10 15\n","    \"aggr\": ['sum'], #,\n","    'lr_scheduler':['CosineAnnealingLR'],\n","    'optimizer': ['Adam'], #Adam\n","    'type_model':['HeteroGNN'],\n","}\n","\n","scaler = ['standard_l2']\n","dim_reduction=['pca']\n","pca_threshold=[0.99]\n","best_model = train_grid(data, hyperparams, scaler, dim_reduction, pca_threshold)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RhZA4OPzvc1b","executionInfo":{"status":"error","timestamp":1742585080133,"user_tz":-60,"elapsed":1692931,"user":{"displayName":"Milena Mazza","userId":"07299589247263017405"}},"outputId":"42d18992-415f-4ecc-d752-1ff1b72bceed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Numero di componenti principali per tx: 73\n","  Numero di componenti principali per wallet: 22\n","  Combinazione 1/1\n","Combinazione: {'hidden_channels': 32, 'num_head': None, 'num_layers': 2, 'num_epoch': 301, 'patience': 50, 'lr': 0.001, 'weight_decay': 0, 'dropout': 0, 'conv_type': 'SAGE', 'p': None, 'factor': None, 'eta_min': 1e-05, 'T_max': 10, 'aggr': 'sum', 'lr_scheduler': 'CosineAnnealingLR', 'optimizer': 'Adam', 'type_model': 'HeteroGNN', 'scaler': 'standard_l2', 'dim_reduction': 'pca', 'pca_threshold': 0.99}\n","Epoch 001:      TX: Train Loss: 0.6918, Acc: 0.7515, F1: 0.8541 Bal: 0.5251 - Val Loss: 0.6908, Accuracy: 0.7525, F1: 0.8543 Bal: 0.5395\n","           WALLETS: Train Loss: 0.69749928, Acc: 0.46947795, F1: 0.62001809 Bal: 0.4711 - Val Loss: 0.69784039, Accuracy: 0.4644, F1: 0.6145 Bal: 0.4700\n","Epoch 002:      TX: Train Loss: 0.6897, Acc: 0.7136, F1: 0.8256 Bal: 0.5560 - Val Loss: 0.6889, Accuracy: 0.7172, F1: 0.8274 Bal: 0.5739\n","           WALLETS: Train Loss: 0.69436580, Acc: 0.53858072, F1: 0.68681983 Bal: 0.4847 - Val Loss: 0.69481456, Accuracy: 0.5353, F1: 0.6834 Bal: 0.4849\n","Epoch 003:      TX: Train Loss: 0.6876, Acc: 0.6749, F1: 0.7945 Bal: 0.5821 - Val Loss: 0.6870, Accuracy: 0.6716, F1: 0.7908 Bal: 0.5997\n","           WALLETS: Train Loss: 0.69153792, Acc: 0.62898579, F1: 0.76376859 Bal: 0.5136 - Val Loss: 0.69209003, Accuracy: 0.6262, F1: 0.7612 Bal: 0.5146\n","Epoch 004:      TX: Train Loss: 0.6856, Acc: 0.6360, F1: 0.7606 Bal: 0.6106 - Val Loss: 0.6851, Accuracy: 0.6334, F1: 0.7574 Bal: 0.6246\n","           WALLETS: Train Loss: 0.68882787, Acc: 0.70074817, F1: 0.81838461 Bal: 0.5365 - Val Loss: 0.68946695, Accuracy: 0.7002, F1: 0.8178 Bal: 0.5379\n","Epoch 005:      TX: Train Loss: 0.6835, Acc: 0.6039, F1: 0.7304 Bal: 0.6343 - Val Loss: 0.6832, Accuracy: 0.6009, F1: 0.7276 Bal: 0.6380\n","           WALLETS: Train Loss: 0.68621063, Acc: 0.83307209, F1: 0.90664594 Bal: 0.5844 - Val Loss: 0.68692744, Accuracy: 0.8319, F1: 0.9059 Bal: 0.5838\n","Epoch 006:      TX: Train Loss: 0.6813, Acc: 0.5814, F1: 0.7082 Bal: 0.6501 - Val Loss: 0.6812, Accuracy: 0.5832, F1: 0.7095 Bal: 0.6557\n","           WALLETS: Train Loss: 0.68365377, Acc: 0.89839770, F1: 0.94554619 Bal: 0.5831 - Val Loss: 0.68443775, Accuracy: 0.8970, F1: 0.9448 Bal: 0.5801\n","Epoch 007:      TX: Train Loss: 0.6791, Acc: 0.5701, F1: 0.6960 Bal: 0.6628 - Val Loss: 0.6790, Accuracy: 0.5687, F1: 0.6945 Bal: 0.6663\n","           WALLETS: Train Loss: 0.68112773, Acc: 0.90930103, F1: 0.95185264 Bal: 0.5683 - Val Loss: 0.68197191, Accuracy: 0.9083, F1: 0.9513 Bal: 0.5661\n","Epoch 008:      TX: Train Loss: 0.6767, Acc: 0.5692, F1: 0.6939 Bal: 0.6757 - Val Loss: 0.6767, Accuracy: 0.5647, F1: 0.6893 Bal: 0.6798\n","           WALLETS: Train Loss: 0.67863935, Acc: 0.91332492, F1: 0.95420842 Bal: 0.5556 - Val Loss: 0.67953217, Accuracy: 0.9119, F1: 0.9534 Bal: 0.5536\n","Epoch 009:      TX: Train Loss: 0.6741, Acc: 0.5761, F1: 0.6997 Bal: 0.6861 - Val Loss: 0.6743, Accuracy: 0.5715, F1: 0.6949 Bal: 0.6914\n","           WALLETS: Train Loss: 0.67614734, Acc: 0.92136181, F1: 0.95875306 Bal: 0.5432 - Val Loss: 0.67707515, Accuracy: 0.9204, F1: 0.9582 Bal: 0.5422\n","Epoch 010:      TX: Train Loss: 0.6713, Acc: 0.5874, F1: 0.7096 Bal: 0.6972 - Val Loss: 0.6717, Accuracy: 0.5847, F1: 0.7067 Bal: 0.7017\n","           WALLETS: Train Loss: 0.67361456, Acc: 0.92230156, F1: 0.95929346 Bal: 0.5399 - Val Loss: 0.67457360, Accuracy: 0.9209, F1: 0.9585 Bal: 0.5382\n","Epoch 011:      TX: Train Loss: 0.6683, Acc: 0.6045, F1: 0.7248 Bal: 0.7086 - Val Loss: 0.6688, Accuracy: 0.6033, F1: 0.7233 Bal: 0.7150\n","           WALLETS: Train Loss: 0.67102498, Acc: 0.92358964, F1: 0.96000866 Bal: 0.5388 - Val Loss: 0.67200994, Accuracy: 0.9226, F1: 0.9595 Bal: 0.5371\n","Epoch 012:      TX: Train Loss: 0.6651, Acc: 0.6224, F1: 0.7405 Bal: 0.7180 - Val Loss: 0.6657, Accuracy: 0.6240, F1: 0.7416 Bal: 0.7235\n","           WALLETS: Train Loss: 0.66837132, Acc: 0.92309981, F1: 0.95976032 Bal: 0.5359 - Val Loss: 0.66937780, Accuracy: 0.9216, F1: 0.9590 Bal: 0.5332\n","Epoch 013:      TX: Train Loss: 0.6616, Acc: 0.6412, F1: 0.7565 Bal: 0.7290 - Val Loss: 0.6624, Accuracy: 0.6439, F1: 0.7587 Bal: 0.7336\n","           WALLETS: Train Loss: 0.66564482, Acc: 0.92283494, F1: 0.95961152 Bal: 0.5364 - Val Loss: 0.66667241, Accuracy: 0.9211, F1: 0.9587 Bal: 0.5328\n","Epoch 014:      TX: Train Loss: 0.6578, Acc: 0.6597, F1: 0.7719 Bal: 0.7395 - Val Loss: 0.6588, Accuracy: 0.6632, F1: 0.7748 Bal: 0.7433\n","           WALLETS: Train Loss: 0.66283196, Acc: 0.92254104, F1: 0.95943034 Bal: 0.5392 - Val Loss: 0.66387427, Accuracy: 0.9207, F1: 0.9584 Bal: 0.5359\n","Epoch 015:      TX: Train Loss: 0.6537, Acc: 0.6767, F1: 0.7857 Bal: 0.7493 - Val Loss: 0.6549, Accuracy: 0.6803, F1: 0.7887 Bal: 0.7518\n","           WALLETS: Train Loss: 0.65991431, Acc: 0.92090100, F1: 0.95850575 Bal: 0.5424 - Val Loss: 0.66096717, Accuracy: 0.9190, F1: 0.9575 Bal: 0.5386\n","Epoch 016:      TX: Train Loss: 0.6493, Acc: 0.6932, F1: 0.7988 Bal: 0.7585 - Val Loss: 0.6506, Accuracy: 0.6931, F1: 0.7989 Bal: 0.7579\n","           WALLETS: Train Loss: 0.65687722, Acc: 0.91958752, F1: 0.95775915 Bal: 0.5455 - Val Loss: 0.65793788, Accuracy: 0.9174, F1: 0.9566 Bal: 0.5413\n","Epoch 017:      TX: Train Loss: 0.6445, Acc: 0.7075, F1: 0.8100 Bal: 0.7662 - Val Loss: 0.6460, Accuracy: 0.7078, F1: 0.8104 Bal: 0.7661\n","           WALLETS: Train Loss: 0.65370655, Acc: 0.91798740, F1: 0.95685536 Bal: 0.5481 - Val Loss: 0.65477121, Accuracy: 0.9157, F1: 0.9556 Bal: 0.5452\n","Epoch 018:      TX: Train Loss: 0.6393, Acc: 0.7198, F1: 0.8194 Bal: 0.7737 - Val Loss: 0.6411, Accuracy: 0.7231, F1: 0.8222 Bal: 0.7726\n","           WALLETS: Train Loss: 0.65039569, Acc: 0.91655781, F1: 0.95601413 Bal: 0.5545 - Val Loss: 0.65146130, Accuracy: 0.9144, F1: 0.9548 Bal: 0.5520\n","Epoch 019:      TX: Train Loss: 0.6337, Acc: 0.7297, F1: 0.8268 Bal: 0.7799 - Val Loss: 0.6357, Accuracy: 0.7350, F1: 0.8310 Bal: 0.7802\n","           WALLETS: Train Loss: 0.64693117, Acc: 0.91606435, F1: 0.95560336 Bal: 0.5718 - Val Loss: 0.64799941, Accuracy: 0.9141, F1: 0.9545 Bal: 0.5713\n","Epoch 020:      TX: Train Loss: 0.6278, Acc: 0.7389, F1: 0.8338 Bal: 0.7854 - Val Loss: 0.6300, Accuracy: 0.7442, F1: 0.8378 Bal: 0.7863\n","           WALLETS: Train Loss: 0.64330429, Acc: 0.91305641, F1: 0.95383204 Bal: 0.5831 - Val Loss: 0.64437544, Accuracy: 0.9109, F1: 0.9526 Bal: 0.5823\n","Epoch 021:      TX: Train Loss: 0.6214, Acc: 0.7471, F1: 0.8397 Bal: 0.7910 - Val Loss: 0.6238, Accuracy: 0.7497, F1: 0.8418 Bal: 0.7903\n","           WALLETS: Train Loss: 0.63949305, Acc: 0.91009564, F1: 0.95200908 Bal: 0.6018 - Val Loss: 0.64055794, Accuracy: 0.9078, F1: 0.9507 Bal: 0.5997\n","Epoch 022:      TX: Train Loss: 0.6146, Acc: 0.7536, F1: 0.8445 Bal: 0.7948 - Val Loss: 0.6172, Accuracy: 0.7565, F1: 0.8467 Bal: 0.7961\n","           WALLETS: Train Loss: 0.63549346, Acc: 0.90636928, F1: 0.94971481 Bal: 0.6224 - Val Loss: 0.63653964, Accuracy: 0.9046, F1: 0.9487 Bal: 0.6231\n","Epoch 023:      TX: Train Loss: 0.6074, Acc: 0.7586, F1: 0.8481 Bal: 0.7980 - Val Loss: 0.6103, Accuracy: 0.7633, F1: 0.8515 Bal: 0.8018\n","           WALLETS: Train Loss: 0.63130546, Acc: 0.90317267, F1: 0.94765809 Bal: 0.6470 - Val Loss: 0.63232744, Accuracy: 0.9016, F1: 0.9467 Bal: 0.6471\n","Epoch 024:      TX: Train Loss: 0.5997, Acc: 0.7635, F1: 0.8517 Bal: 0.8010 - Val Loss: 0.6030, Accuracy: 0.7683, F1: 0.8551 Bal: 0.8056\n","           WALLETS: Train Loss: 0.62692475, Acc: 0.89964587, F1: 0.94542103 Bal: 0.6677 - Val Loss: 0.62791181, Accuracy: 0.8975, F1: 0.9442 Bal: 0.6666\n","Epoch 025:      TX: Train Loss: 0.5917, Acc: 0.7681, F1: 0.8549 Bal: 0.8039 - Val Loss: 0.5952, Accuracy: 0.7716, F1: 0.8575 Bal: 0.8064\n","           WALLETS: Train Loss: 0.62235671, Acc: 0.89675767, F1: 0.94353536 Bal: 0.6879 - Val Loss: 0.62330490, Accuracy: 0.8950, F1: 0.9425 Bal: 0.6895\n","Epoch 026:      TX: Train Loss: 0.5833, Acc: 0.7717, F1: 0.8575 Bal: 0.8053 - Val Loss: 0.5871, Accuracy: 0.7747, F1: 0.8597 Bal: 0.8072\n","           WALLETS: Train Loss: 0.61760110, Acc: 0.89390938, F1: 0.94171957 Bal: 0.7019 - Val Loss: 0.61850470, Accuracy: 0.8926, F1: 0.9409 Bal: 0.7025\n","Epoch 027:      TX: Train Loss: 0.5745, Acc: 0.7764, F1: 0.8608 Bal: 0.8084 - Val Loss: 0.5786, Accuracy: 0.7782, F1: 0.8622 Bal: 0.8101\n","           WALLETS: Train Loss: 0.61265296, Acc: 0.89183756, F1: 0.94036736 Bal: 0.7140 - Val Loss: 0.61351377, Accuracy: 0.8907, F1: 0.9397 Bal: 0.7140\n","Epoch 028:      TX: Train Loss: 0.5654, Acc: 0.7806, F1: 0.8638 Bal: 0.8103 - Val Loss: 0.5697, Accuracy: 0.7806, F1: 0.8639 Bal: 0.8114\n","           WALLETS: Train Loss: 0.60750467, Acc: 0.88930857, F1: 0.93878779 Bal: 0.7214 - Val Loss: 0.60832471, Accuracy: 0.8886, F1: 0.9383 Bal: 0.7220\n","Epoch 029:      TX: Train Loss: 0.5559, Acc: 0.7844, F1: 0.8665 Bal: 0.8123 - Val Loss: 0.5605, Accuracy: 0.7846, F1: 0.8666 Bal: 0.8146\n","           WALLETS: Train Loss: 0.60216177, Acc: 0.88741092, F1: 0.93759452 Bal: 0.7270 - Val Loss: 0.60294926, Accuracy: 0.8871, F1: 0.9374 Bal: 0.7265\n","Epoch 030:      TX: Train Loss: 0.5461, Acc: 0.7894, F1: 0.8700 Bal: 0.8149 - Val Loss: 0.5510, Accuracy: 0.7885, F1: 0.8694 Bal: 0.8168\n","           WALLETS: Train Loss: 0.59664345, Acc: 0.88607567, F1: 0.93672182 Bal: 0.7335 - Val Loss: 0.59739858, Accuracy: 0.8854, F1: 0.9363 Bal: 0.7325\n","Epoch 031:      TX: Train Loss: 0.5360, Acc: 0.7935, F1: 0.8729 Bal: 0.8176 - Val Loss: 0.5412, Accuracy: 0.7953, F1: 0.8741 Bal: 0.8215\n","           WALLETS: Train Loss: 0.59096020, Acc: 0.88512866, F1: 0.93605883 Bal: 0.7416 - Val Loss: 0.59168315, Accuracy: 0.8847, F1: 0.9357 Bal: 0.7402\n","Epoch 032:      TX: Train Loss: 0.5257, Acc: 0.7980, F1: 0.8759 Bal: 0.8204 - Val Loss: 0.5311, Accuracy: 0.8021, F1: 0.8787 Bal: 0.8273\n","           WALLETS: Train Loss: 0.58511984, Acc: 0.88423971, F1: 0.93546743 Bal: 0.7465 - Val Loss: 0.58581686, Accuracy: 0.8840, F1: 0.9353 Bal: 0.7448\n","Epoch 033:      TX: Train Loss: 0.5151, Acc: 0.8024, F1: 0.8790 Bal: 0.8229 - Val Loss: 0.5208, Accuracy: 0.8063, F1: 0.8816 Bal: 0.8286\n","           WALLETS: Train Loss: 0.57914150, Acc: 0.88298428, F1: 0.93466820 Bal: 0.7501 - Val Loss: 0.57981265, Accuracy: 0.8823, F1: 0.9342 Bal: 0.7491\n","Epoch 034:      TX: Train Loss: 0.5044, Acc: 0.8073, F1: 0.8823 Bal: 0.8258 - Val Loss: 0.5103, Accuracy: 0.8102, F1: 0.8842 Bal: 0.8318\n","           WALLETS: Train Loss: 0.57303286, Acc: 0.88232029, F1: 0.93421365 Bal: 0.7546 - Val Loss: 0.57366949, Accuracy: 0.8818, F1: 0.9339 Bal: 0.7539\n","Epoch 035:      TX: Train Loss: 0.4935, Acc: 0.8117, F1: 0.8853 Bal: 0.8287 - Val Loss: 0.4996, Accuracy: 0.8131, F1: 0.8862 Bal: 0.8324\n","           WALLETS: Train Loss: 0.56679827, Acc: 0.88171072, F1: 0.93381005 Bal: 0.7574 - Val Loss: 0.56740010, Accuracy: 0.8813, F1: 0.9335 Bal: 0.7572\n","Epoch 036:      TX: Train Loss: 0.4825, Acc: 0.8160, F1: 0.8882 Bal: 0.8312 - Val Loss: 0.4889, Accuracy: 0.8172, F1: 0.8890 Bal: 0.8347\n","           WALLETS: Train Loss: 0.56046069, Acc: 0.88115194, F1: 0.93343738 Bal: 0.7603 - Val Loss: 0.56102550, Accuracy: 0.8809, F1: 0.9332 Bal: 0.7593\n","Epoch 037:      TX: Train Loss: 0.4715, Acc: 0.8199, F1: 0.8908 Bal: 0.8334 - Val Loss: 0.4781, Accuracy: 0.8208, F1: 0.8914 Bal: 0.8376\n","           WALLETS: Train Loss: 0.55404258, Acc: 0.88060406, F1: 0.93307790 Bal: 0.7625 - Val Loss: 0.55456662, Accuracy: 0.8800, F1: 0.9327 Bal: 0.7609\n","Epoch 038:      TX: Train Loss: 0.4605, Acc: 0.8233, F1: 0.8931 Bal: 0.8351 - Val Loss: 0.4672, Accuracy: 0.8240, F1: 0.8936 Bal: 0.8395\n","           WALLETS: Train Loss: 0.54756999, Acc: 0.87971510, F1: 0.93252131 Bal: 0.7639 - Val Loss: 0.54805028, Accuracy: 0.8789, F1: 0.9320 Bal: 0.7622\n","Epoch 039:      TX: Train Loss: 0.4495, Acc: 0.8267, F1: 0.8954 Bal: 0.8375 - Val Loss: 0.4565, Accuracy: 0.8273, F1: 0.8958 Bal: 0.8413\n","           WALLETS: Train Loss: 0.54107475, Acc: 0.87931960, F1: 0.93224358 Bal: 0.7669 - Val Loss: 0.54150158, Accuracy: 0.8787, F1: 0.9318 Bal: 0.7643\n","Epoch 040:      TX: Train Loss: 0.4387, Acc: 0.8307, F1: 0.8980 Bal: 0.8404 - Val Loss: 0.4458, Accuracy: 0.8304, F1: 0.8978 Bal: 0.8420\n","           WALLETS: Train Loss: 0.53457707, Acc: 0.87873180, F1: 0.93186470 Bal: 0.7686 - Val Loss: 0.53494114, Accuracy: 0.8781, F1: 0.9315 Bal: 0.7657\n","Epoch 041:      TX: Train Loss: 0.4281, Acc: 0.8343, F1: 0.9004 Bal: 0.8428 - Val Loss: 0.4353, Accuracy: 0.8333, F1: 0.8998 Bal: 0.8426\n","           WALLETS: Train Loss: 0.52812004, Acc: 0.87802790, F1: 0.93141328 Bal: 0.7704 - Val Loss: 0.52841610, Accuracy: 0.8777, F1: 0.9312 Bal: 0.7678\n","Epoch 042:      TX: Train Loss: 0.4177, Acc: 0.8372, F1: 0.9023 Bal: 0.8448 - Val Loss: 0.4249, Accuracy: 0.8357, F1: 0.9014 Bal: 0.8440\n","           WALLETS: Train Loss: 0.52173954, Acc: 0.87754895, F1: 0.93109644 Bal: 0.7724 - Val Loss: 0.52195626, Accuracy: 0.8772, F1: 0.9309 Bal: 0.7699\n","Epoch 043:      TX: Train Loss: 0.4076, Acc: 0.8409, F1: 0.9047 Bal: 0.8468 - Val Loss: 0.4149, Accuracy: 0.8407, F1: 0.9047 Bal: 0.8468\n","           WALLETS: Train Loss: 0.51546425, Acc: 0.87696478, F1: 0.93071666 Bal: 0.7742 - Val Loss: 0.51560336, Accuracy: 0.8768, F1: 0.9305 Bal: 0.7723\n","Epoch 044:      TX: Train Loss: 0.3978, Acc: 0.8439, F1: 0.9067 Bal: 0.8490 - Val Loss: 0.4051, Accuracy: 0.8436, F1: 0.9066 Bal: 0.8474\n","           WALLETS: Train Loss: 0.50932670, Acc: 0.87642777, F1: 0.93036304 Bal: 0.7762 - Val Loss: 0.50939178, Accuracy: 0.8764, F1: 0.9303 Bal: 0.7750\n","Epoch 045:      TX: Train Loss: 0.3883, Acc: 0.8474, F1: 0.9089 Bal: 0.8524 - Val Loss: 0.3956, Accuracy: 0.8477, F1: 0.9093 Bal: 0.8497\n","           WALLETS: Train Loss: 0.50336260, Acc: 0.87579643, F1: 0.92994853 Bal: 0.7784 - Val Loss: 0.50335193, Accuracy: 0.8759, F1: 0.9300 Bal: 0.7772\n","Epoch 046:      TX: Train Loss: 0.3792, Acc: 0.8506, F1: 0.9110 Bal: 0.8546 - Val Loss: 0.3864, Accuracy: 0.8510, F1: 0.9114 Bal: 0.8515\n","           WALLETS: Train Loss: 0.49760592, Acc: 0.87508527, F1: 0.92948423 Bal: 0.7806 - Val Loss: 0.49751818, Accuracy: 0.8754, F1: 0.9296 Bal: 0.7801\n","Epoch 047:      TX: Train Loss: 0.3705, Acc: 0.8535, F1: 0.9129 Bal: 0.8560 - Val Loss: 0.3777, Accuracy: 0.8545, F1: 0.9137 Bal: 0.8544\n","           WALLETS: Train Loss: 0.49209374, Acc: 0.87439224, F1: 0.92903937 Bal: 0.7821 - Val Loss: 0.49192920, Accuracy: 0.8743, F1: 0.9289 Bal: 0.7812\n","Epoch 048:      TX: Train Loss: 0.3622, Acc: 0.8566, F1: 0.9149 Bal: 0.8585 - Val Loss: 0.3693, Accuracy: 0.8574, F1: 0.9155 Bal: 0.8570\n","           WALLETS: Train Loss: 0.48685700, Acc: 0.87357223, F1: 0.92851692 Bal: 0.7836 - Val Loss: 0.48661250, Accuracy: 0.8735, F1: 0.9284 Bal: 0.7823\n","Epoch 049:      TX: Train Loss: 0.3543, Acc: 0.8588, F1: 0.9163 Bal: 0.8603 - Val Loss: 0.3613, Accuracy: 0.8605, F1: 0.9174 Bal: 0.8607\n","           WALLETS: Train Loss: 0.48192397, Acc: 0.87241114, F1: 0.92778725 Bal: 0.7848 - Val Loss: 0.48159644, Accuracy: 0.8719, F1: 0.9274 Bal: 0.7839\n","Epoch 050:      TX: Train Loss: 0.3468, Acc: 0.8615, F1: 0.9180 Bal: 0.8628 - Val Loss: 0.3537, Accuracy: 0.8627, F1: 0.9188 Bal: 0.8609\n","           WALLETS: Train Loss: 0.47731161, Acc: 0.87172900, F1: 0.92732864 Bal: 0.7877 - Val Loss: 0.47690162, Accuracy: 0.8714, F1: 0.9271 Bal: 0.7879\n","Epoch 051:      TX: Train Loss: 0.3397, Acc: 0.8637, F1: 0.9194 Bal: 0.8653 - Val Loss: 0.3465, Accuracy: 0.8651, F1: 0.9203 Bal: 0.8642\n","           WALLETS: Train Loss: 0.47303843, Acc: 0.87111580, F1: 0.92692761 Bal: 0.7893 - Val Loss: 0.47254628, Accuracy: 0.8708, F1: 0.9267 Bal: 0.7894\n","Epoch 052:      TX: Train Loss: 0.3331, Acc: 0.8661, F1: 0.9209 Bal: 0.8675 - Val Loss: 0.3397, Accuracy: 0.8666, F1: 0.9213 Bal: 0.8651\n","           WALLETS: Train Loss: 0.46912014, Acc: 0.87024862, F1: 0.92637916 Bal: 0.7903 - Val Loss: 0.46854636, Accuracy: 0.8699, F1: 0.9261 Bal: 0.7900\n","Epoch 053:      TX: Train Loss: 0.3269, Acc: 0.8680, F1: 0.9221 Bal: 0.8696 - Val Loss: 0.3332, Accuracy: 0.8668, F1: 0.9214 Bal: 0.8652\n","           WALLETS: Train Loss: 0.46555707, Acc: 0.87009623, F1: 0.92616987 Bal: 0.7985 - Val Loss: 0.46491149, Accuracy: 0.8696, F1: 0.9258 Bal: 0.7974\n","Epoch 054:      TX: Train Loss: 0.3210, Acc: 0.8704, F1: 0.9236 Bal: 0.8717 - Val Loss: 0.3272, Accuracy: 0.8690, F1: 0.9228 Bal: 0.8664\n","           WALLETS: Train Loss: 0.46234429, Acc: 0.86912382, F1: 0.92554597 Bal: 0.8000 - Val Loss: 0.46163300, Accuracy: 0.8686, F1: 0.9251 Bal: 0.7998\n","Epoch 055:      TX: Train Loss: 0.3155, Acc: 0.8718, F1: 0.9245 Bal: 0.8730 - Val Loss: 0.3215, Accuracy: 0.8703, F1: 0.9237 Bal: 0.8671\n","           WALLETS: Train Loss: 0.45947367, Acc: 0.86814052, F1: 0.92491917 Bal: 0.8013 - Val Loss: 0.45870632, Accuracy: 0.8678, F1: 0.9246 Bal: 0.8005\n","Epoch 056:      TX: Train Loss: 0.3104, Acc: 0.8736, F1: 0.9256 Bal: 0.8741 - Val Loss: 0.3161, Accuracy: 0.8728, F1: 0.9252 Bal: 0.8704\n","           WALLETS: Train Loss: 0.45692095, Acc: 0.86733139, F1: 0.92441113 Bal: 0.8017 - Val Loss: 0.45610362, Accuracy: 0.8668, F1: 0.9240 Bal: 0.8009\n","Epoch 057:      TX: Train Loss: 0.3056, Acc: 0.8747, F1: 0.9263 Bal: 0.8748 - Val Loss: 0.3111, Accuracy: 0.8728, F1: 0.9252 Bal: 0.8695\n","           WALLETS: Train Loss: 0.45466009, Acc: 0.86648960, F1: 0.92386509 Bal: 0.8033 - Val Loss: 0.45380440, Accuracy: 0.8662, F1: 0.9236 Bal: 0.8019\n","Epoch 058:      TX: Train Loss: 0.3012, Acc: 0.8762, F1: 0.9272 Bal: 0.8762 - Val Loss: 0.3064, Accuracy: 0.8738, F1: 0.9258 Bal: 0.8720\n","           WALLETS: Train Loss: 0.45266816, Acc: 0.86496568, F1: 0.92290674 Bal: 0.8039 - Val Loss: 0.45178244, Accuracy: 0.8649, F1: 0.9228 Bal: 0.8022\n","Epoch 059:      TX: Train Loss: 0.2970, Acc: 0.8776, F1: 0.9281 Bal: 0.8775 - Val Loss: 0.3020, Accuracy: 0.8749, F1: 0.9265 Bal: 0.8726\n","           WALLETS: Train Loss: 0.45092675, Acc: 0.86350706, F1: 0.92198867 Bal: 0.8045 - Val Loss: 0.45001456, Accuracy: 0.8635, F1: 0.9219 Bal: 0.8022\n","Epoch 060:      TX: Train Loss: 0.2931, Acc: 0.8791, F1: 0.9290 Bal: 0.8788 - Val Loss: 0.2979, Accuracy: 0.8765, F1: 0.9275 Bal: 0.8755\n","           WALLETS: Train Loss: 0.44940066, Acc: 0.86183800, F1: 0.92094522 Bal: 0.8045 - Val Loss: 0.44846925, Accuracy: 0.8619, F1: 0.9209 Bal: 0.8020\n","Epoch 061:      TX: Train Loss: 0.2894, Acc: 0.8804, F1: 0.9298 Bal: 0.8803 - Val Loss: 0.2940, Accuracy: 0.8780, F1: 0.9284 Bal: 0.8763\n","           WALLETS: Train Loss: 0.44806418, Acc: 0.86102524, F1: 0.92042796 Bal: 0.8051 - Val Loss: 0.44711584, Accuracy: 0.8610, F1: 0.9203 Bal: 0.8029\n","Epoch 062:      TX: Train Loss: 0.2860, Acc: 0.8822, F1: 0.9310 Bal: 0.8813 - Val Loss: 0.2904, Accuracy: 0.8793, F1: 0.9293 Bal: 0.8770\n","           WALLETS: Train Loss: 0.44687974, Acc: 0.86008548, F1: 0.91983310 Bal: 0.8054 - Val Loss: 0.44591674, Accuracy: 0.8602, F1: 0.9198 Bal: 0.8031\n","Epoch 063:      TX: Train Loss: 0.2828, Acc: 0.8832, F1: 0.9316 Bal: 0.8817 - Val Loss: 0.2870, Accuracy: 0.8802, F1: 0.9298 Bal: 0.8785\n","           WALLETS: Train Loss: 0.44581631, Acc: 0.85951946, F1: 0.91947597 Bal: 0.8056 - Val Loss: 0.44484159, Accuracy: 0.8598, F1: 0.9196 Bal: 0.8039\n","Epoch 064:      TX: Train Loss: 0.2798, Acc: 0.8840, F1: 0.9321 Bal: 0.8825 - Val Loss: 0.2839, Accuracy: 0.8826, F1: 0.9313 Bal: 0.8798\n","           WALLETS: Train Loss: 0.44484124, Acc: 0.85910582, F1: 0.91921401 Bal: 0.8057 - Val Loss: 0.44385877, Accuracy: 0.8594, F1: 0.9193 Bal: 0.8040\n","Epoch 065:      TX: Train Loss: 0.2769, Acc: 0.8845, F1: 0.9324 Bal: 0.8827 - Val Loss: 0.2810, Accuracy: 0.8833, F1: 0.9317 Bal: 0.8812\n","           WALLETS: Train Loss: 0.44392836, Acc: 0.85858696, F1: 0.91888933 Bal: 0.8056 - Val Loss: 0.44293806, Accuracy: 0.8590, F1: 0.9190 Bal: 0.8037\n","Epoch 066:      TX: Train Loss: 0.2742, Acc: 0.8849, F1: 0.9327 Bal: 0.8831 - Val Loss: 0.2782, Accuracy: 0.8842, F1: 0.9323 Bal: 0.8807\n","           WALLETS: Train Loss: 0.44305381, Acc: 0.85806447, F1: 0.91856253 Bal: 0.8055 - Val Loss: 0.44205818, Accuracy: 0.8584, F1: 0.9187 Bal: 0.8034\n","Epoch 067:      TX: Train Loss: 0.2716, Acc: 0.8859, F1: 0.9332 Bal: 0.8839 - Val Loss: 0.2756, Accuracy: 0.8855, F1: 0.9331 Bal: 0.8804\n","           WALLETS: Train Loss: 0.44219661, Acc: 0.85769437, F1: 0.91832842 Bal: 0.8056 - Val Loss: 0.44119814, Accuracy: 0.8578, F1: 0.9183 Bal: 0.8030\n","Epoch 068:      TX: Train Loss: 0.2691, Acc: 0.8872, F1: 0.9341 Bal: 0.8845 - Val Loss: 0.2732, Accuracy: 0.8879, F1: 0.9346 Bal: 0.8818\n","           WALLETS: Train Loss: 0.44134241, Acc: 0.85697595, F1: 0.91788019 Bal: 0.8053 - Val Loss: 0.44034213, Accuracy: 0.8570, F1: 0.9178 Bal: 0.8031\n","Epoch 069:      TX: Train Loss: 0.2667, Acc: 0.8881, F1: 0.9347 Bal: 0.8854 - Val Loss: 0.2709, Accuracy: 0.8894, F1: 0.9355 Bal: 0.8836\n","           WALLETS: Train Loss: 0.44047794, Acc: 0.85662400, F1: 0.91765976 Bal: 0.8052 - Val Loss: 0.43947601, Accuracy: 0.8566, F1: 0.9176 Bal: 0.8029\n","Epoch 070:      TX: Train Loss: 0.2643, Acc: 0.8890, F1: 0.9352 Bal: 0.8868 - Val Loss: 0.2688, Accuracy: 0.8905, F1: 0.9362 Bal: 0.8862\n","           WALLETS: Train Loss: 0.43960068, Acc: 0.85633735, F1: 0.91747949 Bal: 0.8052 - Val Loss: 0.43859431, Accuracy: 0.8562, F1: 0.9173 Bal: 0.8027\n","Epoch 071:      TX: Train Loss: 0.2621, Acc: 0.8903, F1: 0.9360 Bal: 0.8879 - Val Loss: 0.2667, Accuracy: 0.8907, F1: 0.9363 Bal: 0.8863\n","           WALLETS: Train Loss: 0.43870252, Acc: 0.85639178, F1: 0.91751299 Bal: 0.8053 - Val Loss: 0.43769252, Accuracy: 0.8562, F1: 0.9173 Bal: 0.8029\n","Epoch 072:      TX: Train Loss: 0.2599, Acc: 0.8912, F1: 0.9365 Bal: 0.8885 - Val Loss: 0.2648, Accuracy: 0.8914, F1: 0.9367 Bal: 0.8867\n","           WALLETS: Train Loss: 0.43777880, Acc: 0.85638815, F1: 0.91751108 Bal: 0.8052 - Val Loss: 0.43676323, Accuracy: 0.8563, F1: 0.9174 Bal: 0.8028\n","Epoch 073:      TX: Train Loss: 0.2578, Acc: 0.8924, F1: 0.9373 Bal: 0.8894 - Val Loss: 0.2630, Accuracy: 0.8918, F1: 0.9370 Bal: 0.8869\n","           WALLETS: Train Loss: 0.43683144, Acc: 0.85653692, F1: 0.91760219 Bal: 0.8054 - Val Loss: 0.43580943, Accuracy: 0.8566, F1: 0.9176 Bal: 0.8031\n","Epoch 074:      TX: Train Loss: 0.2558, Acc: 0.8928, F1: 0.9375 Bal: 0.8900 - Val Loss: 0.2613, Accuracy: 0.8934, F1: 0.9379 Bal: 0.8878\n","           WALLETS: Train Loss: 0.43586785, Acc: 0.85672196, F1: 0.91772373 Bal: 0.8051 - Val Loss: 0.43484226, Accuracy: 0.8568, F1: 0.9177 Bal: 0.8025\n","Epoch 075:      TX: Train Loss: 0.2537, Acc: 0.8939, F1: 0.9382 Bal: 0.8907 - Val Loss: 0.2596, Accuracy: 0.8940, F1: 0.9384 Bal: 0.8881\n","           WALLETS: Train Loss: 0.43489259, Acc: 0.85715374, F1: 0.91799133 Bal: 0.8054 - Val Loss: 0.43386361, Accuracy: 0.8572, F1: 0.9179 Bal: 0.8028\n","Epoch 076:      TX: Train Loss: 0.2518, Acc: 0.8949, F1: 0.9388 Bal: 0.8915 - Val Loss: 0.2581, Accuracy: 0.8940, F1: 0.9383 Bal: 0.8891\n","           WALLETS: Train Loss: 0.43390888, Acc: 0.85753835, F1: 0.91823019 Bal: 0.8056 - Val Loss: 0.43287641, Accuracy: 0.8575, F1: 0.9182 Bal: 0.8028\n","Epoch 077:      TX: Train Loss: 0.2498, Acc: 0.8956, F1: 0.9393 Bal: 0.8924 - Val Loss: 0.2566, Accuracy: 0.8943, F1: 0.9385 Bal: 0.8892\n","           WALLETS: Train Loss: 0.43292511, Acc: 0.85803181, F1: 0.91853888 Bal: 0.8057 - Val Loss: 0.43188548, Accuracy: 0.8578, F1: 0.9183 Bal: 0.8029\n","Epoch 078:      TX: Train Loss: 0.2479, Acc: 0.8962, F1: 0.9396 Bal: 0.8929 - Val Loss: 0.2551, Accuracy: 0.8951, F1: 0.9390 Bal: 0.8897\n","           WALLETS: Train Loss: 0.43195230, Acc: 0.85847448, F1: 0.91881248 Bal: 0.8060 - Val Loss: 0.43090099, Accuracy: 0.8582, F1: 0.9186 Bal: 0.8031\n","Epoch 079:      TX: Train Loss: 0.2461, Acc: 0.8975, F1: 0.9404 Bal: 0.8941 - Val Loss: 0.2538, Accuracy: 0.8958, F1: 0.9394 Bal: 0.8901\n","           WALLETS: Train Loss: 0.43099380, Acc: 0.85892440, F1: 0.91908945 Bal: 0.8064 - Val Loss: 0.42993158, Accuracy: 0.8587, F1: 0.9189 Bal: 0.8033\n","Epoch 080:      TX: Train Loss: 0.2442, Acc: 0.8986, F1: 0.9411 Bal: 0.8951 - Val Loss: 0.2525, Accuracy: 0.8960, F1: 0.9396 Bal: 0.8892\n","           WALLETS: Train Loss: 0.43005180, Acc: 0.85950494, F1: 0.91944822 Bal: 0.8068 - Val Loss: 0.42897573, Accuracy: 0.8593, F1: 0.9193 Bal: 0.8036\n","Epoch 081:      TX: Train Loss: 0.2424, Acc: 0.8999, F1: 0.9419 Bal: 0.8965 - Val Loss: 0.2512, Accuracy: 0.8960, F1: 0.9396 Bal: 0.8892\n","           WALLETS: Train Loss: 0.42912754, Acc: 0.85970088, F1: 0.91956792 Bal: 0.8070 - Val Loss: 0.42803836, Accuracy: 0.8596, F1: 0.9194 Bal: 0.8037\n","Epoch 082:      TX: Train Loss: 0.2407, Acc: 0.9005, F1: 0.9422 Bal: 0.8972 - Val Loss: 0.2499, Accuracy: 0.8951, F1: 0.9390 Bal: 0.8878\n","           WALLETS: Train Loss: 0.42821905, Acc: 0.86018708, F1: 0.91986566 Bal: 0.8074 - Val Loss: 0.42711353, Accuracy: 0.8599, F1: 0.9196 Bal: 0.8039\n","Epoch 083:      TX: Train Loss: 0.2389, Acc: 0.9016, F1: 0.9429 Bal: 0.8985 - Val Loss: 0.2487, Accuracy: 0.8967, F1: 0.9400 Bal: 0.8886\n","           WALLETS: Train Loss: 0.42732492, Acc: 0.86048824, F1: 0.92005206 Bal: 0.8076 - Val Loss: 0.42620221, Accuracy: 0.8602, F1: 0.9198 Bal: 0.8043\n","Epoch 084:      TX: Train Loss: 0.2372, Acc: 0.9022, F1: 0.9433 Bal: 0.8994 - Val Loss: 0.2475, Accuracy: 0.8986, F1: 0.9412 Bal: 0.8897\n","           WALLETS: Train Loss: 0.42644933, Acc: 0.86065877, F1: 0.92015759 Bal: 0.8077 - Val Loss: 0.42530361, Accuracy: 0.8605, F1: 0.9200 Bal: 0.8046\n","Epoch 085:      TX: Train Loss: 0.2355, Acc: 0.9031, F1: 0.9438 Bal: 0.8999 - Val Loss: 0.2463, Accuracy: 0.8997, F1: 0.9419 Bal: 0.8903\n","           WALLETS: Train Loss: 0.42559376, Acc: 0.86085108, F1: 0.92027723 Bal: 0.8078 - Val Loss: 0.42441678, Accuracy: 0.8609, F1: 0.9203 Bal: 0.8051\n","Epoch 086:      TX: Train Loss: 0.2338, Acc: 0.9038, F1: 0.9442 Bal: 0.9001 - Val Loss: 0.2452, Accuracy: 0.9013, F1: 0.9428 Bal: 0.8922\n","           WALLETS: Train Loss: 0.42476255, Acc: 0.86103975, F1: 0.92039129 Bal: 0.8080 - Val Loss: 0.42354846, Accuracy: 0.8611, F1: 0.9204 Bal: 0.8054\n","Epoch 087:      TX: Train Loss: 0.2321, Acc: 0.9047, F1: 0.9448 Bal: 0.9006 - Val Loss: 0.2440, Accuracy: 0.9030, F1: 0.9439 Bal: 0.8931\n","           WALLETS: Train Loss: 0.42395523, Acc: 0.86126108, F1: 0.92051759 Bal: 0.8088 - Val Loss: 0.42270097, Accuracy: 0.8614, F1: 0.9205 Bal: 0.8065\n","Epoch 088:      TX: Train Loss: 0.2304, Acc: 0.9053, F1: 0.9452 Bal: 0.9011 - Val Loss: 0.2429, Accuracy: 0.9032, F1: 0.9440 Bal: 0.8933\n","           WALLETS: Train Loss: 0.42316359, Acc: 0.86145339, F1: 0.92063419 Bal: 0.8091 - Val Loss: 0.42186555, Accuracy: 0.8615, F1: 0.9206 Bal: 0.8067\n","Epoch 089:      TX: Train Loss: 0.2288, Acc: 0.9059, F1: 0.9455 Bal: 0.9017 - Val Loss: 0.2417, Accuracy: 0.9039, F1: 0.9444 Bal: 0.8926\n","           WALLETS: Train Loss: 0.42238936, Acc: 0.86177632, F1: 0.92082922 Bal: 0.8096 - Val Loss: 0.42103982, Accuracy: 0.8619, F1: 0.9208 Bal: 0.8070\n","Epoch 090:      TX: Train Loss: 0.2271, Acc: 0.9070, F1: 0.9461 Bal: 0.9028 - Val Loss: 0.2406, Accuracy: 0.9048, F1: 0.9450 Bal: 0.8931\n","           WALLETS: Train Loss: 0.42163116, Acc: 0.86192508, F1: 0.92091689 Bal: 0.8099 - Val Loss: 0.42022485, Accuracy: 0.8620, F1: 0.9209 Bal: 0.8077\n","Epoch 091:      TX: Train Loss: 0.2255, Acc: 0.9077, F1: 0.9466 Bal: 0.9033 - Val Loss: 0.2394, Accuracy: 0.9057, F1: 0.9455 Bal: 0.8936\n","           WALLETS: Train Loss: 0.42088616, Acc: 0.86204482, F1: 0.92098728 Bal: 0.8102 - Val Loss: 0.41942257, Accuracy: 0.8619, F1: 0.9208 Bal: 0.8077\n","Epoch 092:      TX: Train Loss: 0.2239, Acc: 0.9085, F1: 0.9471 Bal: 0.9040 - Val Loss: 0.2383, Accuracy: 0.9065, F1: 0.9460 Bal: 0.8941\n","           WALLETS: Train Loss: 0.42015201, Acc: 0.86228792, F1: 0.92113717 Bal: 0.8104 - Val Loss: 0.41862610, Accuracy: 0.8618, F1: 0.9208 Bal: 0.8075\n","Epoch 093:      TX: Train Loss: 0.2223, Acc: 0.9094, F1: 0.9476 Bal: 0.9047 - Val Loss: 0.2371, Accuracy: 0.9070, F1: 0.9463 Bal: 0.8953\n","           WALLETS: Train Loss: 0.41942668, Acc: 0.86249111, F1: 0.92125974 Bal: 0.8107 - Val Loss: 0.41783303, Accuracy: 0.8622, F1: 0.9210 Bal: 0.8077\n","Epoch 094:      TX: Train Loss: 0.2208, Acc: 0.9101, F1: 0.9481 Bal: 0.9050 - Val Loss: 0.2359, Accuracy: 0.9079, F1: 0.9468 Bal: 0.8958\n","           WALLETS: Train Loss: 0.41870549, Acc: 0.86271244, F1: 0.92139481 Bal: 0.8109 - Val Loss: 0.41704309, Accuracy: 0.8622, F1: 0.9210 Bal: 0.8077\n","Epoch 095:      TX: Train Loss: 0.2192, Acc: 0.9112, F1: 0.9487 Bal: 0.9062 - Val Loss: 0.2348, Accuracy: 0.9085, F1: 0.9472 Bal: 0.8962\n","           WALLETS: Train Loss: 0.41798723, Acc: 0.86265076, F1: 0.92135476 Bal: 0.8110 - Val Loss: 0.41625282, Accuracy: 0.8623, F1: 0.9211 Bal: 0.8079\n","Epoch 096:      TX: Train Loss: 0.2176, Acc: 0.9124, F1: 0.9494 Bal: 0.9072 - Val Loss: 0.2336, Accuracy: 0.9092, F1: 0.9476 Bal: 0.8975\n","           WALLETS: Train Loss: 0.41726816, Acc: 0.86272696, F1: 0.92139953 Bal: 0.8112 - Val Loss: 0.41545966, Accuracy: 0.8623, F1: 0.9211 Bal: 0.8081\n","Epoch 097:      TX: Train Loss: 0.2161, Acc: 0.9132, F1: 0.9499 Bal: 0.9081 - Val Loss: 0.2324, Accuracy: 0.9096, F1: 0.9479 Bal: 0.8978\n","           WALLETS: Train Loss: 0.41654843, Acc: 0.86280678, F1: 0.92144817 Bal: 0.8113 - Val Loss: 0.41466388, Accuracy: 0.8624, F1: 0.9211 Bal: 0.8078\n","Epoch 098:      TX: Train Loss: 0.2146, Acc: 0.9143, F1: 0.9506 Bal: 0.9089 - Val Loss: 0.2312, Accuracy: 0.9107, F1: 0.9485 Bal: 0.8994\n","           WALLETS: Train Loss: 0.41582805, Acc: 0.86283218, F1: 0.92146125 Bal: 0.8115 - Val Loss: 0.41386577, Accuracy: 0.8627, F1: 0.9213 Bal: 0.8083\n","Epoch 099:      TX: Train Loss: 0.2130, Acc: 0.9149, F1: 0.9510 Bal: 0.9097 - Val Loss: 0.2300, Accuracy: 0.9122, F1: 0.9495 Bal: 0.9002\n","           WALLETS: Train Loss: 0.41510507, Acc: 0.86293015, F1: 0.92151750 Bal: 0.8118 - Val Loss: 0.41306475, Accuracy: 0.8629, F1: 0.9215 Bal: 0.8086\n","Epoch 100:      TX: Train Loss: 0.2115, Acc: 0.9157, F1: 0.9514 Bal: 0.9104 - Val Loss: 0.2288, Accuracy: 0.9129, F1: 0.9499 Bal: 0.9006\n","           WALLETS: Train Loss: 0.41437426, Acc: 0.86314785, F1: 0.92164769 Bal: 0.8122 - Val Loss: 0.41224903, Accuracy: 0.8631, F1: 0.9216 Bal: 0.8091\n","Epoch 101:      TX: Train Loss: 0.2100, Acc: 0.9168, F1: 0.9521 Bal: 0.9115 - Val Loss: 0.2276, Accuracy: 0.9140, F1: 0.9505 Bal: 0.9012\n","           WALLETS: Train Loss: 0.41363284, Acc: 0.86341998, F1: 0.92181665 Bal: 0.8123 - Val Loss: 0.41142416, Accuracy: 0.8633, F1: 0.9217 Bal: 0.8092\n","Epoch 102:      TX: Train Loss: 0.2085, Acc: 0.9176, F1: 0.9526 Bal: 0.9122 - Val Loss: 0.2265, Accuracy: 0.9142, F1: 0.9507 Bal: 0.9013\n","           WALLETS: Train Loss: 0.41288480, Acc: 0.86341635, F1: 0.92181279 Bal: 0.8124 - Val Loss: 0.41059095, Accuracy: 0.8633, F1: 0.9217 Bal: 0.8092\n","Epoch 103:      TX: Train Loss: 0.2071, Acc: 0.9184, F1: 0.9530 Bal: 0.9129 - Val Loss: 0.2253, Accuracy: 0.9153, F1: 0.9513 Bal: 0.9019\n","           WALLETS: Train Loss: 0.41214144, Acc: 0.86354697, F1: 0.92189470 Bal: 0.8124 - Val Loss: 0.40976748, Accuracy: 0.8635, F1: 0.9218 Bal: 0.8090\n","Epoch 104:      TX: Train Loss: 0.2056, Acc: 0.9191, F1: 0.9535 Bal: 0.9134 - Val Loss: 0.2242, Accuracy: 0.9155, F1: 0.9514 Bal: 0.9020\n","           WALLETS: Train Loss: 0.41140127, Acc: 0.86352520, F1: 0.92188353 Bal: 0.8122 - Val Loss: 0.40895501, Accuracy: 0.8634, F1: 0.9217 Bal: 0.8091\n","Epoch 105:      TX: Train Loss: 0.2042, Acc: 0.9202, F1: 0.9541 Bal: 0.9144 - Val Loss: 0.2231, Accuracy: 0.9162, F1: 0.9519 Bal: 0.9014\n","           WALLETS: Train Loss: 0.41066471, Acc: 0.86367034, F1: 0.92197212 Bal: 0.8124 - Val Loss: 0.40815079, Accuracy: 0.8635, F1: 0.9218 Bal: 0.8090\n","Epoch 106:      TX: Train Loss: 0.2027, Acc: 0.9210, F1: 0.9546 Bal: 0.9151 - Val Loss: 0.2220, Accuracy: 0.9171, F1: 0.9524 Bal: 0.9019\n","           WALLETS: Train Loss: 0.40994403, Acc: 0.86373202, F1: 0.92200856 Bal: 0.8125 - Val Loss: 0.40736198, Accuracy: 0.8635, F1: 0.9218 Bal: 0.8092\n","Epoch 107:      TX: Train Loss: 0.2013, Acc: 0.9217, F1: 0.9550 Bal: 0.9157 - Val Loss: 0.2209, Accuracy: 0.9182, F1: 0.9530 Bal: 0.9025\n","           WALLETS: Train Loss: 0.40923697, Acc: 0.86384087, F1: 0.92207409 Bal: 0.8127 - Val Loss: 0.40658790, Accuracy: 0.8634, F1: 0.9218 Bal: 0.8093\n","Epoch 108:      TX: Train Loss: 0.1999, Acc: 0.9223, F1: 0.9554 Bal: 0.9163 - Val Loss: 0.2199, Accuracy: 0.9188, F1: 0.9534 Bal: 0.9029\n","           WALLETS: Train Loss: 0.40854129, Acc: 0.86418920, F1: 0.92228635 Bal: 0.8130 - Val Loss: 0.40582946, Accuracy: 0.8636, F1: 0.9219 Bal: 0.8091\n","Epoch 109:      TX: Train Loss: 0.1985, Acc: 0.9228, F1: 0.9557 Bal: 0.9170 - Val Loss: 0.2189, Accuracy: 0.9190, F1: 0.9536 Bal: 0.9030\n","           WALLETS: Train Loss: 0.40785736, Acc: 0.86434885, F1: 0.92238287 Bal: 0.8132 - Val Loss: 0.40507990, Accuracy: 0.8638, F1: 0.9220 Bal: 0.8097\n","Epoch 110:      TX: Train Loss: 0.1971, Acc: 0.9235, F1: 0.9561 Bal: 0.9183 - Val Loss: 0.2179, Accuracy: 0.9206, F1: 0.9545 Bal: 0.9048\n","           WALLETS: Train Loss: 0.40717414, Acc: 0.86444681, F1: 0.92243973 Bal: 0.8135 - Val Loss: 0.40433311, Accuracy: 0.8638, F1: 0.9220 Bal: 0.8095\n","Epoch 111:      TX: Train Loss: 0.1957, Acc: 0.9246, F1: 0.9567 Bal: 0.9196 - Val Loss: 0.2169, Accuracy: 0.9208, F1: 0.9546 Bal: 0.9059\n","           WALLETS: Train Loss: 0.40649411, Acc: 0.86445770, F1: 0.92244515 Bal: 0.8136 - Val Loss: 0.40359250, Accuracy: 0.8639, F1: 0.9221 Bal: 0.8099\n","Epoch 112:      TX: Train Loss: 0.1943, Acc: 0.9252, F1: 0.9571 Bal: 0.9201 - Val Loss: 0.2160, Accuracy: 0.9212, F1: 0.9548 Bal: 0.9082\n","           WALLETS: Train Loss: 0.40581757, Acc: 0.86458107, F1: 0.92251670 Bal: 0.8140 - Val Loss: 0.40285772, Accuracy: 0.8637, F1: 0.9219 Bal: 0.8099\n","Epoch 113:      TX: Train Loss: 0.1929, Acc: 0.9259, F1: 0.9575 Bal: 0.9207 - Val Loss: 0.2151, Accuracy: 0.9223, F1: 0.9555 Bal: 0.9088\n","           WALLETS: Train Loss: 0.40514359, Acc: 0.86460646, F1: 0.92252947 Bal: 0.8142 - Val Loss: 0.40212846, Accuracy: 0.8639, F1: 0.9220 Bal: 0.8105\n","Epoch 114:      TX: Train Loss: 0.1915, Acc: 0.9266, F1: 0.9579 Bal: 0.9218 - Val Loss: 0.2143, Accuracy: 0.9230, F1: 0.9559 Bal: 0.9091\n","           WALLETS: Train Loss: 0.40447617, Acc: 0.86465001, F1: 0.92255438 Bal: 0.8143 - Val Loss: 0.40141046, Accuracy: 0.8640, F1: 0.9221 Bal: 0.8112\n","Epoch 115:      TX: Train Loss: 0.1902, Acc: 0.9272, F1: 0.9582 Bal: 0.9225 - Val Loss: 0.2135, Accuracy: 0.9237, F1: 0.9563 Bal: 0.9075\n","           WALLETS: Train Loss: 0.40381795, Acc: 0.86479877, F1: 0.92264159 Bal: 0.8147 - Val Loss: 0.40070385, Accuracy: 0.8641, F1: 0.9222 Bal: 0.8113\n","Epoch 116:      TX: Train Loss: 0.1888, Acc: 0.9279, F1: 0.9587 Bal: 0.9234 - Val Loss: 0.2127, Accuracy: 0.9239, F1: 0.9564 Bal: 0.9077\n","           WALLETS: Train Loss: 0.40316400, Acc: 0.86481328, F1: 0.92264796 Bal: 0.8149 - Val Loss: 0.40000114, Accuracy: 0.8640, F1: 0.9221 Bal: 0.8112\n","Epoch 117:      TX: Train Loss: 0.1875, Acc: 0.9283, F1: 0.9589 Bal: 0.9236 - Val Loss: 0.2120, Accuracy: 0.9239, F1: 0.9564 Bal: 0.9077\n","           WALLETS: Train Loss: 0.40251398, Acc: 0.86485682, F1: 0.92267127 Bal: 0.8152 - Val Loss: 0.39930204, Accuracy: 0.8642, F1: 0.9222 Bal: 0.8116\n","Epoch 118:      TX: Train Loss: 0.1861, Acc: 0.9294, F1: 0.9595 Bal: 0.9243 - Val Loss: 0.2113, Accuracy: 0.9247, F1: 0.9569 Bal: 0.9091\n","           WALLETS: Train Loss: 0.40186471, Acc: 0.86472983, F1: 0.92258752 Bal: 0.8155 - Val Loss: 0.39860824, Accuracy: 0.8638, F1: 0.9220 Bal: 0.8106\n","Epoch 119:      TX: Train Loss: 0.1848, Acc: 0.9302, F1: 0.9600 Bal: 0.9253 - Val Loss: 0.2106, Accuracy: 0.9258, F1: 0.9576 Bal: 0.9097\n","           WALLETS: Train Loss: 0.40121242, Acc: 0.86482054, F1: 0.92263991 Bal: 0.8157 - Val Loss: 0.39791426, Accuracy: 0.8637, F1: 0.9219 Bal: 0.8107\n","Epoch 120:      TX: Train Loss: 0.1835, Acc: 0.9308, F1: 0.9604 Bal: 0.9260 - Val Loss: 0.2100, Accuracy: 0.9265, F1: 0.9580 Bal: 0.9101\n","           WALLETS: Train Loss: 0.40055349, Acc: 0.86491125, F1: 0.92269166 Bal: 0.8161 - Val Loss: 0.39721820, Accuracy: 0.8640, F1: 0.9221 Bal: 0.8113\n","Epoch 121:      TX: Train Loss: 0.1822, Acc: 0.9312, F1: 0.9606 Bal: 0.9265 - Val Loss: 0.2093, Accuracy: 0.9269, F1: 0.9583 Bal: 0.9094\n","           WALLETS: Train Loss: 0.39988086, Acc: 0.86504913, F1: 0.92277506 Bal: 0.8163 - Val Loss: 0.39650920, Accuracy: 0.8640, F1: 0.9221 Bal: 0.8117\n","Epoch 122:      TX: Train Loss: 0.1810, Acc: 0.9318, F1: 0.9610 Bal: 0.9272 - Val Loss: 0.2087, Accuracy: 0.9269, F1: 0.9583 Bal: 0.9084\n","           WALLETS: Train Loss: 0.39919686, Acc: 0.86511444, F1: 0.92281371 Bal: 0.8164 - Val Loss: 0.39579558, Accuracy: 0.8638, F1: 0.9220 Bal: 0.8119\n","Epoch 123:      TX: Train Loss: 0.1797, Acc: 0.9327, F1: 0.9615 Bal: 0.9281 - Val Loss: 0.2080, Accuracy: 0.9276, F1: 0.9587 Bal: 0.9078\n","           WALLETS: Train Loss: 0.39850992, Acc: 0.86516887, F1: 0.92284694 Bal: 0.8164 - Val Loss: 0.39508492, Accuracy: 0.8639, F1: 0.9220 Bal: 0.8119\n","Epoch 124:      TX: Train Loss: 0.1784, Acc: 0.9329, F1: 0.9616 Bal: 0.9285 - Val Loss: 0.2074, Accuracy: 0.9283, F1: 0.9591 Bal: 0.9081\n","           WALLETS: Train Loss: 0.39782920, Acc: 0.86530312, F1: 0.92291752 Bal: 0.8173 - Val Loss: 0.39437810, Accuracy: 0.8644, F1: 0.9223 Bal: 0.8136\n","Epoch 125:      TX: Train Loss: 0.1772, Acc: 0.9336, F1: 0.9621 Bal: 0.9293 - Val Loss: 0.2068, Accuracy: 0.9283, F1: 0.9591 Bal: 0.9071\n","           WALLETS: Train Loss: 0.39716136, Acc: 0.86539745, F1: 0.92297247 Bal: 0.8176 - Val Loss: 0.39368078, Accuracy: 0.8648, F1: 0.9225 Bal: 0.8141\n","Epoch 126:      TX: Train Loss: 0.1759, Acc: 0.9342, F1: 0.9624 Bal: 0.9296 - Val Loss: 0.2062, Accuracy: 0.9285, F1: 0.9592 Bal: 0.9073\n","           WALLETS: Train Loss: 0.39650062, Acc: 0.86554259, F1: 0.92305967 Bal: 0.8178 - Val Loss: 0.39299893, Accuracy: 0.8647, F1: 0.9225 Bal: 0.8139\n","Epoch 127:      TX: Train Loss: 0.1747, Acc: 0.9346, F1: 0.9626 Bal: 0.9300 - Val Loss: 0.2056, Accuracy: 0.9285, F1: 0.9592 Bal: 0.9063\n","           WALLETS: Train Loss: 0.39584240, Acc: 0.86558613, F1: 0.92308682 Bal: 0.8178 - Val Loss: 0.39232737, Accuracy: 0.8648, F1: 0.9226 Bal: 0.8140\n","Epoch 128:      TX: Train Loss: 0.1735, Acc: 0.9352, F1: 0.9630 Bal: 0.9307 - Val Loss: 0.2050, Accuracy: 0.9285, F1: 0.9592 Bal: 0.9063\n","           WALLETS: Train Loss: 0.39518541, Acc: 0.86571312, F1: 0.92316189 Bal: 0.8181 - Val Loss: 0.39165834, Accuracy: 0.8651, F1: 0.9227 Bal: 0.8146\n","Epoch 129:      TX: Train Loss: 0.1723, Acc: 0.9359, F1: 0.9634 Bal: 0.9312 - Val Loss: 0.2045, Accuracy: 0.9283, F1: 0.9591 Bal: 0.9042\n","           WALLETS: Train Loss: 0.39453447, Acc: 0.86583286, F1: 0.92323661 Bal: 0.8181 - Val Loss: 0.39099476, Accuracy: 0.8650, F1: 0.9226 Bal: 0.8147\n","Epoch 130:      TX: Train Loss: 0.1711, Acc: 0.9365, F1: 0.9638 Bal: 0.9321 - Val Loss: 0.2040, Accuracy: 0.9289, F1: 0.9595 Bal: 0.9046\n","           WALLETS: Train Loss: 0.39388862, Acc: 0.86598888, F1: 0.92332890 Bal: 0.8185 - Val Loss: 0.39033553, Accuracy: 0.8649, F1: 0.9226 Bal: 0.8150\n","Epoch 131:      TX: Train Loss: 0.1700, Acc: 0.9370, F1: 0.9640 Bal: 0.9326 - Val Loss: 0.2036, Accuracy: 0.9291, F1: 0.9596 Bal: 0.9037\n","           WALLETS: Train Loss: 0.39324629, Acc: 0.86601428, F1: 0.92334582 Bal: 0.8184 - Val Loss: 0.38968340, Accuracy: 0.8648, F1: 0.9225 Bal: 0.8145\n","Epoch 132:      TX: Train Loss: 0.1688, Acc: 0.9373, F1: 0.9642 Bal: 0.9330 - Val Loss: 0.2031, Accuracy: 0.9298, F1: 0.9600 Bal: 0.9051\n","           WALLETS: Train Loss: 0.39260557, Acc: 0.86614853, F1: 0.92342628 Bal: 0.8186 - Val Loss: 0.38903019, Accuracy: 0.8651, F1: 0.9227 Bal: 0.8147\n","Epoch 133:      TX: Train Loss: 0.1676, Acc: 0.9378, F1: 0.9645 Bal: 0.9336 - Val Loss: 0.2027, Accuracy: 0.9305, F1: 0.9604 Bal: 0.9054\n","           WALLETS: Train Loss: 0.39196745, Acc: 0.86621747, F1: 0.92347033 Bal: 0.8185 - Val Loss: 0.38837931, Accuracy: 0.8652, F1: 0.9228 Bal: 0.8150\n","Epoch 134:      TX: Train Loss: 0.1665, Acc: 0.9384, F1: 0.9648 Bal: 0.9341 - Val Loss: 0.2023, Accuracy: 0.9302, F1: 0.9603 Bal: 0.9053\n","           WALLETS: Train Loss: 0.39133218, Acc: 0.86633358, F1: 0.92353992 Bal: 0.8187 - Val Loss: 0.38773289, Accuracy: 0.8652, F1: 0.9228 Bal: 0.8150\n","Epoch 135:      TX: Train Loss: 0.1654, Acc: 0.9387, F1: 0.9651 Bal: 0.9347 - Val Loss: 0.2019, Accuracy: 0.9309, F1: 0.9607 Bal: 0.9057\n","           WALLETS: Train Loss: 0.39070177, Acc: 0.86651863, F1: 0.92365069 Bal: 0.8190 - Val Loss: 0.38708818, Accuracy: 0.8653, F1: 0.9228 Bal: 0.8146\n","Epoch 136:      TX: Train Loss: 0.1643, Acc: 0.9394, F1: 0.9654 Bal: 0.9357 - Val Loss: 0.2015, Accuracy: 0.9318, F1: 0.9612 Bal: 0.9071\n","           WALLETS: Train Loss: 0.39007515, Acc: 0.86678350, F1: 0.92380994 Bal: 0.8194 - Val Loss: 0.38644916, Accuracy: 0.8654, F1: 0.9229 Bal: 0.8153\n","Epoch 137:      TX: Train Loss: 0.1631, Acc: 0.9399, F1: 0.9657 Bal: 0.9362 - Val Loss: 0.2011, Accuracy: 0.9318, F1: 0.9612 Bal: 0.9071\n","           WALLETS: Train Loss: 0.38944995, Acc: 0.86696855, F1: 0.92392288 Bal: 0.8196 - Val Loss: 0.38581541, Accuracy: 0.8659, F1: 0.9232 Bal: 0.8158\n","Epoch 138:      TX: Train Loss: 0.1620, Acc: 0.9404, F1: 0.9660 Bal: 0.9372 - Val Loss: 0.2007, Accuracy: 0.9324, F1: 0.9616 Bal: 0.9065\n","           WALLETS: Train Loss: 0.38882658, Acc: 0.86711731, F1: 0.92401063 Bal: 0.8199 - Val Loss: 0.38518170, Accuracy: 0.8664, F1: 0.9235 Bal: 0.8163\n","Epoch 139:      TX: Train Loss: 0.1609, Acc: 0.9408, F1: 0.9663 Bal: 0.9375 - Val Loss: 0.2004, Accuracy: 0.9329, F1: 0.9618 Bal: 0.9068\n","           WALLETS: Train Loss: 0.38820603, Acc: 0.86720077, F1: 0.92406135 Bal: 0.8200 - Val Loss: 0.38455269, Accuracy: 0.8668, F1: 0.9237 Bal: 0.8169\n","Epoch 140:      TX: Train Loss: 0.1598, Acc: 0.9411, F1: 0.9664 Bal: 0.9378 - Val Loss: 0.2000, Accuracy: 0.9337, F1: 0.9624 Bal: 0.9072\n","           WALLETS: Train Loss: 0.38758922, Acc: 0.86735679, F1: 0.92415639 Bal: 0.8202 - Val Loss: 0.38392830, Accuracy: 0.8668, F1: 0.9237 Bal: 0.8171\n","Epoch 141:      TX: Train Loss: 0.1588, Acc: 0.9416, F1: 0.9667 Bal: 0.9381 - Val Loss: 0.1997, Accuracy: 0.9342, F1: 0.9626 Bal: 0.9075\n","           WALLETS: Train Loss: 0.38697505, Acc: 0.86765069, F1: 0.92433213 Bal: 0.8207 - Val Loss: 0.38330647, Accuracy: 0.8668, F1: 0.9238 Bal: 0.8174\n","Epoch 142:      TX: Train Loss: 0.1577, Acc: 0.9421, F1: 0.9670 Bal: 0.9386 - Val Loss: 0.1993, Accuracy: 0.9351, F1: 0.9631 Bal: 0.9090\n","           WALLETS: Train Loss: 0.38636300, Acc: 0.86786839, F1: 0.92446443 Bal: 0.8209 - Val Loss: 0.38268965, Accuracy: 0.8670, F1: 0.9239 Bal: 0.8178\n","Epoch 143:      TX: Train Loss: 0.1567, Acc: 0.9425, F1: 0.9672 Bal: 0.9394 - Val Loss: 0.1990, Accuracy: 0.9353, F1: 0.9632 Bal: 0.9091\n","           WALLETS: Train Loss: 0.38575295, Acc: 0.86811149, F1: 0.92461419 Bal: 0.8210 - Val Loss: 0.38207829, Accuracy: 0.8669, F1: 0.9238 Bal: 0.8175\n","Epoch 144:      TX: Train Loss: 0.1556, Acc: 0.9428, F1: 0.9674 Bal: 0.9399 - Val Loss: 0.1987, Accuracy: 0.9346, F1: 0.9629 Bal: 0.9087\n","           WALLETS: Train Loss: 0.38514698, Acc: 0.86826751, F1: 0.92470821 Bal: 0.8212 - Val Loss: 0.38147172, Accuracy: 0.8672, F1: 0.9240 Bal: 0.8177\n","Epoch 145:      TX: Train Loss: 0.1546, Acc: 0.9433, F1: 0.9677 Bal: 0.9405 - Val Loss: 0.1985, Accuracy: 0.9353, F1: 0.9632 Bal: 0.9091\n","           WALLETS: Train Loss: 0.38454434, Acc: 0.86828928, F1: 0.92472035 Bal: 0.8213 - Val Loss: 0.38086644, Accuracy: 0.8673, F1: 0.9240 Bal: 0.8181\n","Epoch 146:      TX: Train Loss: 0.1535, Acc: 0.9439, F1: 0.9680 Bal: 0.9416 - Val Loss: 0.1982, Accuracy: 0.9353, F1: 0.9632 Bal: 0.9091\n","           WALLETS: Train Loss: 0.38394201, Acc: 0.86835822, F1: 0.92476115 Bal: 0.8215 - Val Loss: 0.38026330, Accuracy: 0.8670, F1: 0.9239 Bal: 0.8183\n","Epoch 147:      TX: Train Loss: 0.1525, Acc: 0.9444, F1: 0.9683 Bal: 0.9424 - Val Loss: 0.1979, Accuracy: 0.9351, F1: 0.9631 Bal: 0.9090\n","           WALLETS: Train Loss: 0.38333964, Acc: 0.86844893, F1: 0.92481346 Bal: 0.8218 - Val Loss: 0.37966180, Accuracy: 0.8671, F1: 0.9239 Bal: 0.8182\n","Epoch 148:      TX: Train Loss: 0.1515, Acc: 0.9447, F1: 0.9685 Bal: 0.9428 - Val Loss: 0.1976, Accuracy: 0.9355, F1: 0.9634 Bal: 0.9092\n","           WALLETS: Train Loss: 0.38273785, Acc: 0.86855053, F1: 0.92487402 Bal: 0.8219 - Val Loss: 0.37905836, Accuracy: 0.8671, F1: 0.9239 Bal: 0.8182\n","Epoch 149:      TX: Train Loss: 0.1505, Acc: 0.9450, F1: 0.9687 Bal: 0.9432 - Val Loss: 0.1973, Accuracy: 0.9362, F1: 0.9638 Bal: 0.9096\n","           WALLETS: Train Loss: 0.38213515, Acc: 0.86874283, F1: 0.92498564 Bal: 0.8225 - Val Loss: 0.37845141, Accuracy: 0.8674, F1: 0.9241 Bal: 0.8190\n","Epoch 150:      TX: Train Loss: 0.1495, Acc: 0.9451, F1: 0.9688 Bal: 0.9434 - Val Loss: 0.1970, Accuracy: 0.9359, F1: 0.9636 Bal: 0.9104\n","           WALLETS: Train Loss: 0.38153112, Acc: 0.86880815, F1: 0.92502328 Bal: 0.8227 - Val Loss: 0.37784222, Accuracy: 0.8676, F1: 0.9242 Bal: 0.8197\n","Epoch 151:      TX: Train Loss: 0.1485, Acc: 0.9456, F1: 0.9690 Bal: 0.9440 - Val Loss: 0.1967, Accuracy: 0.9357, F1: 0.9635 Bal: 0.9103\n","           WALLETS: Train Loss: 0.38092875, Acc: 0.86898231, F1: 0.92512374 Bal: 0.8233 - Val Loss: 0.37723064, Accuracy: 0.8678, F1: 0.9243 Bal: 0.8203\n","Epoch 152:      TX: Train Loss: 0.1476, Acc: 0.9458, F1: 0.9692 Bal: 0.9443 - Val Loss: 0.1965, Accuracy: 0.9362, F1: 0.9638 Bal: 0.9105\n","           WALLETS: Train Loss: 0.38032773, Acc: 0.86904036, F1: 0.92515351 Bal: 0.8237 - Val Loss: 0.37662220, Accuracy: 0.8677, F1: 0.9242 Bal: 0.8204\n","Epoch 153:      TX: Train Loss: 0.1466, Acc: 0.9462, F1: 0.9694 Bal: 0.9447 - Val Loss: 0.1963, Accuracy: 0.9373, F1: 0.9644 Bal: 0.9112\n","           WALLETS: Train Loss: 0.37972689, Acc: 0.86905850, F1: 0.92516062 Bal: 0.8240 - Val Loss: 0.37601405, Accuracy: 0.8675, F1: 0.9241 Bal: 0.8205\n","Epoch 154:      TX: Train Loss: 0.1456, Acc: 0.9466, F1: 0.9696 Bal: 0.9450 - Val Loss: 0.1961, Accuracy: 0.9375, F1: 0.9645 Bal: 0.9113\n","           WALLETS: Train Loss: 0.37912497, Acc: 0.86911656, F1: 0.92519349 Bal: 0.8242 - Val Loss: 0.37540641, Accuracy: 0.8676, F1: 0.9242 Bal: 0.8210\n","Epoch 155:      TX: Train Loss: 0.1447, Acc: 0.9472, F1: 0.9700 Bal: 0.9455 - Val Loss: 0.1960, Accuracy: 0.9379, F1: 0.9648 Bal: 0.9115\n","           WALLETS: Train Loss: 0.37852442, Acc: 0.86977693, F1: 0.92559838 Bal: 0.8246 - Val Loss: 0.37480173, Accuracy: 0.8680, F1: 0.9244 Bal: 0.8218\n","Epoch 156:      TX: Train Loss: 0.1437, Acc: 0.9473, F1: 0.9701 Bal: 0.9458 - Val Loss: 0.1959, Accuracy: 0.9381, F1: 0.9649 Bal: 0.9116\n","           WALLETS: Train Loss: 0.37792456, Acc: 0.86991118, F1: 0.92567740 Bal: 0.8249 - Val Loss: 0.37419444, Accuracy: 0.8679, F1: 0.9244 Bal: 0.8220\n","Epoch 157:      TX: Train Loss: 0.1428, Acc: 0.9476, F1: 0.9702 Bal: 0.9462 - Val Loss: 0.1957, Accuracy: 0.9384, F1: 0.9650 Bal: 0.9118\n","           WALLETS: Train Loss: 0.37732574, Acc: 0.86986764, F1: 0.92564420 Bal: 0.8253 - Val Loss: 0.37358552, Accuracy: 0.8681, F1: 0.9245 Bal: 0.8228\n","Epoch 158:      TX: Train Loss: 0.1419, Acc: 0.9478, F1: 0.9703 Bal: 0.9464 - Val Loss: 0.1956, Accuracy: 0.9384, F1: 0.9650 Bal: 0.9118\n","           WALLETS: Train Loss: 0.37672564, Acc: 0.86987126, F1: 0.92564273 Bal: 0.8256 - Val Loss: 0.37297699, Accuracy: 0.8680, F1: 0.9244 Bal: 0.8232\n","Epoch 159:      TX: Train Loss: 0.1410, Acc: 0.9482, F1: 0.9706 Bal: 0.9474 - Val Loss: 0.1955, Accuracy: 0.9384, F1: 0.9650 Bal: 0.9127\n","           WALLETS: Train Loss: 0.37612402, Acc: 0.86988941, F1: 0.92564955 Bal: 0.8259 - Val Loss: 0.37236848, Accuracy: 0.8682, F1: 0.9245 Bal: 0.8238\n","Epoch 160:      TX: Train Loss: 0.1401, Acc: 0.9486, F1: 0.9708 Bal: 0.9477 - Val Loss: 0.1954, Accuracy: 0.9384, F1: 0.9650 Bal: 0.9127\n","           WALLETS: Train Loss: 0.37551934, Acc: 0.87002729, F1: 0.92572772 Bal: 0.8264 - Val Loss: 0.37175885, Accuracy: 0.8686, F1: 0.9248 Bal: 0.8248\n","Epoch 161:      TX: Train Loss: 0.1391, Acc: 0.9490, F1: 0.9710 Bal: 0.9480 - Val Loss: 0.1953, Accuracy: 0.9386, F1: 0.9652 Bal: 0.9129\n","           WALLETS: Train Loss: 0.37491196, Acc: 0.87012162, F1: 0.92578255 Bal: 0.8267 - Val Loss: 0.37114927, Accuracy: 0.8686, F1: 0.9247 Bal: 0.8250\n","Epoch 162:      TX: Train Loss: 0.1382, Acc: 0.9493, F1: 0.9712 Bal: 0.9482 - Val Loss: 0.1953, Accuracy: 0.9390, F1: 0.9654 Bal: 0.9141\n","           WALLETS: Train Loss: 0.37430349, Acc: 0.87011074, F1: 0.92577002 Bal: 0.8271 - Val Loss: 0.37054002, Accuracy: 0.8685, F1: 0.9247 Bal: 0.8256\n","Epoch 163:      TX: Train Loss: 0.1373, Acc: 0.9498, F1: 0.9715 Bal: 0.9486 - Val Loss: 0.1953, Accuracy: 0.9392, F1: 0.9655 Bal: 0.9142\n","           WALLETS: Train Loss: 0.37369400, Acc: 0.87009623, F1: 0.92575803 Bal: 0.8273 - Val Loss: 0.36993372, Accuracy: 0.8685, F1: 0.9247 Bal: 0.8257\n","Epoch 164:      TX: Train Loss: 0.1365, Acc: 0.9501, F1: 0.9717 Bal: 0.9489 - Val Loss: 0.1953, Accuracy: 0.9390, F1: 0.9654 Bal: 0.9141\n","           WALLETS: Train Loss: 0.37308335, Acc: 0.87006720, F1: 0.92573528 Bal: 0.8276 - Val Loss: 0.36932963, Accuracy: 0.8684, F1: 0.9246 Bal: 0.8266\n","Epoch 165:      TX: Train Loss: 0.1356, Acc: 0.9505, F1: 0.9719 Bal: 0.9492 - Val Loss: 0.1954, Accuracy: 0.9392, F1: 0.9655 Bal: 0.9142\n","           WALLETS: Train Loss: 0.37247133, Acc: 0.87015065, F1: 0.92578375 Bal: 0.8279 - Val Loss: 0.36872804, Accuracy: 0.8685, F1: 0.9247 Bal: 0.8272\n","Epoch 166:      TX: Train Loss: 0.1347, Acc: 0.9506, F1: 0.9719 Bal: 0.9497 - Val Loss: 0.1953, Accuracy: 0.9394, F1: 0.9657 Bal: 0.9153\n","           WALLETS: Train Loss: 0.37185749, Acc: 0.87006720, F1: 0.92572882 Bal: 0.8281 - Val Loss: 0.36812592, Accuracy: 0.8686, F1: 0.9247 Bal: 0.8279\n","Epoch 167:      TX: Train Loss: 0.1338, Acc: 0.9509, F1: 0.9721 Bal: 0.9501 - Val Loss: 0.1953, Accuracy: 0.9394, F1: 0.9657 Bal: 0.9153\n","           WALLETS: Train Loss: 0.37124157, Acc: 0.87020145, F1: 0.92580663 Bal: 0.8285 - Val Loss: 0.36752552, Accuracy: 0.8687, F1: 0.9248 Bal: 0.8280\n","Epoch 168:      TX: Train Loss: 0.1330, Acc: 0.9512, F1: 0.9723 Bal: 0.9504 - Val Loss: 0.1953, Accuracy: 0.9394, F1: 0.9657 Bal: 0.9153\n","           WALLETS: Train Loss: 0.37062687, Acc: 0.87033570, F1: 0.92588291 Bal: 0.8290 - Val Loss: 0.36693034, Accuracy: 0.8688, F1: 0.9248 Bal: 0.8281\n","Epoch 169:      TX: Train Loss: 0.1321, Acc: 0.9515, F1: 0.9725 Bal: 0.9505 - Val Loss: 0.1954, Accuracy: 0.9392, F1: 0.9655 Bal: 0.9152\n","           WALLETS: Train Loss: 0.37001270, Acc: 0.87038287, F1: 0.92590603 Bal: 0.8294 - Val Loss: 0.36633521, Accuracy: 0.8686, F1: 0.9247 Bal: 0.8285\n","Epoch 170:      TX: Train Loss: 0.1313, Acc: 0.9518, F1: 0.9726 Bal: 0.9507 - Val Loss: 0.1955, Accuracy: 0.9401, F1: 0.9660 Bal: 0.9157\n","           WALLETS: Train Loss: 0.36939818, Acc: 0.87041915, F1: 0.92592401 Bal: 0.8297 - Val Loss: 0.36574090, Accuracy: 0.8688, F1: 0.9248 Bal: 0.8287\n","Epoch 171:      TX: Train Loss: 0.1304, Acc: 0.9523, F1: 0.9729 Bal: 0.9513 - Val Loss: 0.1956, Accuracy: 0.9403, F1: 0.9662 Bal: 0.9158\n","           WALLETS: Train Loss: 0.36878377, Acc: 0.87039375, F1: 0.92590503 Bal: 0.8300 - Val Loss: 0.36514962, Accuracy: 0.8686, F1: 0.9247 Bal: 0.8283\n","Epoch 172:      TX: Train Loss: 0.1296, Acc: 0.9526, F1: 0.9731 Bal: 0.9514 - Val Loss: 0.1957, Accuracy: 0.9410, F1: 0.9666 Bal: 0.9162\n","           WALLETS: Train Loss: 0.36816970, Acc: 0.87054252, F1: 0.92599054 Bal: 0.8304 - Val Loss: 0.36456501, Accuracy: 0.8687, F1: 0.9248 Bal: 0.8282\n","Epoch 173:      TX: Train Loss: 0.1288, Acc: 0.9529, F1: 0.9733 Bal: 0.9517 - Val Loss: 0.1959, Accuracy: 0.9410, F1: 0.9666 Bal: 0.9162\n","           WALLETS: Train Loss: 0.36755285, Acc: 0.87062597, F1: 0.92603840 Bal: 0.8307 - Val Loss: 0.36397922, Accuracy: 0.8688, F1: 0.9248 Bal: 0.8286\n","Epoch 174:      TX: Train Loss: 0.1279, Acc: 0.9530, F1: 0.9734 Bal: 0.9520 - Val Loss: 0.1960, Accuracy: 0.9416, F1: 0.9669 Bal: 0.9165\n","           WALLETS: Train Loss: 0.36693776, Acc: 0.87077836, F1: 0.92612920 Bal: 0.8310 - Val Loss: 0.36339563, Accuracy: 0.8687, F1: 0.9247 Bal: 0.8288\n","Epoch 175:      TX: Train Loss: 0.1271, Acc: 0.9531, F1: 0.9734 Bal: 0.9522 - Val Loss: 0.1961, Accuracy: 0.9419, F1: 0.9671 Bal: 0.9167\n","           WALLETS: Train Loss: 0.36632544, Acc: 0.87082553, F1: 0.92615509 Bal: 0.8312 - Val Loss: 0.36281577, Accuracy: 0.8687, F1: 0.9248 Bal: 0.8292\n","Epoch 176:      TX: Train Loss: 0.1263, Acc: 0.9533, F1: 0.9735 Bal: 0.9529 - Val Loss: 0.1962, Accuracy: 0.9423, F1: 0.9673 Bal: 0.9169\n","           WALLETS: Train Loss: 0.36571485, Acc: 0.87095253, F1: 0.92623060 Bal: 0.8315 - Val Loss: 0.36223549, Accuracy: 0.8689, F1: 0.9248 Bal: 0.8297\n","Epoch 177:      TX: Train Loss: 0.1255, Acc: 0.9536, F1: 0.9737 Bal: 0.9533 - Val Loss: 0.1963, Accuracy: 0.9430, F1: 0.9677 Bal: 0.9183\n","           WALLETS: Train Loss: 0.36510405, Acc: 0.87108678, F1: 0.92630841 Bal: 0.8319 - Val Loss: 0.36165395, Accuracy: 0.8689, F1: 0.9248 Bal: 0.8296\n","Epoch 178:      TX: Train Loss: 0.1247, Acc: 0.9538, F1: 0.9738 Bal: 0.9535 - Val Loss: 0.1964, Accuracy: 0.9434, F1: 0.9680 Bal: 0.9185\n","           WALLETS: Train Loss: 0.36449108, Acc: 0.87111580, F1: 0.92632409 Bal: 0.8320 - Val Loss: 0.36106825, Accuracy: 0.8693, F1: 0.9251 Bal: 0.8299\n","Epoch 179:      TX: Train Loss: 0.1239, Acc: 0.9544, F1: 0.9741 Bal: 0.9541 - Val Loss: 0.1966, Accuracy: 0.9438, F1: 0.9682 Bal: 0.9187\n","           WALLETS: Train Loss: 0.36387697, Acc: 0.87102147, F1: 0.92626312 Bal: 0.8322 - Val Loss: 0.36048296, Accuracy: 0.8693, F1: 0.9251 Bal: 0.8300\n","Epoch 180:      TX: Train Loss: 0.1231, Acc: 0.9548, F1: 0.9744 Bal: 0.9546 - Val Loss: 0.1967, Accuracy: 0.9441, F1: 0.9684 Bal: 0.9189\n","           WALLETS: Train Loss: 0.36326045, Acc: 0.87110129, F1: 0.92630784 Bal: 0.8326 - Val Loss: 0.35989732, Accuracy: 0.8693, F1: 0.9251 Bal: 0.8301\n","Epoch 181:      TX: Train Loss: 0.1223, Acc: 0.9552, F1: 0.9746 Bal: 0.9548 - Val Loss: 0.1969, Accuracy: 0.9443, F1: 0.9685 Bal: 0.9190\n","           WALLETS: Train Loss: 0.36264113, Acc: 0.87181971, F1: 0.92674926 Bal: 0.8329 - Val Loss: 0.35931316, Accuracy: 0.8706, F1: 0.9259 Bal: 0.8314\n","Epoch 182:      TX: Train Loss: 0.1215, Acc: 0.9553, F1: 0.9747 Bal: 0.9550 - Val Loss: 0.1970, Accuracy: 0.9443, F1: 0.9685 Bal: 0.9190\n","           WALLETS: Train Loss: 0.36202192, Acc: 0.87197573, F1: 0.92684343 Bal: 0.8331 - Val Loss: 0.35872880, Accuracy: 0.8710, F1: 0.9262 Bal: 0.8318\n","Epoch 183:      TX: Train Loss: 0.1208, Acc: 0.9557, F1: 0.9749 Bal: 0.9555 - Val Loss: 0.1972, Accuracy: 0.9443, F1: 0.9685 Bal: 0.9190\n","           WALLETS: Train Loss: 0.36140308, Acc: 0.87196122, F1: 0.92683058 Bal: 0.8334 - Val Loss: 0.35814247, Accuracy: 0.8710, F1: 0.9262 Bal: 0.8320\n","Epoch 184:      TX: Train Loss: 0.1200, Acc: 0.9561, F1: 0.9751 Bal: 0.9561 - Val Loss: 0.1974, Accuracy: 0.9449, F1: 0.9689 Bal: 0.9193\n","           WALLETS: Train Loss: 0.36078161, Acc: 0.87190679, F1: 0.92679386 Bal: 0.8336 - Val Loss: 0.35755286, Accuracy: 0.8711, F1: 0.9262 Bal: 0.8325\n","Epoch 185:      TX: Train Loss: 0.1192, Acc: 0.9563, F1: 0.9752 Bal: 0.9561 - Val Loss: 0.1976, Accuracy: 0.9452, F1: 0.9690 Bal: 0.9195\n","           WALLETS: Train Loss: 0.36015856, Acc: 0.87185962, F1: 0.92675916 Bal: 0.8340 - Val Loss: 0.35696390, Accuracy: 0.8711, F1: 0.9262 Bal: 0.8332\n","Epoch 186:      TX: Train Loss: 0.1185, Acc: 0.9568, F1: 0.9755 Bal: 0.9565 - Val Loss: 0.1978, Accuracy: 0.9452, F1: 0.9690 Bal: 0.9195\n","           WALLETS: Train Loss: 0.35953426, Acc: 0.87195759, F1: 0.92681743 Bal: 0.8341 - Val Loss: 0.35637456, Accuracy: 0.8710, F1: 0.9262 Bal: 0.8333\n","Epoch 187:      TX: Train Loss: 0.1177, Acc: 0.9571, F1: 0.9757 Bal: 0.9568 - Val Loss: 0.1980, Accuracy: 0.9449, F1: 0.9689 Bal: 0.9193\n","           WALLETS: Train Loss: 0.35890844, Acc: 0.87194671, F1: 0.92681136 Bal: 0.8341 - Val Loss: 0.35578546, Accuracy: 0.8712, F1: 0.9263 Bal: 0.8334\n","Epoch 188:      TX: Train Loss: 0.1170, Acc: 0.9574, F1: 0.9759 Bal: 0.9575 - Val Loss: 0.1982, Accuracy: 0.9447, F1: 0.9687 Bal: 0.9192\n","           WALLETS: Train Loss: 0.35828057, Acc: 0.87201202, F1: 0.92684323 Bal: 0.8347 - Val Loss: 0.35519147, Accuracy: 0.8711, F1: 0.9262 Bal: 0.8341\n","Epoch 189:      TX: Train Loss: 0.1162, Acc: 0.9575, F1: 0.9760 Bal: 0.9575 - Val Loss: 0.1985, Accuracy: 0.9449, F1: 0.9689 Bal: 0.9193\n","           WALLETS: Train Loss: 0.35765231, Acc: 0.87189591, F1: 0.92676562 Bal: 0.8351 - Val Loss: 0.35459858, Accuracy: 0.8710, F1: 0.9261 Bal: 0.8343\n","Epoch 190:      TX: Train Loss: 0.1155, Acc: 0.9578, F1: 0.9761 Bal: 0.9581 - Val Loss: 0.1989, Accuracy: 0.9449, F1: 0.9689 Bal: 0.9193\n","           WALLETS: Train Loss: 0.35702434, Acc: 0.87176166, F1: 0.92667473 Bal: 0.8356 - Val Loss: 0.35400960, Accuracy: 0.8711, F1: 0.9261 Bal: 0.8355\n","Epoch 191:      TX: Train Loss: 0.1148, Acc: 0.9581, F1: 0.9763 Bal: 0.9587 - Val Loss: 0.1992, Accuracy: 0.9456, F1: 0.9692 Bal: 0.9197\n","           WALLETS: Train Loss: 0.35639802, Acc: 0.87187414, F1: 0.92674254 Bal: 0.8358 - Val Loss: 0.35342070, Accuracy: 0.8708, F1: 0.9260 Bal: 0.8360\n","Epoch 192:      TX: Train Loss: 0.1141, Acc: 0.9589, F1: 0.9767 Bal: 0.9596 - Val Loss: 0.1995, Accuracy: 0.9458, F1: 0.9694 Bal: 0.9198\n","           WALLETS: Train Loss: 0.35577482, Acc: 0.87192856, F1: 0.92677199 Bal: 0.8361 - Val Loss: 0.35283399, Accuracy: 0.8708, F1: 0.9260 Bal: 0.8363\n","Epoch 193:      TX: Train Loss: 0.1134, Acc: 0.9593, F1: 0.9770 Bal: 0.9600 - Val Loss: 0.1998, Accuracy: 0.9467, F1: 0.9699 Bal: 0.9203\n","           WALLETS: Train Loss: 0.35515547, Acc: 0.87196485, F1: 0.92678909 Bal: 0.8365 - Val Loss: 0.35224926, Accuracy: 0.8709, F1: 0.9260 Bal: 0.8368\n","Epoch 194:      TX: Train Loss: 0.1127, Acc: 0.9595, F1: 0.9771 Bal: 0.9603 - Val Loss: 0.2000, Accuracy: 0.9467, F1: 0.9699 Bal: 0.9203\n","           WALLETS: Train Loss: 0.35453889, Acc: 0.87195759, F1: 0.92677674 Bal: 0.8370 - Val Loss: 0.35166243, Accuracy: 0.8706, F1: 0.9258 Bal: 0.8366\n","Epoch 195:      TX: Train Loss: 0.1120, Acc: 0.9597, F1: 0.9772 Bal: 0.9608 - Val Loss: 0.2002, Accuracy: 0.9476, F1: 0.9704 Bal: 0.9208\n","           WALLETS: Train Loss: 0.35392529, Acc: 0.87203379, F1: 0.92681500 Bal: 0.8376 - Val Loss: 0.35107973, Accuracy: 0.8708, F1: 0.9259 Bal: 0.8370\n","Epoch 196:      TX: Train Loss: 0.1113, Acc: 0.9600, F1: 0.9773 Bal: 0.9611 - Val Loss: 0.2004, Accuracy: 0.9478, F1: 0.9705 Bal: 0.9209\n","           WALLETS: Train Loss: 0.35331652, Acc: 0.87207733, F1: 0.92683473 Bal: 0.8382 - Val Loss: 0.35050657, Accuracy: 0.8709, F1: 0.9260 Bal: 0.8376\n","Epoch 197:      TX: Train Loss: 0.1106, Acc: 0.9604, F1: 0.9776 Bal: 0.9620 - Val Loss: 0.2007, Accuracy: 0.9484, F1: 0.9709 Bal: 0.9213\n","           WALLETS: Train Loss: 0.35271144, Acc: 0.87224423, F1: 0.92693262 Bal: 0.8386 - Val Loss: 0.34994081, Accuracy: 0.8712, F1: 0.9262 Bal: 0.8381\n","Epoch 198:      TX: Train Loss: 0.1099, Acc: 0.9608, F1: 0.9778 Bal: 0.9626 - Val Loss: 0.2010, Accuracy: 0.9487, F1: 0.9710 Bal: 0.9214\n","           WALLETS: Train Loss: 0.35211346, Acc: 0.87225512, F1: 0.92693536 Bal: 0.8389 - Val Loss: 0.34938145, Accuracy: 0.8713, F1: 0.9262 Bal: 0.8384\n","Epoch 199:      TX: Train Loss: 0.1093, Acc: 0.9611, F1: 0.9780 Bal: 0.9629 - Val Loss: 0.2013, Accuracy: 0.9491, F1: 0.9713 Bal: 0.9217\n","           WALLETS: Train Loss: 0.35152188, Acc: 0.87214264, F1: 0.92685995 Bal: 0.8393 - Val Loss: 0.34883258, Accuracy: 0.8712, F1: 0.9262 Bal: 0.8392\n","Epoch 200:      TX: Train Loss: 0.1086, Acc: 0.9613, F1: 0.9781 Bal: 0.9630 - Val Loss: 0.2015, Accuracy: 0.9493, F1: 0.9714 Bal: 0.9218\n","           WALLETS: Train Loss: 0.35092774, Acc: 0.87185237, F1: 0.92667624 Bal: 0.8395 - Val Loss: 0.34828222, Accuracy: 0.8711, F1: 0.9261 Bal: 0.8391\n","Epoch 201:      TX: Train Loss: 0.1079, Acc: 0.9615, F1: 0.9782 Bal: 0.9632 - Val Loss: 0.2018, Accuracy: 0.9493, F1: 0.9714 Bal: 0.9218\n","           WALLETS: Train Loss: 0.35033166, Acc: 0.87174714, F1: 0.92660522 Bal: 0.8399 - Val Loss: 0.34773508, Accuracy: 0.8709, F1: 0.9260 Bal: 0.8390\n","Epoch 202:      TX: Train Loss: 0.1073, Acc: 0.9618, F1: 0.9784 Bal: 0.9639 - Val Loss: 0.2020, Accuracy: 0.9491, F1: 0.9713 Bal: 0.9217\n","           WALLETS: Train Loss: 0.34973553, Acc: 0.87178343, F1: 0.92662659 Bal: 0.8400 - Val Loss: 0.34719023, Accuracy: 0.8710, F1: 0.9261 Bal: 0.8391\n","Epoch 203:      TX: Train Loss: 0.1066, Acc: 0.9619, F1: 0.9785 Bal: 0.9642 - Val Loss: 0.2023, Accuracy: 0.9491, F1: 0.9713 Bal: 0.9217\n","           WALLETS: Train Loss: 0.34913868, Acc: 0.87189954, F1: 0.92669395 Bal: 0.8403 - Val Loss: 0.34664112, Accuracy: 0.8709, F1: 0.9260 Bal: 0.8395\n","Epoch 204:      TX: Train Loss: 0.1060, Acc: 0.9622, F1: 0.9787 Bal: 0.9646 - Val Loss: 0.2025, Accuracy: 0.9489, F1: 0.9712 Bal: 0.9215\n","           WALLETS: Train Loss: 0.34854180, Acc: 0.87171449, F1: 0.92657693 Bal: 0.8404 - Val Loss: 0.34609467, Accuracy: 0.8708, F1: 0.9259 Bal: 0.8399\n","Epoch 205:      TX: Train Loss: 0.1054, Acc: 0.9625, F1: 0.9788 Bal: 0.9650 - Val Loss: 0.2028, Accuracy: 0.9491, F1: 0.9713 Bal: 0.9217\n","           WALLETS: Train Loss: 0.34794447, Acc: 0.87160927, F1: 0.92650556 Bal: 0.8408 - Val Loss: 0.34554857, Accuracy: 0.8708, F1: 0.9259 Bal: 0.8407\n","Epoch 206:      TX: Train Loss: 0.1047, Acc: 0.9626, F1: 0.9789 Bal: 0.9652 - Val Loss: 0.2031, Accuracy: 0.9493, F1: 0.9714 Bal: 0.9218\n","           WALLETS: Train Loss: 0.34735042, Acc: 0.87168546, F1: 0.92654689 Bal: 0.8413 - Val Loss: 0.34501201, Accuracy: 0.8709, F1: 0.9260 Bal: 0.8407\n","Epoch 207:      TX: Train Loss: 0.1041, Acc: 0.9627, F1: 0.9789 Bal: 0.9655 - Val Loss: 0.2034, Accuracy: 0.9495, F1: 0.9715 Bal: 0.9219\n","           WALLETS: Train Loss: 0.34675872, Acc: 0.87184148, F1: 0.92663940 Bal: 0.8416 - Val Loss: 0.34447673, Accuracy: 0.8711, F1: 0.9261 Bal: 0.8405\n","Epoch 208:      TX: Train Loss: 0.1035, Acc: 0.9628, F1: 0.9790 Bal: 0.9659 - Val Loss: 0.2037, Accuracy: 0.9504, F1: 0.9721 Bal: 0.9224\n","           WALLETS: Train Loss: 0.34616983, Acc: 0.87193219, F1: 0.92669178 Bal: 0.8419 - Val Loss: 0.34394127, Accuracy: 0.8712, F1: 0.9261 Bal: 0.8406\n","Epoch 209:      TX: Train Loss: 0.1029, Acc: 0.9629, F1: 0.9790 Bal: 0.9660 - Val Loss: 0.2040, Accuracy: 0.9509, F1: 0.9723 Bal: 0.9217\n","           WALLETS: Train Loss: 0.34558010, Acc: 0.87188865, F1: 0.92666107 Bal: 0.8421 - Val Loss: 0.34340283, Accuracy: 0.8712, F1: 0.9261 Bal: 0.8410\n","Epoch 210:      TX: Train Loss: 0.1023, Acc: 0.9631, F1: 0.9791 Bal: 0.9661 - Val Loss: 0.2043, Accuracy: 0.9509, F1: 0.9723 Bal: 0.9217\n","           WALLETS: Train Loss: 0.34498775, Acc: 0.87188865, F1: 0.92665681 Bal: 0.8424 - Val Loss: 0.34286097, Accuracy: 0.8711, F1: 0.9261 Bal: 0.8413\n","Epoch 211:      TX: Train Loss: 0.1017, Acc: 0.9633, F1: 0.9793 Bal: 0.9662 - Val Loss: 0.2047, Accuracy: 0.9511, F1: 0.9724 Bal: 0.9218\n","           WALLETS: Train Loss: 0.34439528, Acc: 0.87194308, F1: 0.92668416 Bal: 0.8429 - Val Loss: 0.34232578, Accuracy: 0.8712, F1: 0.9261 Bal: 0.8418\n","Epoch 212:      TX: Train Loss: 0.1010, Acc: 0.9635, F1: 0.9794 Bal: 0.9663 - Val Loss: 0.2051, Accuracy: 0.9515, F1: 0.9727 Bal: 0.9220\n","           WALLETS: Train Loss: 0.34380367, Acc: 0.87212087, F1: 0.92679218 Bal: 0.8431 - Val Loss: 0.34179923, Accuracy: 0.8714, F1: 0.9263 Bal: 0.8420\n","Epoch 213:      TX: Train Loss: 0.1005, Acc: 0.9638, F1: 0.9795 Bal: 0.9666 - Val Loss: 0.2055, Accuracy: 0.9520, F1: 0.9729 Bal: 0.9223\n","           WALLETS: Train Loss: 0.34321487, Acc: 0.87213538, F1: 0.92679897 Bal: 0.8432 - Val Loss: 0.34127891, Accuracy: 0.8716, F1: 0.9264 Bal: 0.8425\n","Epoch 214:      TX: Train Loss: 0.0999, Acc: 0.9640, F1: 0.9796 Bal: 0.9667 - Val Loss: 0.2058, Accuracy: 0.9524, F1: 0.9732 Bal: 0.9225\n","           WALLETS: Train Loss: 0.34262887, Acc: 0.87205193, F1: 0.92674252 Bal: 0.8436 - Val Loss: 0.34076497, Accuracy: 0.8714, F1: 0.9262 Bal: 0.8429\n","Epoch 215:      TX: Train Loss: 0.0993, Acc: 0.9640, F1: 0.9797 Bal: 0.9670 - Val Loss: 0.2061, Accuracy: 0.9528, F1: 0.9735 Bal: 0.9228\n","           WALLETS: Train Loss: 0.34204894, Acc: 0.87196485, F1: 0.92668322 Bal: 0.8439 - Val Loss: 0.34025517, Accuracy: 0.8716, F1: 0.9263 Bal: 0.8438\n","Epoch 216:      TX: Train Loss: 0.0987, Acc: 0.9642, F1: 0.9798 Bal: 0.9672 - Val Loss: 0.2065, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9229\n","           WALLETS: Train Loss: 0.34147421, Acc: 0.87197936, F1: 0.92668879 Bal: 0.8442 - Val Loss: 0.33975425, Accuracy: 0.8715, F1: 0.9263 Bal: 0.8445\n","Epoch 217:      TX: Train Loss: 0.0981, Acc: 0.9642, F1: 0.9798 Bal: 0.9672 - Val Loss: 0.2069, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9229\n","           WALLETS: Train Loss: 0.34090248, Acc: 0.87213901, F1: 0.92678325 Bal: 0.8445 - Val Loss: 0.33925700, Accuracy: 0.8717, F1: 0.9264 Bal: 0.8448\n","Epoch 218:      TX: Train Loss: 0.0976, Acc: 0.9646, F1: 0.9800 Bal: 0.9676 - Val Loss: 0.2074, Accuracy: 0.9528, F1: 0.9735 Bal: 0.9228\n","           WALLETS: Train Loss: 0.34033149, Acc: 0.87219344, F1: 0.92681244 Bal: 0.8448 - Val Loss: 0.33875290, Accuracy: 0.8720, F1: 0.9266 Bal: 0.8456\n","Epoch 219:      TX: Train Loss: 0.0970, Acc: 0.9649, F1: 0.9802 Bal: 0.9680 - Val Loss: 0.2078, Accuracy: 0.9528, F1: 0.9735 Bal: 0.9228\n","           WALLETS: Train Loss: 0.33976427, Acc: 0.87208821, F1: 0.92674108 Bal: 0.8452 - Val Loss: 0.33824885, Accuracy: 0.8719, F1: 0.9265 Bal: 0.8460\n","Epoch 220:      TX: Train Loss: 0.0964, Acc: 0.9649, F1: 0.9802 Bal: 0.9680 - Val Loss: 0.2080, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9229\n","           WALLETS: Train Loss: 0.33920237, Acc: 0.87216078, F1: 0.92678173 Bal: 0.8456 - Val Loss: 0.33775467, Accuracy: 0.8718, F1: 0.9264 Bal: 0.8462\n","Epoch 221:      TX: Train Loss: 0.0959, Acc: 0.9651, F1: 0.9803 Bal: 0.9681 - Val Loss: 0.2084, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9229\n","           WALLETS: Train Loss: 0.33864528, Acc: 0.87210998, F1: 0.92674593 Bal: 0.8459 - Val Loss: 0.33726811, Accuracy: 0.8719, F1: 0.9265 Bal: 0.8468\n","Epoch 222:      TX: Train Loss: 0.0953, Acc: 0.9652, F1: 0.9804 Bal: 0.9683 - Val Loss: 0.2087, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9229\n","           WALLETS: Train Loss: 0.33809388, Acc: 0.87225512, F1: 0.92682815 Bal: 0.8464 - Val Loss: 0.33678854, Accuracy: 0.8721, F1: 0.9266 Bal: 0.8474\n","Epoch 223:      TX: Train Loss: 0.0948, Acc: 0.9653, F1: 0.9804 Bal: 0.9685 - Val Loss: 0.2091, Accuracy: 0.9530, F1: 0.9736 Bal: 0.9229\n","           WALLETS: Train Loss: 0.33754438, Acc: 0.87234583, F1: 0.92688026 Bal: 0.8467 - Val Loss: 0.33631331, Accuracy: 0.8723, F1: 0.9267 Bal: 0.8478\n","Epoch 224:      TX: Train Loss: 0.0942, Acc: 0.9655, F1: 0.9805 Bal: 0.9688 - Val Loss: 0.2095, Accuracy: 0.9535, F1: 0.9738 Bal: 0.9231\n","           WALLETS: Train Loss: 0.33699700, Acc: 0.87231317, F1: 0.92685502 Bal: 0.8471 - Val Loss: 0.33583751, Accuracy: 0.8722, F1: 0.9266 Bal: 0.8482\n","Epoch 225:      TX: Train Loss: 0.0937, Acc: 0.9657, F1: 0.9806 Bal: 0.9687 - Val Loss: 0.2098, Accuracy: 0.9533, F1: 0.9737 Bal: 0.9230\n","           WALLETS: Train Loss: 0.33644772, Acc: 0.87214627, F1: 0.92674571 Bal: 0.8475 - Val Loss: 0.33535072, Accuracy: 0.8720, F1: 0.9265 Bal: 0.8484\n","Epoch 226:      TX: Train Loss: 0.0932, Acc: 0.9658, F1: 0.9807 Bal: 0.9686 - Val Loss: 0.2102, Accuracy: 0.9537, F1: 0.9740 Bal: 0.9232\n","           WALLETS: Train Loss: 0.33589554, Acc: 0.87220069, F1: 0.92677582 Bal: 0.8477 - Val Loss: 0.33486271, Accuracy: 0.8719, F1: 0.9265 Bal: 0.8485\n","Epoch 227:      TX: Train Loss: 0.0926, Acc: 0.9660, F1: 0.9808 Bal: 0.9686 - Val Loss: 0.2106, Accuracy: 0.9533, F1: 0.9737 Bal: 0.9230\n","           WALLETS: Train Loss: 0.33534402, Acc: 0.87233857, F1: 0.92685847 Bal: 0.8480 - Val Loss: 0.33437854, Accuracy: 0.8722, F1: 0.9266 Bal: 0.8486\n","Epoch 228:      TX: Train Loss: 0.0921, Acc: 0.9662, F1: 0.9809 Bal: 0.9690 - Val Loss: 0.2110, Accuracy: 0.9535, F1: 0.9738 Bal: 0.9231\n","           WALLETS: Train Loss: 0.33479553, Acc: 0.87246194, F1: 0.92692976 Bal: 0.8483 - Val Loss: 0.33389613, Accuracy: 0.8721, F1: 0.9266 Bal: 0.8485\n","Epoch 229:      TX: Train Loss: 0.0916, Acc: 0.9664, F1: 0.9810 Bal: 0.9692 - Val Loss: 0.2114, Accuracy: 0.9539, F1: 0.9741 Bal: 0.9234\n","           WALLETS: Train Loss: 0.33425099, Acc: 0.87253813, F1: 0.92697326 Bal: 0.8486 - Val Loss: 0.33341658, Accuracy: 0.8721, F1: 0.9266 Bal: 0.8486\n","Epoch 230:      TX: Train Loss: 0.0911, Acc: 0.9665, F1: 0.9811 Bal: 0.9695 - Val Loss: 0.2117, Accuracy: 0.9539, F1: 0.9741 Bal: 0.9234\n","           WALLETS: Train Loss: 0.33371213, Acc: 0.87255628, F1: 0.92698229 Bal: 0.8488 - Val Loss: 0.33294532, Accuracy: 0.8723, F1: 0.9267 Bal: 0.8492\n","Epoch 231:      TX: Train Loss: 0.0906, Acc: 0.9666, F1: 0.9811 Bal: 0.9697 - Val Loss: 0.2120, Accuracy: 0.9541, F1: 0.9742 Bal: 0.9235\n","           WALLETS: Train Loss: 0.33317527, Acc: 0.87327833, F1: 0.92742420 Bal: 0.8493 - Val Loss: 0.33248594, Accuracy: 0.8730, F1: 0.9272 Bal: 0.8497\n","Epoch 232:      TX: Train Loss: 0.0901, Acc: 0.9667, F1: 0.9812 Bal: 0.9699 - Val Loss: 0.2125, Accuracy: 0.9541, F1: 0.9742 Bal: 0.9235\n","           WALLETS: Train Loss: 0.33263874, Acc: 0.87340169, F1: 0.92749907 Bal: 0.8494 - Val Loss: 0.33203372, Accuracy: 0.8731, F1: 0.9272 Bal: 0.8499\n","Epoch 233:      TX: Train Loss: 0.0896, Acc: 0.9669, F1: 0.9813 Bal: 0.9700 - Val Loss: 0.2130, Accuracy: 0.9546, F1: 0.9745 Bal: 0.9237\n","           WALLETS: Train Loss: 0.33210209, Acc: 0.87331461, F1: 0.92744106 Bal: 0.8497 - Val Loss: 0.33157378, Accuracy: 0.8732, F1: 0.9273 Bal: 0.8502\n","Epoch 234:      TX: Train Loss: 0.0891, Acc: 0.9671, F1: 0.9814 Bal: 0.9701 - Val Loss: 0.2135, Accuracy: 0.9546, F1: 0.9745 Bal: 0.9237\n","           WALLETS: Train Loss: 0.33156773, Acc: 0.87321664, F1: 0.92737363 Bal: 0.8502 - Val Loss: 0.33111393, Accuracy: 0.8731, F1: 0.9272 Bal: 0.8501\n","Epoch 235:      TX: Train Loss: 0.0886, Acc: 0.9672, F1: 0.9815 Bal: 0.9703 - Val Loss: 0.2139, Accuracy: 0.9546, F1: 0.9745 Bal: 0.9237\n","           WALLETS: Train Loss: 0.33103535, Acc: 0.87332912, F1: 0.92744153 Bal: 0.8503 - Val Loss: 0.33066466, Accuracy: 0.8732, F1: 0.9273 Bal: 0.8506\n","Epoch 236:      TX: Train Loss: 0.0881, Acc: 0.9675, F1: 0.9816 Bal: 0.9711 - Val Loss: 0.2142, Accuracy: 0.9550, F1: 0.9747 Bal: 0.9240\n","           WALLETS: Train Loss: 0.33050409, Acc: 0.87341258, F1: 0.92749159 Bal: 0.8504 - Val Loss: 0.33021113, Accuracy: 0.8733, F1: 0.9273 Bal: 0.8508\n","Epoch 237:      TX: Train Loss: 0.0876, Acc: 0.9676, F1: 0.9817 Bal: 0.9712 - Val Loss: 0.2145, Accuracy: 0.9552, F1: 0.9748 Bal: 0.9241\n","           WALLETS: Train Loss: 0.32997367, Acc: 0.87339444, F1: 0.92747291 Bal: 0.8510 - Val Loss: 0.32974917, Accuracy: 0.8732, F1: 0.9272 Bal: 0.8511\n","Epoch 238:      TX: Train Loss: 0.0871, Acc: 0.9678, F1: 0.9818 Bal: 0.9716 - Val Loss: 0.2150, Accuracy: 0.9552, F1: 0.9748 Bal: 0.9241\n","           WALLETS: Train Loss: 0.32944438, Acc: 0.87340169, F1: 0.92747013 Bal: 0.8515 - Val Loss: 0.32929099, Accuracy: 0.8734, F1: 0.9273 Bal: 0.8515\n","Epoch 239:      TX: Train Loss: 0.0867, Acc: 0.9680, F1: 0.9819 Bal: 0.9718 - Val Loss: 0.2155, Accuracy: 0.9552, F1: 0.9748 Bal: 0.9241\n","           WALLETS: Train Loss: 0.32891944, Acc: 0.87357586, F1: 0.92757413 Bal: 0.8518 - Val Loss: 0.32884446, Accuracy: 0.8738, F1: 0.9276 Bal: 0.8522\n","Epoch 240:      TX: Train Loss: 0.0862, Acc: 0.9682, F1: 0.9821 Bal: 0.9718 - Val Loss: 0.2159, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9243\n","           WALLETS: Train Loss: 0.32840112, Acc: 0.87400038, F1: 0.92783398 Bal: 0.8521 - Val Loss: 0.32840455, Accuracy: 0.8744, F1: 0.9279 Bal: 0.8527\n","Epoch 241:      TX: Train Loss: 0.0857, Acc: 0.9683, F1: 0.9821 Bal: 0.9720 - Val Loss: 0.2164, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9243\n","           WALLETS: Train Loss: 0.32788372, Acc: 0.87413100, F1: 0.92790999 Bal: 0.8525 - Val Loss: 0.32795787, Accuracy: 0.8746, F1: 0.9281 Bal: 0.8528\n","Epoch 242:      TX: Train Loss: 0.0853, Acc: 0.9684, F1: 0.9822 Bal: 0.9722 - Val Loss: 0.2169, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9243\n","           WALLETS: Train Loss: 0.32736796, Acc: 0.87430516, F1: 0.92801154 Bal: 0.8529 - Val Loss: 0.32752281, Accuracy: 0.8747, F1: 0.9281 Bal: 0.8527\n","Epoch 243:      TX: Train Loss: 0.0848, Acc: 0.9686, F1: 0.9823 Bal: 0.9727 - Val Loss: 0.2173, Accuracy: 0.9559, F1: 0.9752 Bal: 0.9245\n","           WALLETS: Train Loss: 0.32685250, Acc: 0.87452287, F1: 0.92814399 Bal: 0.8532 - Val Loss: 0.32709882, Accuracy: 0.8749, F1: 0.9283 Bal: 0.8531\n","Epoch 244:      TX: Train Loss: 0.0844, Acc: 0.9688, F1: 0.9824 Bal: 0.9729 - Val Loss: 0.2178, Accuracy: 0.9563, F1: 0.9755 Bal: 0.9257\n","           WALLETS: Train Loss: 0.32633743, Acc: 0.87459543, F1: 0.92818226 Bal: 0.8536 - Val Loss: 0.32668144, Accuracy: 0.8749, F1: 0.9283 Bal: 0.8536\n","Epoch 245:      TX: Train Loss: 0.0839, Acc: 0.9689, F1: 0.9825 Bal: 0.9731 - Val Loss: 0.2184, Accuracy: 0.9559, F1: 0.9752 Bal: 0.9254\n","           WALLETS: Train Loss: 0.32582453, Acc: 0.87472243, F1: 0.92825484 Bal: 0.8541 - Val Loss: 0.32626823, Accuracy: 0.8750, F1: 0.9283 Bal: 0.8538\n","Epoch 246:      TX: Train Loss: 0.0835, Acc: 0.9691, F1: 0.9826 Bal: 0.9732 - Val Loss: 0.2190, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9243\n","           WALLETS: Train Loss: 0.32531172, Acc: 0.87484942, F1: 0.92832891 Bal: 0.8544 - Val Loss: 0.32585829, Accuracy: 0.8751, F1: 0.9284 Bal: 0.8540\n","Epoch 247:      TX: Train Loss: 0.0830, Acc: 0.9694, F1: 0.9827 Bal: 0.9734 - Val Loss: 0.2196, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9243\n","           WALLETS: Train Loss: 0.32480285, Acc: 0.87495102, F1: 0.92838858 Bal: 0.8547 - Val Loss: 0.32544985, Accuracy: 0.8756, F1: 0.9287 Bal: 0.8551\n","Epoch 248:      TX: Train Loss: 0.0826, Acc: 0.9695, F1: 0.9828 Bal: 0.9735 - Val Loss: 0.2201, Accuracy: 0.9557, F1: 0.9751 Bal: 0.9243\n","           WALLETS: Train Loss: 0.32430154, Acc: 0.87492925, F1: 0.92837313 Bal: 0.8548 - Val Loss: 0.32505006, Accuracy: 0.8755, F1: 0.9286 Bal: 0.8553\n","Epoch 249:      TX: Train Loss: 0.0821, Acc: 0.9697, F1: 0.9829 Bal: 0.9737 - Val Loss: 0.2207, Accuracy: 0.9555, F1: 0.9750 Bal: 0.9242\n","           WALLETS: Train Loss: 0.32380268, Acc: 0.87493287, F1: 0.92837149 Bal: 0.8551 - Val Loss: 0.32465526, Accuracy: 0.8755, F1: 0.9286 Bal: 0.8557\n","Epoch 250:      TX: Train Loss: 0.0817, Acc: 0.9700, F1: 0.9831 Bal: 0.9742 - Val Loss: 0.2211, Accuracy: 0.9555, F1: 0.9750 Bal: 0.9242\n","           WALLETS: Train Loss: 0.32330802, Acc: 0.87497642, F1: 0.92839702 Bal: 0.8552 - Val Loss: 0.32426831, Accuracy: 0.8758, F1: 0.9288 Bal: 0.8561\n","Epoch 251:      TX: Train Loss: 0.0813, Acc: 0.9702, F1: 0.9832 Bal: 0.9744 - Val Loss: 0.2217, Accuracy: 0.9559, F1: 0.9752 Bal: 0.9245\n","           WALLETS: Train Loss: 0.32281592, Acc: 0.87521589, F1: 0.92854219 Bal: 0.8555 - Val Loss: 0.32388446, Accuracy: 0.8759, F1: 0.9289 Bal: 0.8565\n","Epoch 252:      TX: Train Loss: 0.0808, Acc: 0.9703, F1: 0.9833 Bal: 0.9747 - Val Loss: 0.2221, Accuracy: 0.9559, F1: 0.9752 Bal: 0.9245\n","           WALLETS: Train Loss: 0.32232878, Acc: 0.87533925, F1: 0.92861402 Bal: 0.8558 - Val Loss: 0.32349634, Accuracy: 0.8760, F1: 0.9289 Bal: 0.8567\n","Epoch 253:      TX: Train Loss: 0.0804, Acc: 0.9705, F1: 0.9834 Bal: 0.9747 - Val Loss: 0.2227, Accuracy: 0.9559, F1: 0.9752 Bal: 0.9245\n","           WALLETS: Train Loss: 0.32184276, Acc: 0.87539731, F1: 0.92864638 Bal: 0.8561 - Val Loss: 0.32310936, Accuracy: 0.8760, F1: 0.9289 Bal: 0.8572\n","Epoch 254:      TX: Train Loss: 0.0800, Acc: 0.9707, F1: 0.9835 Bal: 0.9747 - Val Loss: 0.2235, Accuracy: 0.9561, F1: 0.9754 Bal: 0.9246\n","           WALLETS: Train Loss: 0.32135472, Acc: 0.87544811, F1: 0.92867458 Bal: 0.8563 - Val Loss: 0.32271516, Accuracy: 0.8759, F1: 0.9289 Bal: 0.8575\n","Epoch 255:      TX: Train Loss: 0.0796, Acc: 0.9707, F1: 0.9835 Bal: 0.9749 - Val Loss: 0.2241, Accuracy: 0.9563, F1: 0.9755 Bal: 0.9237\n","           WALLETS: Train Loss: 0.32086086, Acc: 0.87571298, F1: 0.92883410 Bal: 0.8567 - Val Loss: 0.32232687, Accuracy: 0.8761, F1: 0.9289 Bal: 0.8580\n","Epoch 256:      TX: Train Loss: 0.0791, Acc: 0.9709, F1: 0.9836 Bal: 0.9752 - Val Loss: 0.2246, Accuracy: 0.9570, F1: 0.9759 Bal: 0.9241\n","           WALLETS: Train Loss: 0.32036343, Acc: 0.87628627, F1: 0.92918237 Bal: 0.8572 - Val Loss: 0.32194528, Accuracy: 0.8765, F1: 0.9292 Bal: 0.8581\n","Epoch 257:      TX: Train Loss: 0.0787, Acc: 0.9710, F1: 0.9837 Bal: 0.9753 - Val Loss: 0.2252, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9242\n","           WALLETS: Train Loss: 0.31986618, Acc: 0.87630078, F1: 0.92918714 Bal: 0.8575 - Val Loss: 0.32155186, Accuracy: 0.8764, F1: 0.9291 Bal: 0.8582\n","Epoch 258:      TX: Train Loss: 0.0783, Acc: 0.9712, F1: 0.9838 Bal: 0.9757 - Val Loss: 0.2258, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9242\n","           WALLETS: Train Loss: 0.31937116, Acc: 0.87627175, F1: 0.92916464 Bal: 0.8579 - Val Loss: 0.32115263, Accuracy: 0.8764, F1: 0.9291 Bal: 0.8585\n","Epoch 259:      TX: Train Loss: 0.0779, Acc: 0.9715, F1: 0.9839 Bal: 0.9758 - Val Loss: 0.2264, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9253\n","           WALLETS: Train Loss: 0.31888154, Acc: 0.87650397, F1: 0.92930522 Bal: 0.8581 - Val Loss: 0.32077438, Accuracy: 0.8766, F1: 0.9293 Bal: 0.8585\n","Epoch 260:      TX: Train Loss: 0.0775, Acc: 0.9717, F1: 0.9841 Bal: 0.9760 - Val Loss: 0.2269, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9256\n","           WALLETS: Train Loss: 0.31839511, Acc: 0.87659831, F1: 0.92936128 Bal: 0.8583 - Val Loss: 0.32039788, Accuracy: 0.8769, F1: 0.9294 Bal: 0.8586\n","Epoch 261:      TX: Train Loss: 0.0771, Acc: 0.9719, F1: 0.9842 Bal: 0.9762 - Val Loss: 0.2274, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9256\n","           WALLETS: Train Loss: 0.31791952, Acc: 0.87647494, F1: 0.92928068 Bal: 0.8586 - Val Loss: 0.32002589, Accuracy: 0.8769, F1: 0.9294 Bal: 0.8589\n","Epoch 262:      TX: Train Loss: 0.0767, Acc: 0.9720, F1: 0.9842 Bal: 0.9765 - Val Loss: 0.2277, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9256\n","           WALLETS: Train Loss: 0.31744096, Acc: 0.87661645, F1: 0.92936418 Bal: 0.8589 - Val Loss: 0.31966096, Accuracy: 0.8768, F1: 0.9294 Bal: 0.8589\n","Epoch 263:      TX: Train Loss: 0.0763, Acc: 0.9722, F1: 0.9843 Bal: 0.9767 - Val Loss: 0.2281, Accuracy: 0.9577, F1: 0.9762 Bal: 0.9254\n","           WALLETS: Train Loss: 0.31696472, Acc: 0.87690309, F1: 0.92954160 Bal: 0.8589 - Val Loss: 0.31930169, Accuracy: 0.8771, F1: 0.9296 Bal: 0.8589\n","Epoch 264:      TX: Train Loss: 0.0759, Acc: 0.9723, F1: 0.9844 Bal: 0.9767 - Val Loss: 0.2287, Accuracy: 0.9581, F1: 0.9765 Bal: 0.9257\n","           WALLETS: Train Loss: 0.31649041, Acc: 0.87706274, F1: 0.92963971 Bal: 0.8590 - Val Loss: 0.31893843, Accuracy: 0.8773, F1: 0.9297 Bal: 0.8591\n","Epoch 265:      TX: Train Loss: 0.0755, Acc: 0.9724, F1: 0.9845 Bal: 0.9768 - Val Loss: 0.2291, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9256\n","           WALLETS: Train Loss: 0.31601617, Acc: 0.87733850, F1: 0.92980627 Bal: 0.8593 - Val Loss: 0.31858236, Accuracy: 0.8775, F1: 0.9298 Bal: 0.8593\n","Epoch 266:      TX: Train Loss: 0.0751, Acc: 0.9725, F1: 0.9845 Bal: 0.9773 - Val Loss: 0.2294, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9246\n","           WALLETS: Train Loss: 0.31554317, Acc: 0.87746549, F1: 0.92988026 Bal: 0.8597 - Val Loss: 0.31822053, Accuracy: 0.8777, F1: 0.9299 Bal: 0.8595\n","Epoch 267:      TX: Train Loss: 0.0747, Acc: 0.9726, F1: 0.9846 Bal: 0.9776 - Val Loss: 0.2297, Accuracy: 0.9577, F1: 0.9762 Bal: 0.9235\n","           WALLETS: Train Loss: 0.31507078, Acc: 0.87769408, F1: 0.93001673 Bal: 0.8601 - Val Loss: 0.31786728, Accuracy: 0.8779, F1: 0.9300 Bal: 0.8596\n","Epoch 268:      TX: Train Loss: 0.0743, Acc: 0.9728, F1: 0.9847 Bal: 0.9776 - Val Loss: 0.2303, Accuracy: 0.9577, F1: 0.9762 Bal: 0.9235\n","           WALLETS: Train Loss: 0.31459993, Acc: 0.87788276, F1: 0.93012963 Bal: 0.8604 - Val Loss: 0.31751662, Accuracy: 0.8779, F1: 0.9301 Bal: 0.8596\n","Epoch 269:      TX: Train Loss: 0.0739, Acc: 0.9730, F1: 0.9848 Bal: 0.9778 - Val Loss: 0.2309, Accuracy: 0.9577, F1: 0.9762 Bal: 0.9235\n","           WALLETS: Train Loss: 0.31412587, Acc: 0.87800250, F1: 0.93020089 Bal: 0.8606 - Val Loss: 0.31716070, Accuracy: 0.8781, F1: 0.9301 Bal: 0.8597\n","Epoch 270:      TX: Train Loss: 0.0735, Acc: 0.9731, F1: 0.9849 Bal: 0.9779 - Val Loss: 0.2316, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9233\n","           WALLETS: Train Loss: 0.31365448, Acc: 0.87819843, F1: 0.93031877 Bal: 0.8608 - Val Loss: 0.31680778, Accuracy: 0.8784, F1: 0.9303 Bal: 0.8600\n","Epoch 271:      TX: Train Loss: 0.0732, Acc: 0.9731, F1: 0.9849 Bal: 0.9779 - Val Loss: 0.2319, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9233\n","           WALLETS: Train Loss: 0.31318423, Acc: 0.87831091, F1: 0.93038616 Bal: 0.8610 - Val Loss: 0.31644675, Accuracy: 0.8783, F1: 0.9303 Bal: 0.8598\n","Epoch 272:      TX: Train Loss: 0.0728, Acc: 0.9731, F1: 0.9849 Bal: 0.9780 - Val Loss: 0.2321, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9233\n","           WALLETS: Train Loss: 0.31271559, Acc: 0.87854676, F1: 0.93052872 Bal: 0.8613 - Val Loss: 0.31609178, Accuracy: 0.8784, F1: 0.9303 Bal: 0.8600\n","Epoch 273:      TX: Train Loss: 0.0724, Acc: 0.9733, F1: 0.9850 Bal: 0.9784 - Val Loss: 0.2325, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9232\n","           WALLETS: Train Loss: 0.31225118, Acc: 0.87891322, F1: 0.93074797 Bal: 0.8618 - Val Loss: 0.31574571, Accuracy: 0.8787, F1: 0.9305 Bal: 0.8604\n","Epoch 274:      TX: Train Loss: 0.0721, Acc: 0.9735, F1: 0.9851 Bal: 0.9785 - Val Loss: 0.2333, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9233\n","           WALLETS: Train Loss: 0.31179014, Acc: 0.87913456, F1: 0.93088043 Bal: 0.8622 - Val Loss: 0.31539291, Accuracy: 0.8788, F1: 0.9306 Bal: 0.8603\n","Epoch 275:      TX: Train Loss: 0.0717, Acc: 0.9737, F1: 0.9852 Bal: 0.9785 - Val Loss: 0.2342, Accuracy: 0.9577, F1: 0.9762 Bal: 0.9235\n","           WALLETS: Train Loss: 0.31133285, Acc: 0.87933412, F1: 0.93099499 Bal: 0.8628 - Val Loss: 0.31502894, Accuracy: 0.8789, F1: 0.9307 Bal: 0.8610\n","Epoch 276:      TX: Train Loss: 0.0713, Acc: 0.9738, F1: 0.9853 Bal: 0.9787 - Val Loss: 0.2348, Accuracy: 0.9579, F1: 0.9764 Bal: 0.9236\n","           WALLETS: Train Loss: 0.31088006, Acc: 0.87967881, F1: 0.93120396 Bal: 0.8631 - Val Loss: 0.31468645, Accuracy: 0.8793, F1: 0.9309 Bal: 0.8612\n","Epoch 277:      TX: Train Loss: 0.0710, Acc: 0.9740, F1: 0.9853 Bal: 0.9790 - Val Loss: 0.2350, Accuracy: 0.9577, F1: 0.9762 Bal: 0.9235\n","           WALLETS: Train Loss: 0.31042787, Acc: 0.87987838, F1: 0.93132105 Bal: 0.8636 - Val Loss: 0.31433240, Accuracy: 0.8794, F1: 0.9309 Bal: 0.8614\n","Epoch 278:      TX: Train Loss: 0.0706, Acc: 0.9741, F1: 0.9854 Bal: 0.9792 - Val Loss: 0.2353, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9222\n","           WALLETS: Train Loss: 0.30998206, Acc: 0.87984209, F1: 0.93129347 Bal: 0.8640 - Val Loss: 0.31397536, Accuracy: 0.8794, F1: 0.9310 Bal: 0.8619\n","Epoch 279:      TX: Train Loss: 0.0702, Acc: 0.9741, F1: 0.9854 Bal: 0.9792 - Val Loss: 0.2358, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9224\n","           WALLETS: Train Loss: 0.30953893, Acc: 0.88037547, F1: 0.93161505 Bal: 0.8646 - Val Loss: 0.31364599, Accuracy: 0.8800, F1: 0.9313 Bal: 0.8627\n","Epoch 280:      TX: Train Loss: 0.0699, Acc: 0.9743, F1: 0.9855 Bal: 0.9793 - Val Loss: 0.2364, Accuracy: 0.9570, F1: 0.9759 Bal: 0.9211\n","           WALLETS: Train Loss: 0.30909902, Acc: 0.88049883, F1: 0.93168869 Bal: 0.8648 - Val Loss: 0.31331739, Accuracy: 0.8804, F1: 0.9315 Bal: 0.8632\n","Epoch 281:      TX: Train Loss: 0.0695, Acc: 0.9745, F1: 0.9856 Bal: 0.9794 - Val Loss: 0.2370, Accuracy: 0.9574, F1: 0.9761 Bal: 0.9214\n","           WALLETS: Train Loss: 0.30866376, Acc: 0.88060043, F1: 0.93174705 Bal: 0.8651 - Val Loss: 0.31299278, Accuracy: 0.8802, F1: 0.9314 Bal: 0.8633\n","Epoch 282:      TX: Train Loss: 0.0692, Acc: 0.9744, F1: 0.9856 Bal: 0.9796 - Val Loss: 0.2374, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9213\n","           WALLETS: Train Loss: 0.30822885, Acc: 0.88075645, F1: 0.93183892 Bal: 0.8655 - Val Loss: 0.31267607, Accuracy: 0.8804, F1: 0.9315 Bal: 0.8636\n","Epoch 283:      TX: Train Loss: 0.0688, Acc: 0.9746, F1: 0.9857 Bal: 0.9798 - Val Loss: 0.2377, Accuracy: 0.9570, F1: 0.9759 Bal: 0.9202\n","           WALLETS: Train Loss: 0.30779830, Acc: 0.88090521, F1: 0.93192692 Bal: 0.8658 - Val Loss: 0.31236622, Accuracy: 0.8804, F1: 0.9315 Bal: 0.8638\n","Epoch 284:      TX: Train Loss: 0.0685, Acc: 0.9747, F1: 0.9857 Bal: 0.9797 - Val Loss: 0.2381, Accuracy: 0.9572, F1: 0.9760 Bal: 0.9203\n","           WALLETS: Train Loss: 0.30736727, Acc: 0.88077096, F1: 0.93183845 Bal: 0.8662 - Val Loss: 0.31203461, Accuracy: 0.8802, F1: 0.9314 Bal: 0.8637\n","Epoch 285:      TX: Train Loss: 0.0681, Acc: 0.9748, F1: 0.9858 Bal: 0.9799 - Val Loss: 0.2386, Accuracy: 0.9561, F1: 0.9754 Bal: 0.9167\n","           WALLETS: Train Loss: 0.30693617, Acc: 0.88102858, F1: 0.93199377 Bal: 0.8665 - Val Loss: 0.31172475, Accuracy: 0.8803, F1: 0.9315 Bal: 0.8639\n","Epoch 286:      TX: Train Loss: 0.0678, Acc: 0.9749, F1: 0.9859 Bal: 0.9802 - Val Loss: 0.2390, Accuracy: 0.9563, F1: 0.9755 Bal: 0.9168\n","           WALLETS: Train Loss: 0.30650619, Acc: 0.88117008, F1: 0.93207818 Bal: 0.8667 - Val Loss: 0.31141350, Accuracy: 0.8804, F1: 0.9315 Bal: 0.8640\n","Epoch 287:      TX: Train Loss: 0.0675, Acc: 0.9751, F1: 0.9860 Bal: 0.9804 - Val Loss: 0.2395, Accuracy: 0.9561, F1: 0.9754 Bal: 0.9167\n","           WALLETS: Train Loss: 0.30607662, Acc: 0.88109752, F1: 0.93203078 Bal: 0.8669 - Val Loss: 0.31109130, Accuracy: 0.8802, F1: 0.9314 Bal: 0.8640\n","Epoch 288:      TX: Train Loss: 0.0671, Acc: 0.9753, F1: 0.9861 Bal: 0.9805 - Val Loss: 0.2400, Accuracy: 0.9563, F1: 0.9755 Bal: 0.9168\n","           WALLETS: Train Loss: 0.30564928, Acc: 0.88133336, F1: 0.93217474 Bal: 0.8670 - Val Loss: 0.31078950, Accuracy: 0.8802, F1: 0.9314 Bal: 0.8638\n","Epoch 289:      TX: Train Loss: 0.0668, Acc: 0.9753, F1: 0.9861 Bal: 0.9804 - Val Loss: 0.2404, Accuracy: 0.9563, F1: 0.9755 Bal: 0.9168\n","           WALLETS: Train Loss: 0.30522323, Acc: 0.88151841, F1: 0.93228458 Bal: 0.8674 - Val Loss: 0.31048021, Accuracy: 0.8801, F1: 0.9313 Bal: 0.8636\n","Epoch 290:      TX: Train Loss: 0.0665, Acc: 0.9753, F1: 0.9861 Bal: 0.9804 - Val Loss: 0.2408, Accuracy: 0.9561, F1: 0.9754 Bal: 0.9167\n","           WALLETS: Train Loss: 0.30480015, Acc: 0.88136239, F1: 0.93218655 Bal: 0.8675 - Val Loss: 0.31016064, Accuracy: 0.8800, F1: 0.9313 Bal: 0.8636\n","Epoch 291:      TX: Train Loss: 0.0662, Acc: 0.9755, F1: 0.9862 Bal: 0.9808 - Val Loss: 0.2412, Accuracy: 0.9561, F1: 0.9754 Bal: 0.9167\n","           WALLETS: Train Loss: 0.30438033, Acc: 0.88151115, F1: 0.93227763 Bal: 0.8676 - Val Loss: 0.30986229, Accuracy: 0.8803, F1: 0.9315 Bal: 0.8636\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-bdc4fcf7605a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdim_reduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpca_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_reduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-3f28e706bbde>\u001b[0m in \u001b[0;36mtrain_grid\u001b[0;34m(data_full, param_grid, scalers, dim_reductions, pca_thresholds)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 model = ResidualHeteroGNN(params['conv_type'], hidden_channels = params['hidden_channels'], num_layers = params['num_layers'],\n\u001b[1;32m    169\u001b[0m                                   aggr=params['aggr'], dropout_prob=params['dropout'], num_head=params['num_head'])\n\u001b[0;32m--> 170\u001b[0;31m               \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Configurazione {combination_counter}/{total_combinations} già testata, salto...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-3f28e706bbde>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, params)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mout_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_wallet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_wallet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_wallet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-d789140c1fa0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_wallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wallet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/hetero_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args_dict, **kwargs_dict)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdst\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/sage_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_l\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_9wjiptai.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         kwargs = self.collect(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_9wjiptai.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mx_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m_index_select\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_select_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_index_select_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36m_index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_index_select_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["hyperparams = {\n","    \"hidden_channels\": [32, 64, 128],\n","    \"num_layers\": [2,3,4],\n","    \"num_epoch\": [300],\n","    \"patience\": [50],\n","    \"lr\": [0.001],\n","    \"weight_decay\": [0, 1e-5],\n","    \"dropout\": [0],\n","    \"conv_type\": ['SAGE', 'Transformer', 'GAT'],\n","    \"p\": [5],\n","    \"factor\": [0.5, 0.2],\n","    \"eta_min\": [1e-5],\n","    \"T_max\": [10, 15], #10 15\n","    \"aggr\": ['mean', 'sum'], #,\n","    'lr_scheduler':['CosineAnnealingLR'],\n","    'optimizer': ['Adam'], #Adam\n","    'type_model':['HeteroGNN'],\n","}\n","\n","scaler = ['no','standard', 'standard_Ll']\n","dim_reduction=['no','pca']\n","pca_threshold=[0.99]\n","best_model = train_grid(data, hyperparams, scaler, dim_reduction, pca_threshold)"],"metadata":{"id":"5-X8mjb7sjLI"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1wIoomnaeh74t1AKH1zYUzUr42JRiCeq-","timestamp":1739893838211}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}